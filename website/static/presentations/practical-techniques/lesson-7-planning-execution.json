{
  "metadata": {
    "title": "Planning and Execution: From Context to Code",
    "lessonId": "lesson-7-planning-execution",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Master active context engineering",
      "Review plans before execution",
      "Enable parallel agent workflows",
      "Force grounding with evidence"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Planning and Execution",
      "subtitle": "From Context to Code",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson bridges the gap between gathering context (Lesson 5) and using it effectively. We shift from passive context loading to active context engineering—deliberate techniques that keep agents grounded in your codebase rather than defaulting to training data patterns. The core challenge: agents are good at pattern completion, not reasoning. Your job is to enforce grounding through planning review and evidence requirements.",
        "timing": "1 minute",
        "discussion": "Ask students: 'How many of you have seen an agent suggest code that doesn't match your codebase patterns?' This frames the problem we're solving.",
        "context": "By the end of this lesson, students will have tactical techniques for reviewing agent plans, requiring evidence-based reasoning, and orchestrating parallel workflows—skills that transform agents from code generators into reliable development partners.",
        "transition": "Let's start with the most fundamental principle: why context engineering matters."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering vs Passive Loading",
      "content": [
        "Grounding is continuous, not one-time",
        "Load context → review plan → execute → validate",
        "Agent defaults to training data patterns",
        "Your codebase specifics must override statistics"
      ],
      "speakerNotes": {
        "talkingPoints": "Grounding isn't something you do upfront and forget. It's a feedback loop: you load relevant context, the agent plans based on that context, you review before execution, and you validate. The critical insight is that agents don't 'think'—they complete patterns. Without explicit grounding, they fall back to training data, which means generic solutions that don't fit your specific architecture.",
        "timing": "2 minutes",
        "discussion": "Poll: 'How do you currently prevent agents from suggesting duplicate code?' Most answers will be reactive (reviewing PRs). This lesson shows proactive techniques.",
        "context": "Real production scenario: agent suggests a new Redis client connection in every middleware function instead of discovering your existing RedisService singleton. This happens because the agent pattern-completed a reasonable Redis implementation from training data instead of searching your codebase.",
        "transition": "Let's look at the first active technique: always ground implementations in actual code, not abstractions."
      }
    },
    {
      "type": "codeComparison",
      "title": "Grounding: Concrete vs Abstract",
      "leftCode": {
        "label": "Abstract (Leads to Generic Code)",
        "language": "text",
        "code": "Add rate limiting middleware to the\nAPI"
      },
      "rightCode": {
        "label": "Concrete (Grounds in Your Patterns)",
        "language": "text",
        "code": "Search existing middleware patterns,\nespecially authentication. Check Redis\nconfig. Propose rate limiting following\nthe same error handling, export\nstructure, and Redis client usage."
      },
      "speakerNotes": {
        "talkingPoints": "The concrete prompt forces the agent to search for and ground itself in your actual patterns. The abstract prompt leaves the agent guessing based on what 'looks right' from training data. You're not being verbose—you're being specific about where to find grounding.",
        "timing": "2 minutes",
        "discussion": "Have students share a prompt they'd normally write. Rewrite it together to be concrete. What specific files or patterns should the agent look for first?",
        "context": "In production: concrete prompts reduce iteration from 5+ rounds to 1-2. The agent isn't second-guessing your codebase conventions because it already found them.",
        "transition": "Next principle: questions aren't for verification—they're tools for loading context into the window."
      }
    },
    {
      "type": "concept",
      "title": "Questions as Context Engineering",
      "content": [
        "Question triggers: search → read → synthesis",
        "Synthesis loads information into context window",
        "Follow-up prompts have 'warm start' with context",
        "Safe to execute autonomously (read-only operations)"
      ],
      "speakerNotes": {
        "talkingPoints": "When you ask 'How does our authentication middleware work?', you're not testing knowledge. You're triggering a sequence: agent searches for auth files, reads implementations, analyzes patterns, and synthesizes findings. That synthesis now lives in the context window. When you follow up with 'Add rate limiting following the same pattern,' the agent already has middleware structure, error handling conventions, and export patterns loaded. You've essentially pre-loaded the context.",
        "timing": "2 minutes",
        "discussion": "Strategic question framing: 'What would you ask an agent before asking it to implement a major feature?' Guide toward: 'How do we currently handle X?' or 'What's our pattern for Y?'",
        "context": "Real workflow: Engineer asks 'Walk me through how our authentication flow works' → agent searches, reads auth.ts, oauth.ts, session stores, synthesizes findings → 15 relevant files now loaded in context. Then: 'Add OAuth callback rate limiting following existing error patterns' → agent has all grounding already.",
        "transition": "But questions are only half the story. Sometimes the agent will generate plausible answers from training data. We need to force evidence."
      }
    },
    {
      "type": "codeComparison",
      "title": "Forcing Grounding with Evidence Requirements",
      "leftCode": {
        "label": "Without Evidence",
        "language": "text",
        "code": "Why is the authentication endpoint\nreturning 500 errors?"
      },
      "rightCode": {
        "label": "With Evidence Requirements",
        "language": "text",
        "code": "Why is the authentication endpoint\nreturning 500 errors? Provide evidence:\n- File paths with line numbers\n- Actual values from config/logs\n- Stack traces or log entries\n- Explain the connection between each"
      },
      "speakerNotes": {
        "talkingPoints": "Without evidence requirements, the agent will pattern-complete a plausible answer: 'probably a database timeout.' With evidence requirements, the agent must read your actual code, trace execution, and cite specifics. This converts guessing into grounded analysis. The agent cannot provide evidence without retrieving it.",
        "timing": "2 minutes",
        "discussion": "Show what good evidence looks like: 'src/api/auth.ts:67 where user.profile.email is accessed. Profile is null for OAuth users (src/services/oauth.ts:134). Stack trace: TypeError: Cannot read property email of null.' That's grounded. 'Probably a database issue' is not.",
        "context": "Production example: you ask 'Is our cache expiration correct?' With evidence requirement, agent must check Redis config, trace TTL values, cite actual settings. Without evidence, agent guesses. Evidence requirements shift from opinion to observation.",
        "transition": "Now let's look at how you use these techniques during plan review—before the agent executes any code."
      }
    },
    {
      "type": "concept",
      "title": "Plan Review: Before Execution",
      "content": [
        "Check strategy and reasoning, not just output",
        "Was grounding thorough? (Files read, constraints understood?)",
        "Did agent miss considerations? (Security, compat, edge cases?)",
        "Pattern adherence: does this fit your architecture?",
        "Scope fit: right modules, appropriate scale?"
      ],
      "speakerNotes": {
        "talkingPoints": "Before autonomous execution, review the plan. This is high-level architectural fit, not line-by-line code review. You're ensuring the agent understood your codebase and is proposing an approach that matches your established patterns. Key questions: How was this plan derived? Did it read relevant files? Did it understand constraints? Does the reasoning hold?",
        "timing": "3 minutes",
        "discussion": "Common failure mode: 'Agent proposes creating a new validation library instead of using our existing Zod schemas.' This is a grounding failure. At plan review, you catch it and correct before execution.",
        "context": "Real scenario: Agent plans to add email validation by creating src/lib/validators/email.ts. But you already have src/validation/ with Zod schemas. Plan review catches this before code generation. You stop and clarify: 'We use Zod schemas in src/validation/. Show me our existing approach before proposing additions.'",
        "transition": "Plan review is where you catch a specific pattern: agents inventing instead of discovering what already exists."
      }
    },
    {
      "type": "concept",
      "title": "Watch For: Invention vs Reuse",
      "content": [
        "AI-generated code has 8x more duplication",
        "Red flags: 'create new utility', 'build helper'",
        "Agent pattern-completes instead of code-discovers",
        "Force discovery-first with evidence requirements",
        "Enforce DRY during plan review, not after"
      ],
      "speakerNotes": {
        "talkingPoints": "Research shows AI-generated code contains 8x more duplicated blocks than human-written code. Why? Because agents pattern-complete from training data instead of searching your codebase. During plan review, watch for these phrases: 'create new utility function', 'implement a helper', 'build error handling logic.' Each signals the agent is inventing rather than discovering. Stop and require evidence: 'Search for existing utilities for this. Show me what exists before proposing new code.'",
        "timing": "2 minutes",
        "discussion": "Hands-on: show a plan that says 'Create error logging utility.' Ask: what are the red flags? How would you intervene? Expected answer: require agent to search existing logging, show what's already there.",
        "context": "Real codebase example: agent proposes new parseJSON utility. But you have a src/utils/parsing.ts with 12 helper functions including parseJSON. The agent didn't search; it pattern-completed from training data. Plan review catches this.",
        "transition": "Now let's talk about safety during this process: checkpointing."
      }
    },
    {
      "type": "concept",
      "title": "Checkpointing: Your Safety Net",
      "content": [
        "Agents make mistakes frequently (early in learning)",
        "Checkpoint before risky operations",
        "Validate results, then keep or revert",
        "Claude Code: Press ESC twice to checkpoint",
        "Without built-in checkpoints: commit after each success"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic coding is probabilistic. Agents will make mistakes—especially while you're learning effective prompting. Checkpointing is your safety net. Create a restore point before risky operations, let the agent execute, validate results, and keep or revert. Claude Code has built-in checkpointing: press ESC twice. Without it, commit frequently after each successful increment. This creates a timeline of known-good states you can revert to.",
        "timing": "2 minutes",
        "discussion": "Poll: 'How often do you commit?' Most developers commit maybe every 30 minutes. With agents, consider checkpointing every 5-10 minutes during risky operations. The validation phase (Lesson 9) determines keep/discard—checkpointing makes that decision reversible.",
        "context": "Real workflow: checkpoint before 'Refactor all error handling to use ErrorContext pattern.' Let agent execute. If tests fail or logic breaks, revert to checkpoint instantly. No manual rollback needed.",
        "transition": "Once your plan is solid and grounding is confirmed, you can execute autonomously. For complex features, parallel execution accelerates development dramatically."
      }
    },
    {
      "type": "concept",
      "title": "Git Worktrees: Parallel Agent Workflows",
      "content": [
        "One repo, multiple working directories",
        "Each worktree: different branch, isolated context",
        "Run 3 agents simultaneously on different features",
        "Zero conflicts, zero synchronization overhead",
        "Faster development cycles on complex projects"
      ],
      "speakerNotes": {
        "talkingPoints": "Git worktrees enable true parallelization. Instead of switching branches, you create separate working directories from a single repository. Each worktree is a complete checkout with its own branch. This means you can run 3 agent instances on different tasks simultaneously—one on feature A, one on feature B, one on bugfix C—with zero interference. No context-switching, no merge conflicts.",
        "timing": "2 minutes",
        "discussion": "Ask: 'How many of you context-switch between branches frequently?' That's the problem worktrees solve. Show the workflow: main branch, create worktree for feature, agent executes in that directory, completely isolated.",
        "context": "Real scenario: main branch is testing. You create worktree-auth for authentication refactor, worktree-cache for caching optimization, worktree-api for API expansion. Three agent instances run in parallel. You integrate results back to main incrementally.",
        "transition": "Running parallel worktrees efficiently requires good terminal setup. Let's talk about that."
      }
    },
    {
      "type": "concept",
      "title": "Terminal Customization for Multi-Agent Workflows",
      "content": [
        "Modern terminals: GPU acceleration, layouts, scripting",
        "Options: Ghostty, Kitty, WezTerm, Alacritty",
        "Invest in keybindings for rapid context switching",
        "Notification systems for long-running processes",
        "Terminal becomes mission-critical infrastructure"
      ],
      "speakerNotes": {
        "talkingPoints": "When you manage multiple worktrees and agent sessions, your terminal becomes critical infrastructure. Modern terminals offer IDE-level features: GPU acceleration for smooth rendering, programmable layouts to manage multiple panes, rich scripting for automation, and notification systems for alerts. Investing in terminal customization pays dividends every session. Set up keybindings for rapid context-switching between agent instances, configure notifications to alert when long processes complete, and organize your layout for monitoring multiple worktrees.",
        "timing": "2 minutes",
        "discussion": "Poll: 'What terminal do you use?' Introduce modern alternatives if most use iTerm or basic terminals. Discuss: what would make managing 3 concurrent agent sessions easier?",
        "context": "Real workflow in a terminal like WezTerm or Kitty: left pane shows main branch, center pane shows worktree-auth (agent session), right pane shows lazygit for branch management. Custom keybindings switch panes instantly. Notifications alert when agent completes work.",
        "transition": "Next: the specific CLI tools that make multi-agent workflows efficient."
      }
    },
    {
      "type": "concept",
      "title": "Essential CLI Tools for Workflows",
      "content": [
        "micro: Terminal editor (Ctrl+S save, Ctrl+Q quit)",
        "eza: Modern ls with git integration and colors",
        "fzf: Fuzzy finder for files, history, branches",
        "lazygit: Git UI for branches, staging, commits",
        "Combines with agent workflows for efficiency"
      ],
      "speakerNotes": {
        "talkingPoints": "These tools reduce friction in multi-agent workflows. micro is a lightweight terminal editor—faster than switching to IDE for one-line changes. eza replaces ls with better formatting and git status. fzf lets you fuzzy-find files across worktrees instantly. lazygit manages branching visually. Together, they create a cohesive CLI environment for managing parallel agent sessions.",
        "timing": "2 minutes",
        "discussion": "Live demo concept: show how to navigate to a worktree, fuzzy-find a file with fzf, quick edit in micro, commit with lazygit—all without touching the IDE. Fast context-switching.",
        "context": "Real workflow: You have 3 agent sessions running. One agent finishes. You switch terminals (keybinding), use lazygit to review commits, use eza to navigate the directory, use fzf to find test files, use micro to tweak a validation rule, commit.",
        "transition": "But pragmatism matters: CLI is powerful but not a religion. Let's talk about mixing CLI and UI tools."
      }
    },
    {
      "type": "comparison",
      "title": "CLI vs IDE: Pragmatic Tool Use",
      "left": {
        "label": "Over-Index on CLI",
        "content": [
          "Everything in terminal (dogmatism)",
          "Fighting tools that aren't optimal",
          "Symbol search is slow, complex logic is hard",
          "Large file navigation becomes painful"
        ]
      },
      "right": {
        "label": "Use Best Tool for Task",
        "content": [
          "IDE for navigation, symbol search, go-to-definition",
          "CLI for quick edits, git ops, multi-worktree mgmt",
          "Large files in IDE (better folding, syntax)",
          "Small changes in terminal (faster than IDE context-switch)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Don't be dogmatic. IDEs remain the best tools for symbol search, go-to-definition, viewing large files, and understanding call hierarchies. CLI excels at quick edits, git operations across worktrees, and rapid task-switching. Use the best tool for each task. Open your IDE for complex navigation; use micro for one-line changes in a terminal. This pragmatism beats purism every time.",
        "timing": "2 minutes",
        "discussion": "Scenario: You're reviewing 500-line middleware file. IDE (symbol search, folding). You need to change one config value. Terminal (micro, fast). Which is faster?",
        "context": "Real development: you keep IDE open for exploration, CLIs in separate terminals for git/editing. Not either/or—both, optimized.",
        "transition": "Now let's synthesize everything into key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Active context engineering prevents hallucinations—ground in code, not abstractions",
        "Questions load context for follow-up execution, not for verification",
        "Evidence requirements force agents to search your codebase, not guess",
        "Plan review catches grounding failures and invention-vs-reuse patterns before code",
        "Checkpoints + worktrees enable safe, parallel agent workflows at production scale"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles form the foundation of effective agent-driven development. Active context engineering is about being deliberate: when you prompt, you're either explicitly grounding the agent or letting it fall back to training data. Questions are a tool—they load context for subsequent steps. Evidence forces grounding. Plan review catches architecture mismatches early. And infrastructure (checkpoints, worktrees, terminal setup) makes iteration fast and safe.",
        "timing": "2 minutes",
        "discussion": "Synthesis: 'Which of these five will change how you work with agents?' Typically, students identify evidence requirements and plan review as immediately applicable.",
        "context": "By mastering these techniques, you shift from 'agent as code generator' (luck-dependent, high iteration) to 'agent as reliable development partner' (systematic, low iteration). The difference is measurable in productivity and code quality.",
        "transition": "Next lesson covers validation and testing—how to ensure agent-generated code meets requirements and maintains quality standards."
      }
    }
  ]
}
