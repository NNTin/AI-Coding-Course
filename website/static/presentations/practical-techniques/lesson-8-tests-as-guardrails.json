{
  "metadata": {
    "title": "Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Use tests to ground agent implementations",
      "Prevent regressions at agent scale",
      "Separate code, tests, and debugging contexts",
      "Deploy agent-driven testing discovery"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "Constraining Agent Velocity, Grounding Implementation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Agents can refactor half your codebase in minutes. This velocity is powerful—but dangerous. Small logic errors compound at scale. Tests are your constraint system: they define boundaries agents cannot cross and ground implementation decisions in your actual codebase rather than statistical patterns.",
        "timing": "1 minute",
        "discussion": "How many times has an agent missed an edge case in your code? What happened when that error hit production?",
        "context": "This lesson builds on Lessons 1-7, showing how to control agent velocity through testing patterns and fresh contexts.",
        "transition": "Let's start by understanding what agents actually read from your tests..."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation Agents Read",
      "content": [
        "Tests load into context during research phase",
        "Good tests ground code in actual codebase examples",
        "Bad tests (heavy mocking, unclear names) pollute context",
        "Test quality determines agent implementation quality"
      ],
      "speakerNotes": {
        "talkingPoints": "Before agents write code, they research. Tests aren't learning material—they're concrete constraints. When an auth refactor loads 50 tests into context, their quality determines whether the agent's implementation is grounded in your constraints or completes patterns from training. A test like 'test: OAuth users skip email verification' is documentation an agent will use. A test like 'test(\"works\")' with unclear assertions just fills context with noise.",
        "timing": "3-4 minutes",
        "discussion": "Look at your test suite. If an agent read all of them, what would it learn about your system? What tests confuse more than they clarify?",
        "context": "In production, test names and structure matter more than comments. Agents parse test code itself, not documentation strings. A test named testValidateUserWithOAuth() shows more than a comment ever could.",
        "transition": "So how do we discover what tests we actually need?"
      }
    },
    {
      "type": "concept",
      "title": "Research First: Discover Edge Cases",
      "content": [
        "Use planning techniques to discover testing needs",
        "Ask questions about existing implementations",
        "Search for related tests and analyze coverage",
        "Identify untested paths vs actual code",
        "Build grounded test list from implementation reality"
      ],
      "speakerNotes": {
        "talkingPoints": "Before writing tests, research what needs testing. Don't start with generic testing best practices. Ask the agent to find the function, read its implementation, discover existing tests, and synthesize findings. This loads concrete constraints into context: OAuth users skip email verification, admin users bypass rate limits, deleted users are rejected. These are discovered from actual code, not invented from templates.",
        "timing": "2-3 minutes",
        "discussion": "When was the last time you discovered an edge case through code exploration rather than a bug report? How could agents help you do this systematically?",
        "context": "This connects to Lesson 7's planning methodology: use agents to synthesize requirements from code, not from assumptions.",
        "transition": "Once we know what to test, we need a process to write both code and tests correctly..."
      }
    },
    {
      "type": "code",
      "title": "Edge Case Discovery: Prompt Pattern",
      "language": "text",
      "code": "How does validateUser() work? What edge cases exist\n  in the current implementation?\nWhat special handling exists for different auth providers?\nSearch for related tests and analyze what they cover.",
      "caption": "Research implementation before writing tests",
      "speakerNotes": {
        "talkingPoints": "This pattern loads implementation details and edge cases into context. The agent searches the function, reads implementation, finds tests, synthesizes findings. Follow up by asking what's NOT covered, then you have a grounded list derived from actual code.",
        "timing": "2 minutes",
        "discussion": "What questions would you ask to discover edge cases in your own systems?",
        "context": "This is Chain-of-Thought applied to research: sequential discovery rather than asking 'write tests for validateUser()'.",
        "transition": "Now the tricky part: how do we write code and tests without them inheriting the same flawed assumptions?"
      }
    },
    {
      "type": "concept",
      "title": "Cycle of Self-Deception: The Risk",
      "content": [
        "Same context → same flawed assumptions for code AND tests",
        "Agent writes endpoint accepting zero quantities",
        "Agent writes test verifying zero quantities succeed",
        "Both artifacts validated by same reasoning state",
        "Bug compound at scale: 1% of cycles × 1000 changes"
      ],
      "speakerNotes": {
        "talkingPoints": "When code and tests are generated in the same conversation, they inherit identical blind spots. An agent might write an API accepting zero quantities, then generate tests that verify this behavior succeeds. No external validation caught the shared assumption. At scale—agents making 1000 changes across a codebase—even 1% defect rate becomes critical. This is Goodhart's Law: when tests become the optimization target, agents optimize for passing tests rather than correctness.",
        "timing": "3-4 minutes",
        "discussion": "Have you seen 'specification gaming' in your own testing? Where tests pass but code doesn't work as intended?",
        "context": "Research shows this occurs in ~1% of test-code generation cycles. At scale, it compounds. The solution is architectural, not procedural.",
        "transition": "The fix is surprisingly simple: use fresh contexts..."
      }
    },
    {
      "type": "visual",
      "title": "Three-Context Workflow",
      "component": "ThreeContextWorkflow",
      "caption": "Separate contexts prevent shared assumptions and enable objective analysis",
      "speakerNotes": {
        "talkingPoints": "Context A: Write code—research patterns, plan, execute, verify. Context B: Write tests in fresh context—research requirements independently, agent doesn't remember implementing. Context C: Triage failures—analyze with fresh eyes, determine root cause objectively. This stateless approach (Lessons 1-2) prevents agents from defending prior decisions. Salesforce reduced debugging time 30% using this workflow.",
        "timing": "4-5 minutes",
        "discussion": "In your current workflow, do you test code in the same session you wrote it? What would change if you used fresh contexts?",
        "context": "This is architectural: it leverages statelessness as a feature, not a limitation.",
        "transition": "Now let's talk about what makes tests actually effective at preventing regressions..."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavily Mocked",
        "content": [
          "Stubs internal functions",
          "Verifies function calls, not behavior",
          "Passes when implementations break",
          "False confidence in coverage"
        ]
      },
      "right": {
        "label": "Sociable",
        "content": [
          "Uses real internal implementations",
          "Verifies actual functionality",
          "Catches real breakage immediately",
          "Tests what users experience"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "A heavily mocked auth test stubs findByEmail(), verify(), and create()—passes even when all three break. A sociable test uses real database queries, real password hashing, real session tokens. When the agent breaks any part, the test fails. Mock ONLY external systems (APIs, third-party services). Use real implementations for internal code.",
        "timing": "2-3 minutes",
        "discussion": "Look at a test in your suite. Could an agent break the actual code and have this test still pass? If yes, it's too mocked.",
        "context": "This connects to Lesson 5's grounding principle: real behavior beats mocked behavior.",
        "transition": "With the right test structure, we also need speed..."
      }
    },
    {
      "type": "concept",
      "title": "Fast Feedback with Smoke Tests",
      "content": [
        "Sub-30-second smoke test suite only",
        "Cover critical junctions: auth, core flow, database",
        "Run after each task catches failures immediately",
        "10-minute comprehensive suite gets skipped",
        "Codify in AGENTS.md for automatic execution"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute test suite doesn't fit agent iteration loops. You'll skip it until debugging becomes expensive. Build smoke tests covering only critical junctions—core user journey, authentication boundaries, database connectivity. Run after each task. When context is fresh, you catch failures immediately. When you make 20 changes before discovering which broke the system, debugging costs compound. Codify this practice in your AGENTS.md so agents automatically run smoke tests without reminders.",
        "timing": "2 minutes",
        "discussion": "What's your current test iteration loop? How many changes happen before you discover a failure?",
        "context": "This is about operational discipline: making the right behavior the default behavior.",
        "transition": "Beyond deterministic tests, there's another testing strategy agents excel at..."
      }
    },
    {
      "type": "concept",
      "title": "Agent Simulation: Non-Deterministic Testing",
      "content": [
        "Agents explore state spaces through randomized decisions",
        "Same test script = different paths each iteration",
        "Unreliable for regression testing in CI/CD",
        "Excellent for discovering unknown edge cases",
        "Complement deterministic tests, not replace them"
      ],
      "speakerNotes": {
        "talkingPoints": "Agents make probabilistic decisions. Run the same test script twice and the agent explores different paths. This isn't a bug—it's a feature for discovery. One iteration tests the happy path, another discovers a race condition from rapid clicking, a third finds unicode character edge cases. This randomness fails in CI/CD, but it's perfect for finding unknown unknowns. Use agents for discovery, solidify findings into deterministic tests.",
        "timing": "3 minutes",
        "discussion": "What edge cases have you only discovered through user reports? Could agents find those proactively?",
        "context": "This bridges automated testing and adversarial testing: agents are efficient explorers.",
        "transition": "Now the practical workflow when tests do fail..."
      }
    },
    {
      "type": "code",
      "title": "Test Failure Diagnosis: Prompt Pattern",
      "language": "text",
      "code": "$FAILURE_DESCRIPTION\n\nUse the code research to analyze the test failure above.\n\nDIAGNOSE:\n1. Examine the test code and its assertions.\n2.\n  Understand and clearly explain the intention and\n  reasoning of the test - what is it testing?\n3. Compare against the implementation code being tested\n4. Identify the root cause of failure\n\nDETERMINE:\nIs this a test that\n  needs updating or a real bug in the implementation?\n\nProvide your conclusion with evidence.",
      "caption": "Structured diagnosis forces sequential analysis and evidence-based conclusions",
      "speakerNotes": {
        "talkingPoints": "This diagnostic pattern applies techniques from Lesson 4: Chain-of-Thought sequential steps, constraints requiring evidence, structured format. The fenced code block preserves error formatting and prevents misinterpretation. 'Use the code research' forces codebase grounding. Numbered DIAGNOSE steps implement CoT—agents can't jump to conclusions. DETERMINE constrains to binary decision. 'Provide evidence' requires file paths and line numbers, not vague assertions. You can adapt this for performance issues or security vulnerabilities.",
        "timing": "3 minutes",
        "discussion": "When tests fail, do you have a systematic process? Or do you debug reactively?",
        "context": "This is the four-phase workflow (Research > Plan > Execute > Validate) specialized for debugging.",
        "transition": "Let's consolidate what we've learned..."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests ground agent code in your codebase; test quality determines implementation quality",
        "Use separate contexts for code, tests, and debugging to prevent self-deception",
        "Write sociable tests using real implementations; mock only external systems",
        "Build sub-30-second smoke tests and run after each task for fast feedback loops",
        "Use agent-driven testing for edge case discovery, then codify as regression tests"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles form a system: tests as documentation for agents → prevent shared assumptions through context separation → ensure tests actually exercise real code → maintain fast iteration → discover and prevent regressions. Each principle builds on Lessons 1-7 and connects to production practice at scale.",
        "timing": "2-3 minutes",
        "discussion": "Which of these principles is missing from your current testing workflow? What would change if you implemented them?",
        "context": "This lesson transforms testing from 'QA activity' to 'agent constraint system'—a critical operational difference.",
        "transition": "In Lesson 9, we'll review how to evaluate code quality before committing changes at scale..."
      }
    }
  ]
}
