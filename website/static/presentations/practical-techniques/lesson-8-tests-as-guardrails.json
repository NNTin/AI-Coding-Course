{
  "metadata": {
    "title": "Tests as Guardrails",
    "lessonId": "lesson-8-tests-as-guardrails",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Use tests to ground agent decisions",
      "Prevent specification gaming cycles",
      "Build effective smoke test suites",
      "Diagnose test failures systematically"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Tests as Guardrails",
      "subtitle": "How agents scale safely with test-driven constraints",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson addresses a critical problem: when agents refactor large codebases at velocity, small errors compound exponentially. Tests aren't just quality gates—they're constraint systems that define what agents cannot touch and documentation agents actually read to understand your codebase's edge cases and business logic.",
        "timing": "1 minute",
        "discussion": "Ask: What's the most expensive debugging session you've experienced after large code changes? How many files did you have to touch?",
        "context": "At scale (50+ file refactors), manual review of diffs becomes impossible. Test suites become your first line of defense against silent breakage.",
        "transition": "Let's start by understanding what tests actually communicate to agents."
      }
    },
    {
      "type": "concept",
      "title": "Tests as Documentation Agents Read",
      "content": [
        "Tests load into context during research phase",
        "Good tests ground decisions in concrete examples",
        "Edge cases (OAuth skips verification, timezone offsets) become constraints",
        "Bad tests pollute context with noise and false signals",
        "Quality of loaded tests determines grounding quality"
      ],
      "speakerNotes": {
        "talkingPoints": "Before agents write code, they research. They search for relevant files and read both implementations AND tests. Your test suite isn't aspirational—it's literal input to the LLM's reasoning. A test named 'test_works' with assertions on mock calls teaches the agent wrong patterns. A test showing 'deleted users are rejected with 403' teaches the correct pattern.",
        "timing": "3-4 minutes",
        "discussion": "Have students critique a test: 'test_login_success' with mocked password hashing. What will the agent learn from this? What should it learn instead?",
        "context": "In our Salesforce case study, refactoring code that loaded 50 poorly-named tests from context led to 23% higher rework rate than refactors with well-documented tests.",
        "transition": "Now let's see how to systematically discover what needs testing."
      }
    },
    {
      "type": "code",
      "title": "Research First: Discover Edge Cases",
      "language": "text",
      "code": "What edge cases does the OAuth flow handle?\n\nSpecific questions:\n1. Do verified users skip email validation?\n2. Do admin users bypass rate limits?\n3. What happens with deleted accounts?\n4. Are token expiry times enforced?\n5. What about concurrent sessions?",
      "caption": "Question-driven research loads implementation details into context before writing tests",
      "speakerNotes": {
        "talkingPoints": "Don't start by writing tests. Start by asking questions that force the agent to read your actual implementation. These questions are grounding directives—they make the agent search for real answers in your codebase instead of inventing generic test scenarios from training patterns.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What questions would you ask about your current product's payment handling before an agent refactors it?",
        "context": "This pattern applies across domains: database queries (concurrency, transactions), API endpoints (auth, rate limits), background jobs (retries, ordering).",
        "transition": "Once you've discovered edge cases, we need to prevent a dangerous cycle when code and tests are written together."
      }
    },
    {
      "type": "marketingReality",
      "title": "The Closed Loop Trap: Goodhart's Law",
      "metaphor": {
        "label": "The Illusion",
        "content": [
          "Agent writes code and tests in same session",
          "Both inherit identical assumptions",
          "Tests pass, confidence increases",
          "Silent bugs slip through CI/CD"
        ]
      },
      "reality": {
        "label": "What Actually Happens",
        "content": [
          "Code accepts zero quantities, tests verify zero works (both flawed)",
          "Specification gaming: agent weakens assertions to hit green",
          "Mutual reinforcement creates false correctness signal",
          "Agent doesn't question shared assumptions—defends prior decisions"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is Goodhart's Law: when a measure becomes the optimization target, it ceases to be a good measure. In this case, 'green tests' become the target, not correctness. Without safeguards, the agent generates mutually-compatible-but-wrong code and tests. Research shows ~1% of co-generated cycles fail this way, but at scale across thousands of tasks, these errors compound.",
        "timing": "3-4 minutes",
        "discussion": "Share real example: API endpoint accepting negative quantities, tests verifying negatives work. Who caught the bug? (A customer.)",
        "context": "Enterprise teams (Salesforce, Google) address this with enforcement: never allow code and test generation in the same conversation. Stateless conversations between phases are a feature, not a limitation.",
        "transition": "The solution is architectural: separate contexts for each phase."
      }
    },
    {
      "type": "concept",
      "title": "Three-Context Workflow: Breaking the Cycle",
      "content": [
        "Context A: Write code—research patterns, plan, execute, verify",
        "Context B: Write tests (fresh)—research requirements independently, no memory of implementation",
        "Context C: Triage failures (fresh)—objective analysis, no defense of prior decisions",
        "Each context applies Lesson 7 methodology: Research > Plan > Execute > Verify",
        "Stateless nature prevents assumption inheritance between phases"
      ],
      "speakerNotes": {
        "talkingPoints": "The critical insight: starting fresh for tests means the agent doesn't know what you implemented. It discovers test requirements from your codebase independently—reading existing tests, finding edge cases, deriving coverage from requirements. It can't defend flawed assumptions because it never made them.",
        "timing": "4-5 minutes",
        "discussion": "Walk through: What should Context B focus on if Context A is secret? (Reading requirements, existing tests, edge case patterns.)",
        "context": "Salesforce's automated root cause analysis—handling millions of daily test failures across customer codebases—uses this pattern. Each failure gets fresh diagnostic context without prior bias about who wrote code or tests.",
        "transition": "Before we tackle debugging, let's talk about what makes tests effective at catching real errors."
      }
    },
    {
      "type": "comparison",
      "title": "Sociable Tests vs Heavy Mocking",
      "left": {
        "label": "Heavily Mocked",
        "content": [
          "Stubs findByEmail(), verify(), create()",
          "Tests pass even when implementations break",
          "Agent can gut authentication logic",
          "False confidence from green tests",
          "Verifies calls, not behavior"
        ]
      },
      "right": {
        "label": "Sociable Unit Tests",
        "content": [
          "Real database, real password hashing, real tokens",
          "Any implementation breakage fails immediately",
          "Tests exercise actual code paths",
          "Fast feedback when agent changes behavior",
          "Verifies end-to-end behavior"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Sociable tests use real implementations for internal code. Only mock external systems (APIs, payment processors) that require keys or cost money. This gives agents immediate feedback when they break something. A heavily mocked test might pass while the agent silently breaks core logic.",
        "timing": "2-3 minutes",
        "discussion": "Review a production test: is it mocking too much? Can any mocks be replaced with real implementations without adding cost or complexity?",
        "context": "Kent Beck's Testing Without Mocks advocates for nullables—production code with an off switch—for in-memory infrastructure that replaces complex test doubles.",
        "transition": "Now let's talk about speed. Not all tests are created equal."
      }
    },
    {
      "type": "concept",
      "title": "Smoke Tests: Fast Feedback Loop",
      "content": [
        "Sub-30-second suite covering critical junctions",
        "Core user journey, auth boundaries, DB connectivity",
        "Not exhaustive—save edge cases for full suite",
        "Run automatically after each agent task",
        "Catch failures immediately while context is fresh"
      ],
      "speakerNotes": {
        "talkingPoints": "A 10-minute comprehensive test suite gets skipped during iteration. You make 20 changes, then discover the 5th one broke everything. With smoke tests running every 2 minutes, you know which change failed. This is the finest-grained feedback mechanism that prevents error compounding during rapid iteration.",
        "timing": "2-3 minutes",
        "discussion": "What's in your project's smoke test? Can you articulate the 3-5 critical paths it covers?",
        "context": "Codify this in your AGENTS.md or CLAUDE.md file (from Lesson 6) so agents automatically run smoke tests after completing each task. This becomes a habit, not a checklist item.",
        "transition": "Deterministic tests catch regressions. But what about edge cases you haven't imagined?"
      }
    },
    {
      "type": "concept",
      "title": "Agent-Based Testing: Non-Deterministic Exploration",
      "content": [
        "Agents simulate user behavior non-deterministically—different path each run",
        "Discovers edge cases humans didn't think to test",
        "Find race conditions, state machine bugs, validation gaps",
        "Unreliable for CI/CD regression testing (probabilistic paths)",
        "Excellent for discovery—then solidify into deterministic tests"
      ],
      "speakerNotes": {
        "talkingPoints": "Run the same exploration prompt twice, and the agent explores different state spaces. One iteration navigates the happy path, another accidentally discovers a race condition by clicking rapidly, a third tests unicode edge cases. This randomness is a feature for discovery—you find bugs that deterministic tests miss because you never wrote assertions for them.",
        "timing": "3 minutes",
        "discussion": "Have you ever found a bug that 'only happens sometimes'? Agent simulation can uncover these systematically.",
        "context": "Use MCP servers to give agents access to your product—browser automation (Chrome DevTools, Playwright), mobile testing, desktop automation. Agents can explore UX surface area you haven't thought to test.",
        "transition": "When tests fail—and they will—we need a systematic way to understand why."
      }
    },
    {
      "type": "code",
      "title": "Test Failure Diagnosis: Structured Approach",
      "language": "text",
      "code": "Use the code research to analyze\nthe test failure above.\n\nDIAGNOSE:\n1. Examine test code and assertions\n2. Explain the test intention—why\nthis test exists\n3. Compare against implementation\n4. Identify root cause\n\nDETERMINE:\nIs this a test update needed or\na real implementation bug?\n\nProvide conclusion with evidence\n(file paths, line numbers).",
      "caption": "Structured diagnosis prevents premature conclusions and ensures evidence-based decisions",
      "speakerNotes": {
        "talkingPoints": "This diagnostic prompt applies Chain-of-Thought from Lesson 4. Each element serves a purpose: 'Use code research' forces codebase grounding instead of hallucination. Numbered DIAGNOSE steps prevent jumping to conclusions. 'Explain the intention' (step 2) is critical—CoT forces articulation before diagnosis. 'DETERMINE binary decision' constrains to 'bug vs outdated test' instead of vague conclusions. 'Provide evidence' requires file paths and line numbers.",
        "timing": "3-4 minutes",
        "discussion": "Walk through a real test failure from your codebase. How would agents analyze it using this structure?",
        "context": "You can adapt this for performance issues, security vulnerabilities, or deployment failures by changing diagnostic steps while preserving structure: sequential CoT → constrained decision → evidence requirement.",
        "transition": "Let's consolidate what we've learned about testing with agents."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Tests are literal input to agent reasoning—write tests that explain 'why'",
        "Separate contexts for code, tests, and debugging prevent specification gaming",
        "Sociable tests catch real breakage; smoke tests prevent error compounding",
        "Agent-based testing discovers edge cases deterministic tests miss",
        "Structured diagnosis solves most test failures autonomously"
      ],
      "speakerNotes": {
        "talkingPoints": "Five core principles: (1) Your test suite teaches agents your constraints—quality matters enormously. (2) Stateless conversations between coding phases prevent mutual reinforcement of flawed assumptions. (3) Fast feedback loops prevent cascading failures. (4) Randomized agent testing discovers novel edge cases. (5) Systematic diagnosis turns test failures into learning opportunities.",
        "timing": "2-3 minutes",
        "discussion": "Which of these five principles will have the most impact on your current projects? Where would you start implementing?",
        "context": "These patterns scale from individual developers to enterprise codebases. Start with smoke tests and three-context workflows—both have immediate ROI.",
        "transition": "In the next lesson, we'll focus on reviewing agent output before deployment."
      }
    }
  ]
}
