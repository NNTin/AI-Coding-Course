{
  "metadata": {
    "title": "Code Review: The Validate Phase",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Review code in fresh context",
      "Identify probabilistic agent errors",
      "Iterate until green or diminishing returns",
      "Optimize PR descriptions for dual audiences"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Code Review: The Validate Phase",
      "subtitle": "Quality gates before shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Tests pass, implementation complete—but is it actually correct? This lesson covers the Validate phase: the systematic quality gate before shipping. We'll explore how to review code objectively, identify and fix probabilistic errors agents introduce, and optimize the review process for production environments.",
        "timing": "1 minute",
        "discussion": "Ask: Have you caught bugs in AI-generated code that tests didn't catch? What kind of bugs?",
        "context": "This bridges Lesson 8 (tests as guardrails) and production deployment. Code review catches what tests miss: logic subtleties, architectural fit, edge cases.",
        "transition": "Let's start with the fundamental principle: why reviewing in fresh context matters."
      }
    },
    {
      "type": "concept",
      "title": "Fresh Context Prevents Bias",
      "content": [
        "Agent defending decisions in same conversation",
        "Confirmation bias: only sees supporting evidence",
        "Fresh context agent analyzes objectively",
        "Stateless nature (Lessons 1–2) is advantage",
        "Separate review conversation breaks attachment"
      ],
      "speakerNotes": {
        "talkingPoints": "The core principle: an agent reviewing its own work in the same conversation will defend its prior decisions. It has invested tokens in those choices. Fresh context—a new conversation—forces objective analysis without attachment. This stateless property we learned in Lessons 1–2 becomes a strength here. We're deliberately using the agent's lack of memory to eliminate confirmation bias.",
        "timing": "3 minutes",
        "discussion": "Compare to human code review: do you review better when you wrote the code or when someone else did? Same principle applies to agents.",
        "context": "This is why ChatGPT isn't optimal for review—single conversation context. Claude Code CLI handles this with separate agent invocations.",
        "transition": "This principle applies universally. But the review process itself differs based on codebase type."
      }
    },
    {
      "type": "concept",
      "title": "Agent-Only vs Mixed Codebases",
      "content": [
        "Agent-only: AI maintains code exclusively",
        "Mixed: Humans and AI collaborate directly",
        "Same engineering standards apply (DRY, YAGNI)",
        "Mixed codebases need manual human review",
        "Tune project rules (Lesson 6) to guide style"
      ],
      "speakerNotes": {
        "talkingPoints": "Most production codebases are mixed: humans and AI both work directly with the code. They have different style preferences. Agents optimize for their training data; humans optimize for brevity and readability. Without explicit project rules guiding style, there's a mismatch. That's why Lesson 6 emphasizes project rules—they're not optional in mixed codebases. After review, you must manually verify the code meets human readability standards. This adds a step but prevents style drift.",
        "timing": "2-3 minutes",
        "discussion": "What's your codebase type? Does it have explicit style guidelines? Are they documented in your project rules file?",
        "context": "Agent-only codebases (rare in practice) optimize documentation and type annotations slightly more for agent navigability. Mixed codebases require hybrid optimization.",
        "transition": "Now let's look at the structured review process itself."
      }
    },
    {
      "type": "code",
      "title": "Review Prompt Structure",
      "language": "bash",
      "code": "You are a code reviewer. Review this TypeScript code:\n[CODEBLOCK]\n\nAnalyze for:\n- Logic bugs or edge cases\n- Architectural fit with existing patterns\n- Readability and maintainability\n- Test coverage adequacy\n\nProvide: file:line references for issues\nFormat: Critical/Major/Minor severity",
      "caption": "Structured review applies Lesson 4 principles",
      "speakerNotes": {
        "talkingPoints": "This template integrates techniques from Lesson 4: persona (code reviewer), structure (what to analyze), constraints (format requirements), grounding (file:line references). The key: evidence requirements force the agent to reference actual code, not statistical guesses. Without 'provide file:line references,' agents often hallucinate issues that don't exist.",
        "timing": "3-4 minutes",
        "discussion": "Compare this to a vague prompt: 'Is this code good?' vs this structured version. Which generates better reviews?",
        "context": "This template adapts to other review types: security audits (add 'common vulnerabilities'), performance reviews (add 'bottlenecks'), architectural reviews (add 'design patterns').",
        "transition": "But review rarely ends in one pass. Let's discuss iteration."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review Cycle",
      "content": [
        "First review: finds substantive issues",
        "Fix issues, re-run tests (Lesson 8)",
        "Review again in fresh context",
        "Continue: fix → validate → review",
        "Stop at green or diminishing returns"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is iterative. You review, the agent finds issues, you fix them, tests must still pass (this is critical—a 'fix' that breaks tests means the review was wrong), then review again in fresh context. Never let the same agent defend its own suggested fixes—fresh context prevents that bias. This cycle continues until you reach a green light or hit diminishing returns.",
        "timing": "2-3 minutes",
        "discussion": "Have you experienced a code review suggesting a 'fix' that broke tests? That's the agent hallucinating—trust your tests as the arbiter.",
        "context": "In production, typical review cycles: 1 iteration (green) to 2–3 iterations (good enough). Beyond that, costs exceed benefits.",
        "transition": "How do you know when to stop iterating?"
      }
    },
    {
      "type": "concept",
      "title": "Recognizing Diminishing Returns",
      "content": [
        "Nitpicking: trivial style preferences",
        "Hallucinations: inventing non-existent issues",
        "Review-induced failures: breaks passing tests",
        "Excessive cost: 4+ iterations for minor fixes",
        "Verdict: trust tests, ship the code"
      ],
      "speakerNotes": {
        "talkingPoints": "At some point, further iteration wastes time. The agent might suggest renaming variables, inventing edge cases that don't exist in your domain, or 'improvements' that break things. This is the agent's probabilistic nature: it guesses, and guesses aren't always right. When review findings become trivial or cause test failures, stop. Your tests are the objective arbiter—they tell the truth. Further AI review risks degrading quality while consuming engineering time.",
        "timing": "2-3 minutes",
        "discussion": "Tell me about a recent code review where you thought, 'This isn't worth fixing.' That's diminishing returns.",
        "context": "This requires operator judgment: when to trust the tests over the agent. As teams mature, they develop intuition for this.",
        "transition": "Now let's shift to pull requests: they serve both human and AI reviewers."
      }
    },
    {
      "type": "comparison",
      "title": "How Humans and AI Process PRs",
      "left": {
        "label": "Human Reviewers",
        "content": [
          "Scan quickly for context",
          "Value concise summaries (1–3 paragraphs)",
          "Infer meaning from context",
          "Want to understand 'why' and business value"
        ]
      },
      "right": {
        "label": "AI Review Assistants",
        "content": [
          "Parse content chunk-by-chunk",
          "Need explicit structure and terminology",
          "Struggle with vague pronouns",
          "Require detailed technical context (files, patterns)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Humans and AI consume information fundamentally differently. Humans are fast, contextual, inferential. AI is sequential, literal, structure-dependent. Traditional PR descriptions optimize for one audience—too verbose for humans, too vague for AI. The solution: generate both descriptions in a coordinated workflow.",
        "timing": "2-3 minutes",
        "discussion": "Think about the last PR you reviewed with both a human and GitHub Copilot. Did the AI review make sense? Did it miss context?",
        "context": "This is why Claude Code uses sub-agents (Lesson 5) to explore commits without bloating main context. Humans can scan summaries; AI assistants need full grounding.",
        "transition": "How do we generate descriptions that serve both audiences?"
      }
    },
    {
      "type": "codeComparison",
      "title": "Dual-Optimized PR Descriptions",
      "leftCode": {
        "label": "Human-Optimized",
        "language": "text",
        "code": "## Summary\nAdded authentication middleware\nfor user sessions.\n\n## Breaking Changes\nNone\n\n## Testing\nAdded unit tests for\nJWT validation."
      },
      "rightCode": {
        "label": "AI-Optimized",
        "language": "text",
        "code": "## Technical Context\nModified: src/middleware/auth.ts\nAdded JWT token validation.\nImplements RS256 signature verification.\nMatches existing passport.js patterns\nin src/app.ts:142.\n\nFiles changed: 3\nTests added: 8\nNo breaking changes to exported APIs."
      },
      "speakerNotes": {
        "talkingPoints": "The human description answers 'what changed and why'—readable at a glance. The AI description provides technical grounding: specific file paths, pattern references, architectural context. Neither is 'better'—they serve different purposes. Humans scan the left side; AI assistants consume the right side. Generate both in a coordinated workflow: exploratory sub-agent discovers patterns, then produces both descriptions simultaneously.",
        "timing": "3-4 minutes",
        "discussion": "Have you seen AI review tools miss context from vague PR descriptions? This solves that.",
        "context": "Sub-agents (Lesson 5) preserve main context: exploration happens separately, findings returned concisely. Without this, exploring 20 changed files bloats context by 40K+ tokens.",
        "transition": "When you're receiving a PR with these dual descriptions, how do you use them?"
      }
    },
    {
      "type": "concept",
      "title": "Consuming Dual PR Descriptions",
      "content": [
        "Read human summary first: understand 'why'",
        "Scan for breaking changes and key files",
        "Feed AI description to review assistant",
        "AI gets comprehensive technical context",
        "Reduced hallucinations, improved accuracy"
      ],
      "speakerNotes": {
        "talkingPoints": "Workflow: first read the human description to understand business value and get your mental model oriented. Scan for breaking changes and architecture-critical modifications. Then feed the AI-optimized description to your review assistant (Copilot, Codex, Claude Code). It has the structured context it needs for accurate analysis. This prevents the AI from hallucinating context—it knows the exact files, architectural patterns, and constraints.",
        "timing": "2 minutes",
        "discussion": "Next time you review a PR, try this workflow. Feed the AI both descriptions, but explicitly reference the AI-optimized one.",
        "context": "This is production practice: GitHub and most CI systems don't yet generate dual descriptions. Teams that do this manually report 40–60% fewer false positives in AI review.",
        "transition": "Let's look at one more optimization: Chain of Draft."
      }
    },
    {
      "type": "code",
      "title": "Chain of Draft (CoD) for Reviews",
      "language": "text",
      "code": "Think step by step, but keep\nonly a minimal draft for each step.\nMax 5 words per step.\n\nReturn final assessment after ####\nseparator at the end.\n\n#### [FINAL VERDICT]",
      "caption": "Efficient reasoning: CoT benefits, CoD speed",
      "speakerNotes": {
        "talkingPoints": "Chain of Thought (CoT) improves reasoning but consumes tokens. Chain of Draft (CoD) is an optimization: think through steps structured and carefully, but keep each step's draft minimal—5 words max—then return the final assessment after a separator. You get the reasoning benefits of CoT with reduced token consumption and faster responses. For reviews, this is especially useful: the agent reasons carefully through each concern but doesn't verbose each one.",
        "timing": "2-3 minutes",
        "discussion": "Try this on your next review. Measure token count compared to traditional CoT.",
        "context": "Research: original paper by Wei et al. Shows CoD maintains reasoning quality while reducing overhead by 30–50%.",
        "transition": "Let's bring it together: the complete review methodology."
      }
    },
    {
      "type": "codeExecution",
      "title": "Complete Review Workflow",
      "steps": [
        {
          "line": "Code complete, tests passing",
          "highlightType": "summary",
          "annotation": "Entry point: implementation validated with tests"
        },
        {
          "line": "Engineer initiates: 'Review this code for\nlogic bugs and architectural fit'",
          "highlightType": "human",
          "annotation": "Clear task with scope and evidence requirements"
        },
        {
          "line": "Review agent analyzes: checks logic,\nedge cases, patterns against codebase",
          "highlightType": "prediction",
          "annotation": "Fresh context provides objective perspective"
        },
        {
          "line": "Review findings: file:line refs,\nseverity levels, specific issues",
          "highlightType": "feedback",
          "annotation": "Grounded review—no hallucinations without evidence"
        },
        {
          "line": "Engineer evaluates: fixes substantive issues,\nrejects nitpicks or hallucinations",
          "highlightType": "human",
          "annotation": "Operator judgment: trust tests as arbiter"
        },
        {
          "line": "Tests re-run: must still pass",
          "highlightType": "execution",
          "annotation": "Validation: 'fix' that breaks tests was wrong"
        },
        {
          "line": "If major issues remain, review\nagain in fresh context",
          "highlightType": "prediction",
          "annotation": "Iterate until green or diminishing returns"
        },
        {
          "line": "Green light or acceptable trade-offs reached",
          "highlightType": "summary",
          "annotation": "Decision: ship or continue iteration"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the complete cycle: implementation → review in fresh context → evaluation → testing → iteration decision. Notice the operator judgment at step 5: you decide which findings matter. Review is probabilistic; you're the arbiter. The tests are your objective truth—a 'fix' that breaks them means the review was wrong. The fresh context at step 7 prevents the same agent from defending its prior suggestions.",
        "timing": "4-5 minutes",
        "discussion": "Walk through a recent code review. Did you follow this flow? What would change?",
        "context": "This applies to all review types: security reviews, performance analysis, architectural validation. The structure is universal; only the analysis criteria change.",
        "transition": "Let's close with the key principles you'll apply immediately."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Principles",
      "content": [
        "Review in fresh context: eliminates confirmation bias",
        "Iteration stops at green or diminishing returns",
        "Operator judgment is essential: tests are arbiter",
        "Dual-optimized descriptions serve human and AI",
        "Evidence requirements force grounded analysis"
      ],
      "speakerNotes": {
        "talkingPoints": "Five principles to take into production: First, always review in fresh context—this is non-negotiable for objective analysis. Second, iterate systematically but recognize when further review wastes time. Third, you make the final call—trust your tests, reject bad suggestions. Fourth, when writing PRs, generate dual descriptions: one for humans, one for AI assistants. Fifth, demand evidence: file:line references prevent hallucinations. Apply these immediately in your team's workflow.",
        "timing": "2 minutes",
        "discussion": "Which of these principles does your current review process already follow? Which is missing?",
        "context": "These aren't theoretical—they come from production practice at teams using Claude Code and similar tools at scale.",
        "transition": "Lesson 10 takes everything we've learned and applies it to debugging: finding and fixing problems under uncertainty."
      }
    }
  ]
}
