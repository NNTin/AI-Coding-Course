{
  "metadata": {
    "title": "Lesson 9: Reviewing Code",
    "lessonId": "lesson-9-reviewing-code",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Apply systematic code review process",
      "Prevent confirmation bias through fresh context",
      "Structure reviews with strategic prompting",
      "Optimize PRs for dual audiences"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Lesson 9: Reviewing Code",
      "subtitle": "The final quality gate before shipping",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to the validation phase—the critical quality gate that separates working code from production-ready code. Your tests pass. Your agent executed the plan. Now you need to answer the hard question: is this code actually correct? Code review catches the probabilistic errors that agents inevitably introduce: subtle logic bugs, architectural mismatches, edge cases handled incorrectly, patterns that don't quite fit your codebase.",
        "timing": "1 minute",
        "discussion": "Ask students: 'Who here has shipped code that passed tests but had hidden bugs? What caught them in production?'",
        "context": "Code review is where engineering discipline meets AI capability. It's the moment where you, as the senior engineer, validate that the AI's output meets your standards.",
        "transition": "Let's start with the most critical principle that makes AI code review effective: fresh context."
      }
    },
    {
      "type": "concept",
      "title": "Fresh Context: The Critical Principle",
      "content": [
        "Review in separate conversation from implementation",
        "Prevents confirmation bias—agent won't defend prior decisions",
        "Stateless analysis—agent analyzes objectively",
        "Catches errors that iterative cycles missed"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the core insight that makes AI code review work: an agent reviewing its own work in the same conversation will defend its decisions. It has context of why it made those choices and will rationalize them. An agent in a fresh context has no attachment to prior decisions—it analyzes purely based on the code itself and your specifications.",
        "timing": "2-3 minutes",
        "discussion": "Draw parallel to human code review: wouldn't you be tempted to defend your own code? The fresh context removes that temptation entirely.",
        "context": "This principle applies to your own code review too. Reading code days later in a fresh mental state catches bugs that your implementation mindset glossed over.",
        "transition": "Now, the actual review process depends on what kind of codebase you're working in. Let me explain the critical distinction."
      }
    },
    {
      "type": "comparison",
      "title": "Codebase Strategy: Agent-Only vs Mixed",
      "left": {
        "label": "Agent-Only Codebases",
        "content": [
          "AI maintains code exclusively",
          "Optimize style toward AI clarity",
          "Explicit type annotations required",
          "Architecture context files critical"
        ]
      },
      "right": {
        "label": "Mixed Codebases",
        "content": [
          "Humans and AI collaborate on code",
          "Balance clarity for both audiences",
          "Add manual review step pre-commit",
          "Most production codebases (critical)"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The engineering standards—DRY, YAGNI, architecture, maintainability—apply everywhere. What differs is the review emphasis. In mixed codebases, where both humans and AI touch the code, you must add an explicit manual review step before committing. Without this, agents generate code that follows patterns from their training data, not necessarily your team's readability standards. This is non-negotiable.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Which category is your production codebase?' Most will answer 'mixed'. That's why the manual review step matters so much.",
        "context": "The key difference: in mixed codebases, tune your project rules (Lesson 6) to guide agents toward your style, then verify the output meets those expectations. It's a two-step process.",
        "transition": "Whether agent-only or mixed, the review process follows the same template. Let me show you the structured prompt pattern that powers effective reviews."
      }
    },
    {
      "type": "concept",
      "title": "The Review Process: Four Phases",
      "content": [
        "Research: Understand implementation intent and context",
        "Plan: Structure the review against those objectives",
        "Execute: Run code analysis and pattern validation",
        "Validate: Decide to ship, fix, or regenerate"
      ],
      "speakerNotes": {
        "talkingPoints": "You've seen this four-phase methodology before (Lesson 3). It applies to code review too. Research your implementation context—why was this code written? What architectural patterns should it follow? Plan your review by establishing clear criteria. Execute by actually analyzing the code. Validate by making a firm decision rather than endless iteration.",
        "timing": "2 minutes",
        "discussion": "This mirrors the exact workflow from Lesson 3. Consistent methodology across all engineering tasks.",
        "context": "The power of this framework is that it forces you to be systematic. Each phase has a purpose; skipping phases leads to incomplete reviews.",
        "transition": "Now let's dive into the actual review prompt template. This demonstrates multiple prompting techniques from Lessons 4 and 5."
      }
    },
    {
      "type": "code",
      "title": "Review Prompt Template Pattern",
      "language": "text",
      "code": "You are a code reviewer specializing in production systems.\n\nReview this code:\n- Architecture: Does it match the patterns in auth.ts:30?\n- Maintainability: Can a new engineer understand it?\n- Edge cases: What happens at boundaries?\n- Testing: Do tests cover the happy path AND failures?\n\nProvide file:line references for all findings.\nThink step by step, keep drafts to 5 words max.\nReturn assessment at end after ####.",
      "caption": "Structured review persona with grounding and constraints",
      "speakerNotes": {
        "talkingPoints": "This template integrates techniques from Lesson 4: Prompting 101. Notice the persona (code reviewer), specific review criteria, grounding (reference actual code patterns), evidence requirements (file:line references), and a constraint on thinking (Chain of Draft—5 words max per step). Each element exists for a reason.",
        "timing": "3-4 minutes",
        "discussion": "Walk through each element: 'Why does the persona matter? Why reference specific files instead of vague architecture? Why Chain of Draft instead of free-form thinking?'",
        "context": "This template is adaptable. Use the same structure for security reviews, performance analysis, or architectural validation. The pattern works; only the criteria change.",
        "transition": "The template covers single reviews, but in practice, review is iterative. Let me show you how to know when to stop iterating."
      }
    },
    {
      "type": "concept",
      "title": "Iterative Review: When to Stop",
      "content": [
        "Tests passing + review green = Ship",
        "Tests passing + nitpicking = Ship (diminishing returns)",
        "Tests fail after fixes = Review was probably wrong",
        "4+ iterations on minor issues = Stop and ship"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is probabilistic—the agent can be wrong. Tests are your objective arbiter. If a review 'fix' breaks passing tests, the review was probably wrong and you should reject it. Diminishing returns appear as nitpicking (trivial style preferences), hallucinations (inventing non-existent issues), or excessive cost. At that point, trust your tests and ship.",
        "timing": "2-3 minutes",
        "discussion": "This is about judgment calls: when does further AI review become counterproductive? Help students recognize the signs of diminishing returns.",
        "context": "In production, you'll face time constraints. Further AI review must provide value exceeding its cost. That's the fundamental tradeoff.",
        "transition": "So far we've covered reviewing code you wrote. Now let's shift to reviewing code through pull requests—this involves a different audience dynamic."
      }
    },
    {
      "type": "concept",
      "title": "Dual Audiences: Human vs AI Reviewers",
      "content": [
        "Humans scan quickly, infer from context, need 1-3 paragraph summary",
        "AI reads chunk-by-chunk, needs explicit structure",
        "Traditional PRs optimize for one or the other",
        "Solution: Generate both in coordinated workflow"
      ],
      "speakerNotes": {
        "talkingPoints": "Pull requests serve two audiences with fundamentally different information processing. A human reviewer wants to understand the 'why' and business value at a glance. They can infer meaning from context and skip details. An AI review assistant reads text sequentially, struggles with vague pronouns, and needs explicit architecture patterns and file paths. Most PR descriptions optimize for one audience, leaving the other with incomplete context.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Have you ever written a PR description that was too detailed for humans or too vague for AI review assistants?'",
        "context": "This is a practical problem in production: you want both human reviewers and AI tools (Copilot, Claude Code) to understand your changes effectively.",
        "transition": "The advanced prompt pattern shows how to generate both optimizations in a single coordinated workflow."
      }
    },
    {
      "type": "codeExecution",
      "title": "Dual-Optimized PR Generation Workflow",
      "steps": [
        {
          "line": "Human specifies: 'Generate dual PR descriptions for this changeset'",
          "highlightType": "human",
          "annotation": "Engineer requests both optimizations with explicit workflow"
        },
        {
          "line": "Sub-agent spawns to explore git history using task tool",
          "highlightType": "execution",
          "annotation": "Context conservation: separate agent prevents main context bloat"
        },
        {
          "line": "Sub-agent returns synthesized findings from 20-30 changed files",
          "highlightType": "feedback",
          "annotation": "Filtered summary replaces raw 40K+ token diffs"
        },
        {
          "line": "Main agent learns architecture via ChunkHound semantic search",
          "highlightType": "execution",
          "annotation": "Grounding: agent understands codebase patterns before writing"
        },
        {
          "line": "Agent researches PR best practices via ArguSeek",
          "highlightType": "execution",
          "annotation": "Multi-source grounding from external and internal knowledge"
        },
        {
          "line": "Agent generates human-optimized summary (1-3 paragraphs)",
          "highlightType": "prediction",
          "annotation": "Concise, scannable, focuses on business value"
        },
        {
          "line": "Agent generates AI-optimized description with explicit structure",
          "highlightType": "prediction",
          "annotation": "Technical context, file paths, architectural patterns enumerated"
        },
        {
          "line": "Both outputs ready for review workflow",
          "highlightType": "summary",
          "annotation": "Coordinated generation serves both audiences effectively"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This workflow demonstrates multiple techniques from Lessons 4, 5, and 7. Sub-agents conserve context (Lesson 5) by letting a separate agent explore git history, returning only synthesized findings. Multi-source grounding combines architecture discovery and external research. Structured prompting directs the agent toward dual outputs. The result: human reviewers get scannable summaries, AI assistants get comprehensive technical context.",
        "timing": "4-5 minutes",
        "discussion": "Highlight the sub-agent pattern: 'This is unique to Claude Code CLI. Other tools require sequential prompts. Why does that matter? Because context matters.' Walk through context conservation.",
        "context": "This pattern is production-ready. Use it in your PR workflow to generate both descriptions automatically, ensuring both audiences get what they need.",
        "transition": "Now let's talk about consuming these dual descriptions when you're reviewing a PR yourself."
      }
    },
    {
      "type": "concept",
      "title": "Reviewing PRs: Consume Both Descriptions",
      "content": [
        "Read human description first for business context",
        "Scan for breaking changes and key affected files",
        "Feed AI description to your review assistant",
        "AI gets comprehensive technical context for accurate analysis"
      ],
      "speakerNotes": {
        "talkingPoints": "When you're on the receiving end of a PR with dual descriptions, use them strategically. The human-optimized description gives you the 'why'—the business value and intent. Read that first to form your mental model. Then, feed the AI-optimized description to your review assistant (GitHub Copilot, Claude Code, etc). This gives the AI the grounding it needs for accurate analysis without hallucinations.",
        "timing": "2 minutes",
        "discussion": "This is the workflow: human context first, then AI analysis informed by comprehensive technical grounding.",
        "context": "You're not replacing human judgment; you're equipping your AI reviewer with the information needed to make better predictions.",
        "transition": "Let me show you the specific prompt pattern for this review workflow."
      }
    },
    {
      "type": "code",
      "title": "PR Review Prompt Pattern",
      "language": "text",
      "code": "Review this PR using the AI-optimized context provided.\n\nAssess step by step:\n1. Architecture fit with existing patterns\n2. Test coverage for happy and sad paths\n3. Breaking changes or regressions\n4. Reusability: Refactoring opportunities?\n\nSummary: [One sentence verdict]\nStrengths: [2-3 items]\nIssues: [By severity: Critical/Major/Minor]\nDecision: [APPROVE/REQUEST CHANGES/REJECT]",
      "caption": "Structured review with clear decision framework",
      "speakerNotes": {
        "talkingPoints": "This pattern provides structure—the AI knows exactly what to evaluate and how to present findings. One-sentence verdict first, then evidence. Critical issues separated from minor nitpicks. Clear decision at the end prevents ambiguity. This is Chain of Draft in action: think through the analysis, but keep outputs concise and structured.",
        "timing": "2-3 minutes",
        "discussion": "Compare to free-form PR reviews: 'What's the difference in accuracy and clarity?'",
        "context": "The decision field (APPROVE/REQUEST CHANGES/REJECT) forces a binary choice. No ambiguous 'seems fine but...' reviews.",
        "transition": "We've covered the technical process. Let me introduce an optimization technique that makes reviews more efficient."
      }
    },
    {
      "type": "concept",
      "title": "Chain of Draft: CoT Optimization",
      "content": [
        "Chain of Thought (CoT) ensures structured reasoning",
        "Chain of Draft (CoD) adds efficiency constraint",
        "Keep each draft step to 5 words maximum",
        "Return assessment after separator for quick scanning"
      ],
      "speakerNotes": {
        "talkingPoints": "Chain of Draft is an optimization of Chain of Thought that we introduced in the review template. Instead of verbose step-by-step explanations, CoD instructs the LLM to think through each step but keep the draft minimal—5 words at most. Then return the final assessment after a clear separator (####). You get the same reasoning benefits as CoT—structured analysis, fewer hallucinations—but with reduced token consumption and faster response times. For code review, this means faster feedback cycles without sacrificing accuracy.",
        "timing": "2-3 minutes",
        "discussion": "Efficiency matters: 'How much time and cost is saved with CoD vs verbose CoT when reviewing large changesets?'",
        "context": "This is a practical technique you'll use repeatedly. Learn Prompting and the original research paper have deeper dives.",
        "transition": "We've covered the full code review process. Let me summarize the key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways: Code Review Mastery",
      "content": [
        "Review in fresh context—prevents confirmation bias and attachment to prior decisions",
        "Apply four-phase methodology—Research, Plan, Execute, Validate applies to reviews too",
        "Use structured prompts with Chain of Draft—efficiency without sacrificing reasoning",
        "Iterate to green light OR diminishing returns—tests are your objective arbiter",
        "Generate dual-optimized PRs—humans need scannable summaries, AI needs explicit structure",
        "Leverage sub-agents for context conservation—separate agents prevent token bloat",
        "Evidence requirements force grounding—file:line references ensure reviews are based on actual code"
      ],
      "speakerNotes": {
        "talkingPoints": "Code review is the validation phase—where probabilistic AI output meets engineering standards. Fresh context removes bias. Structured prompts ensure complete analysis. Iteration with clear stopping conditions prevents waste. Dual-optimized PRs serve both human and AI reviewers. These techniques combine into a comprehensive code review workflow that you can apply immediately to production codebases.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Which of these techniques will have the biggest impact on your current workflow?' Have students identify specific applications.",
        "context": "You now have a complete framework: planning (Lesson 7), testing (Lesson 8), and reviewing (Lesson 9) with AI. This is production-ready methodology.",
        "transition": "Next lesson: Debugging with AI. When tests fail, how do you systematically find the root cause?"
      }
    }
  ]
}
