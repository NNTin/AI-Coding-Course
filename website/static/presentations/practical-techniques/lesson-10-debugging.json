{
  "metadata": {
    "title": "Debugging with AI Agents",
    "lessonId": "lesson-10-debugging",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Demand reproducible proof for all fixes",
      "Leverage AI for log pattern analysis",
      "Build closed-loop debugging workflows",
      "Place agents inside failing environments"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Debugging with AI Agents",
      "subtitle": "Evidence, Not Speculation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to debugging with AI agents. The core shift from traditional debugging is moving from 'what do you think is wrong?' to 'prove the bug exists, then prove your fix works.' This lesson covers the techniques for building verifiable debugging workflows.",
        "timing": "1 minute",
        "discussion": "Ask students: What's your current debugging process when code fails in production?",
        "context": "Most senior engineers rely on intuition and pattern recognition. AI agents excel at systematic investigation but need structure and proof requirements.",
        "transition": "Let's start with the fundamental principle: evidence over speculation."
      }
    },
    {
      "type": "concept",
      "title": "The Debugging Shift",
      "content": [
        "Move from 'what do you think is wrong?' to 'prove it'",
        "AI excels at systematic investigation with concrete data",
        "AI fails spectacularly when forced to speculate",
        "Require before/after evidence at every step",
        "Never accept fixes without reproducible proof"
      ],
      "speakerNotes": {
        "talkingPoints": "Traditional debugging is inherently speculative—you describe a symptom and hope the fix works. With AI agents, we reverse this. We demand evidence of the bug, systematic investigation of the cause, and proof that the fix resolves the issue. This isn't about not trusting the agent; it's about building confidence through verification.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Have you ever applied a fix that seemed right but broke something else?' This happens because fixes lack verification. With AI agents, we build verification into the workflow.",
        "context": "In production environments, an unverified fix can be worse than no fix. A common pattern: agent suggests optimizing a query that turns out to be called millions of times, introducing a bottleneck. Verification catches this immediately.",
        "transition": "Before jumping to logs or reproduction, we need to understand the system. Let's look at code inspection."
      }
    },
    {
      "type": "concept",
      "title": "Code Inspection: Before Logs",
      "content": [
        "Understand architecture and execution flow first",
        "Use semantic code search for relevant components",
        "Focus on critical paths, not every line",
        "Ask agent to trace request paths and data flow",
        "Identify edge cases from code structure"
      ],
      "speakerNotes": {
        "talkingPoints": "Most engineers jump straight to logs when a bug occurs. But logs show symptoms, not causes. Code inspection reveals the architecture and execution flow—understanding what *should* happen before analyzing what actually happened. The agent's code-based analysis often reveals assumptions you missed or edge cases buried in conditional logic.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'When you see a stack trace, do you immediately check the logs? What's missing?' Answer: Context about what the code was supposed to do and what assumptions it made.",
        "context": "Example: A race condition in microservices won't show up in normal logs—you need to understand the concurrency model first. Code inspection identifies shared state and lock patterns; logs confirm the race happened.",
        "transition": "Once you understand the architecture, logs become much more valuable. Let's talk about what AI can do with logs."
      }
    },
    {
      "type": "concept",
      "title": "Log Analysis: AI's Superpower",
      "content": [
        "AI excels processing chaos humans can't parse",
        "Spot patterns across inconsistent log formats",
        "Correlate cascading errors in microservices",
        "Identify timing patterns indicating race conditions",
        "Works with raw logs—doesn't need perfect structure"
      ],
      "speakerNotes": {
        "talkingPoints": "This is where AI has the biggest advantage over humans. Thousands of log lines across multiple services with inconsistent formatting? That's exactly where AI's pattern recognition excels. What takes senior engineers days of correlation happens in minutes. The messier the logs, the more AI's advantage grows.",
        "timing": "4-5 minutes",
        "discussion": "Ask: 'How many hours have you spent correlating logs across services?' Most engineers answer 'way too many.' That's the use case for AI.",
        "context": "Real example: A SaaS platform had intermittent payment processing failures. Engineers spent 3 days correlating logs from payment service, database, and queue service. AI analyzed the same logs in 10 minutes, identified a specific ordering issue when high volume hit the queue during database maintenance window. The pattern was scattered across thousands of lines in different services.",
        "transition": "Logs are powerful when you have them. But sometimes you need to create evidence. That's where reproduction scripts come in."
      }
    },
    {
      "type": "concept",
      "title": "Reproduction Scripts: Code is Cheap",
      "content": [
        "AI generates complex environments in minutes",
        "Docker configs, database snapshots, mock services",
        "Eliminates ambiguity with verifiable test cases",
        "Captures full context: state, API responses, config",
        "Turns tedious setup into simple prompts"
      ],
      "speakerNotes": {
        "talkingPoints": "When code inspection and logs aren't sufficient, you need bulletproof evidence. Reproduction scripts create that evidence. The key insight: environments that take humans hours to set up take AI minutes to generate. K8s clusters, database snapshots, mock APIs, state initialization—AI generates all of it trivially.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'How long would it take you to write a script that sets up your entire production environment locally?' Most answer '4-8 hours.' AI does this in 10 minutes.",
        "context": "Example: A complex authentication bug that only occurs with specific JWT claims and expired tokens. Human approach: manually construct test tokens, set up mock services, figure out database seeding. AI approach: one prompt, 5 minutes later you have a complete reproduction script.",
        "transition": "Reproduction scripts are valuable, but the real power comes when you can test fixes inside these environments. That's closed-loop debugging."
      }
    },
    {
      "type": "comparison",
      "title": "Open-Loop vs Closed-Loop Debugging",
      "left": {
        "label": "Open-Loop",
        "content": [
          "Agent researches code and online issues",
          "Proposes solution based on analysis",
          "Reports: 'Try adding this fix'",
          "Engineer must verify manually",
          "No feedback to agent on success/failure"
        ]
      },
      "right": {
        "label": "Closed-Loop",
        "content": [
          "Agent researches and forms hypothesis",
          "Applies fix and re-runs reproduction",
          "Observes results: passes/fails",
          "Reports: 'Fixed and verified'",
          "Iterates if verification fails"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The difference is verification. Open-loop agents are consultants: they analyze and recommend. Closed-loop agents are investigators: they analyze, test, and prove. Closed-loop is where AI debugging becomes exponentially more powerful because the agent gets immediate feedback on whether its reasoning actually works.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'When an agent tells you to add validation to jwt.ts:67, do you test it immediately or do you trust it?' Most engineers test it. Closed-loop means the agent tests it for you.",
        "context": "In production, open-loop debugging is risky. The agent can't see the effects of its fix in the actual environment. Closed-loop gives you confidence because the agent has already verified the fix works.",
        "transition": "Closed-loop debugging requires environment access. Let's look at the BUILD → REPRODUCE → PLACE → INVESTIGATE → VERIFY workflow."
      }
    },
    {
      "type": "codeExecution",
      "title": "The Closed-Loop Debugging Workflow",
      "steps": [
        {
          "line": "BUILD: Create reproducible environment\n(Docker container, database snapshot, config)",
          "highlightType": "human",
          "annotation": "Engineer specifies what environment reproduces the bug"
        },
        {
          "line": "REPRODUCE: Verify bug manifests consistently\n(Run request, capture logs, confirm failure)",
          "highlightType": "execution",
          "annotation": "Concrete evidence the bug exists"
        },
        {
          "line": "PLACE: Give agent tool access in environment\n(CLI access, not just code inspection)",
          "highlightType": "human",
          "annotation": "Agent can execute commands inside failing environment"
        },
        {
          "line": "INVESTIGATE: Agent analyzes runtime behavior\n(Execute diagnostics, inspect responses, research code)",
          "highlightType": "prediction",
          "annotation": "Agent forms hypothesis based on evidence"
        },
        {
          "line": "Agent applies fix to code\n(Edit file, update configuration)",
          "highlightType": "execution",
          "annotation": "Deterministic code change"
        },
        {
          "line": "VERIFY: Agent re-runs reproduction\n(Execute diagnostic, capture output)",
          "highlightType": "execution",
          "annotation": "Test the fix"
        },
        {
          "line": "Bug resolved? → Report verified fix\nBug remains? → Form new hypothesis, iterate",
          "highlightType": "summary",
          "annotation": "Feedback loop: proof or next hypothesis"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This workflow is the core of AI-assisted debugging. Each step requires concrete evidence before moving forward. BUILD creates the environment. REPRODUCE proves the bug exists. PLACE gives the agent runtime access. INVESTIGATE combines code research, runtime diagnostics, and knowledge of similar bugs. VERIFY proves the fix works. If verification fails, the agent iterates with a new hypothesis.",
        "timing": "6-7 minutes",
        "discussion": "Walk through a concrete example. For an authentication bug: BUILD creates a Docker container with app + database. REPRODUCE tries the failing request. PLACE gives agent shell access. INVESTIGATE reads auth code, checks logs, researches JWT vulnerabilities. Agent applies fix. VERIFY re-runs request and confirms 401 instead of 500.",
        "context": "This workflow scales from simple bugs (missing validation) to complex ones (race conditions in distributed systems). The key is that each step builds on evidence, not speculation.",
        "transition": "Closed-loop debugging requires CLI agents. Let's understand why IDE agents have limitations."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs IDE Agents",
      "left": {
        "label": "IDE Agents",
        "content": [
          "Tied to local development machine",
          "Access to code and file system",
          "Can't run production environments",
          "Can't execute in remote servers",
          "Limited to local testing"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Work anywhere you have shell access",
          "Inside Docker containers",
          "On remote servers and CI/CD pipelines",
          "On production instances",
          "Complete debugging environment access"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is a critical distinction. IDE agents (VS Code Copilot, JetBrains AI) are excellent for code generation and local testing. But they can't run inside your Docker container, on a remote server, or in a CI/CD pipeline. CLI agents (Claude Code, GitHub Copilot CLI) can execute anywhere you can open a terminal. For debugging production issues, CLI agents are essential.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How would you debug a bug that only occurs in your staging environment?' With IDE agents, you'd need to reproduce it locally (often impossible). With CLI agents, you give them access to staging directly.",
        "context": "Real scenario: A memory leak in a long-running service. It only manifests after 48 hours of production load. No way to reproduce locally. CLI agent can connect to the running service in staging, trigger memory diagnostics, analyze heap dumps, and verify fixes—all without leaving the remote environment.",
        "transition": "Sometimes you can't access the failing environment at all—customer deployments, locked-down production. That's where remote diagnosis comes in."
      }
    },
    {
      "type": "concept",
      "title": "Remote Diagnosis: When Access is Limited",
      "content": [
        "Ground yourself in codebase architecture first",
        "Research known issues and similar bugs",
        "Generate ranked hypotheses from evidence",
        "Create targeted diagnostic scripts",
        "Trade developer time for compute time"
      ],
      "speakerNotes": {
        "talkingPoints": "Sometimes you face limited information and no iteration cycle: customer deployments you can't access, locked-down production, air-gapped systems. This is where AI's probabilistic reasoning becomes a feature. The agent forms ranked hypotheses based on architectural understanding and known issues, then generates diagnostic scripts that collect targeted evidence for each hypothesis.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Have you ever had to debug a production issue where you couldn't directly access the system?' This is common in enterprise environments. Remote diagnosis is your tool.",
        "context": "Example: A customer reports 'the API is slow sometimes.' Limited information. No direct access. Traditional approach: ask for logs, wait for response, analyze, ask for more data. AI approach: understand the architecture, generate comprehensive diagnostic script (checks database queries, CPU/memory, network latency, GC, everything), customer runs it, agent analyzes output, identifies root cause. What takes days becomes hours.",
        "transition": "The key insight: generate thorough diagnostics trivially. Agents write diagnostic scripts that check dozens of potential issues. Humans would never write such thorough diagnostics manually because it's tedious. Agents generate them in minutes."
      }
    },
    {
      "type": "code",
      "title": "Diagnostic Script Pattern",
      "language": "bash",
      "code": "#!/bin/bash\n# Check hypothesis: database connection pool\necho \"=== Database Checks ===\"\nnetstat -an | grep -c ESTABLISHED\npsql -c \"SELECT current_setting(\n  'max_connections')\"\npsql -c \"SELECT count(*) FROM\n  pg_stat_activity\"\n\n# Check hypothesis: query slowness\necho \"=== Slow Queries ===\"\npsql -c \"SELECT query, mean_time\n  FROM pg_stat_statements\n  ORDER BY mean_time DESC LIMIT 5\"\n\n# Check hypothesis: GC pauses\necho \"=== JVM GC ===\"\njstat -gc -t $(pgrep java) 1000",
      "caption": "Generate diagnostic scripts for each hypothesis",
      "speakerNotes": {
        "talkingPoints": "This is a simple diagnostic script that checks multiple hypotheses: database connections, slow queries, and GC pauses. A human would never write this—it's too tedious to cover all possibilities. An agent generates this in 2 minutes. The script outputs structured data the agent can analyze. Running this script often eliminates 80% of the hypotheses, leaving the agent with high-confidence leads.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Would you write a diagnostic script that checks 20 different things?' Most answer 'no, too much work.' That's the point: agents make thorough diagnostics trivial.",
        "context": "In production scenarios, comprehensive diagnostics save massive amounts of time. The script runs once, produces detailed output, and the agent correlates the findings with hypotheses. What would take a human days of back-and-forth becomes one script execution.",
        "transition": "Remote diagnosis gives you evidence even without direct environment access. Now let's review the complete debugging methodology."
      }
    },
    {
      "type": "concept",
      "title": "Grounding + Closed-Loop = Superpower",
      "content": [
        "Code research: Understand architecture deeply",
        "Runtime access: Execute in failing environment",
        "Reproducibility: Verify every fix works",
        "Iteration: Form new hypothesis if needed",
        "Audit trail: Document investigation path"
      ],
      "speakerNotes": {
        "talkingPoints": "Combine two Lesson 5 concepts—grounding (deep understanding of your codebase and known issues)—with closed-loop access (ability to test fixes). This combination is where AI debugging becomes exponentially more powerful than traditional approaches. The agent understands the system deeply, can test hypotheses immediately, and provides proof with each fix.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What would ideal debugging look like?' Most answer: 'Understand the code, form a hypothesis, test it, and verify it works.' That's exactly what this gives you.",
        "context": "The audit trail aspect is often overlooked. When the agent documents its investigation (checked A, formed hypothesis B, discovered C, applied D, verified E), you have a complete record of why the fix is correct. This is invaluable for future engineers.",
        "transition": "Let's wrap up with the key takeaways for debugging with AI agents."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Always require reproducible proof—never accept\nfixes without verification",
        "Code inspection reveals architecture and execution\nflow before logs show symptoms",
        "Log analysis is AI's superpower—process chaos\nhumans can't correlate manually",
        "Closed-loop debugging with CLI agents provides\nfeedback and proof for every iteration"
      ],
      "speakerNotes": {
        "talkingPoints": "These four principles transform debugging from speculative troubleshooting to systematic investigation. Evidence first, always. Understand the system before diving into logs. Leverage AI's pattern recognition on messy data. And close the loop so the agent can prove its fixes work.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Which of these four principles would have the biggest impact on your debugging process?' Different answers will reveal where each engineer struggles most.",
        "context": "In production environments with thousands of concurrent users, these principles prevent bad fixes from shipping. Evidence, understanding, pattern recognition, and proof save your reputation.",
        "transition": "You're now equipped to debug with AI agents. Remember: the agent is your tireless investigator—give it the tools and demand proof."
      }
    }
  ]
}
