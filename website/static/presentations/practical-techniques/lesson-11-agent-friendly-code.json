{
  "metadata": {
    "title": "Agent-Friendly Code",
    "lessonId": "lesson-11-agent-friendly-code",
    "estimatedDuration": "30-40 minutes",
    "learningObjectives": [
      "Understand pattern compounding effects",
      "Co-locate constraints for agents",
      "Use comments as guardrails",
      "Avoid knowledge cache anti-patterns"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Agent-Friendly Code",
      "subtitle": "Patterns that compound quality, not debt",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson addresses a fundamental truth: agents amplify whatever patterns they find. Clean code generates more clean code. Scattered logic generates more scattered logic. We'll cover how to structure code so agents help rather than hurt.",
        "timing": "1 minute",
        "discussion": "Ask: How many of you have seen AI-generated code that duplicated existing problems in your codebase?",
        "context": "Research shows AI-generated code contains 8x more duplicated blocks than human-written code—not because AI creates duplication, but because it amplifies existing patterns.",
        "transition": "Let's start by understanding the compounding mechanism that drives this."
      }
    },
    {
      "type": "visual",
      "title": "The Compounding Mechanism",
      "component": "CompoundQualityVisualization",
      "caption": "Every accepted PR becomes pattern context for future agents.",
      "speakerNotes": {
        "talkingPoints": "During code research, agents grep for patterns, read implementations, and load examples into context. The code they find becomes the pattern context for generation. This creates exponential quality curves—upward or downward. You control the direction.",
        "timing": "2-3 minutes",
        "discussion": "What patterns in your current codebase would you NOT want amplified?",
        "context": "This visualization shows how quality compounds over iterations. Good patterns lead to exponential improvement; bad patterns lead to exponential debt.",
        "transition": "There are two distinct ways quality drifts when working with AI—let's examine both."
      }
    },
    {
      "type": "comparison",
      "title": "Two Sources of Quality Drift",
      "left": {
        "label": "Copy Machine Effect",
        "content": [
          "Predictable pattern amplification",
          "Agent finds existing code as examples",
          "Duplication begets more duplication",
          "Missing tests beget missing tests"
        ]
      },
      "right": {
        "label": "Dice Roll Effect",
        "content": [
          "Random probabilistic errors",
          "LLMs generate through weighted randomness",
          "Hallucinated functions that don't exist",
          "Complexity increases error probability"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "The Copy Machine Effect is predictable: show messy patterns, get messy code. The Dice Roll is inherent to LLMs—probabilistic systems produce random errors regardless of pattern quality. Both feed the same exponential curve when errors are accepted.",
        "timing": "3-4 minutes",
        "discussion": "Can you recall a time when an AI hallucinated a function that didn't exist? What was the downstream impact?",
        "context": "You can't eliminate random errors with better prompts—they're inherent to how LLMs work. Your critical role is rejecting both pattern amplification AND random errors during code review.",
        "transition": "Understanding these mechanisms, let's look at practical patterns to improve agent outcomes."
      }
    },
    {
      "type": "codeComparison",
      "title": "Co-locate Related Constraints",
      "leftCode": {
        "label": "Scattered Constraints",
        "language": "typescript",
        "code": "// File: services/auth.ts\nfunction createUser(email: string, password: string) {\n  return db.users.insert({ email, password: hashPassword(password) })\n}\n\n// File: config/validation.ts\nconst MIN_PASSWORD_LENGTH = 12  // ← Agent never searches for this file"
      },
      "rightCode": {
        "label": "Co-located Constraints",
        "language": "typescript",
        "code": "// File: services/auth.ts\nconst MIN_PASSWORD_LENGTH = 12  // ← Agent sees this in same file\n\nfunction createUser(email: string, password: string) {\n  if (password.length < MIN_PASSWORD_LENGTH) {\n    throw new ValidationError(`Password must be ${MIN_PASSWORD_LENGTH}+ chars`)\n  }\n  return db.users.insert({ email, password: hashPassword(password) })\n}"
      },
      "speakerNotes": {
        "talkingPoints": "Agents only see code they explicitly find through search. When constraints scatter across files, the agent might generate code that violates constraints it never discovered. The right pattern keeps validation constants with the code that uses them.",
        "timing": "3-4 minutes",
        "discussion": "Think about your codebase: where are validation rules defined vs. where are they enforced?",
        "context": "The scattered pattern leads to agents accepting 3-character passwords because they never saw MIN_PASSWORD_LENGTH. This is a security vulnerability created by code organization, not AI capability.",
        "transition": "But sometimes separation is required—like DRY. How do we handle that?"
      }
    },
    {
      "type": "concept",
      "title": "Semantic Bridges for Required Separation",
      "content": [
        "Use explicit comments pointing to related files",
        "Format: // Related: path/to/file.ts - reason",
        "Agent searches will find these breadcrumbs",
        "Creates discoverable links between scattered code"
      ],
      "speakerNotes": {
        "talkingPoints": "When DRY requires separating related code, create semantic bridges—explicit comments that agents will find during search. These breadcrumbs help agents navigate to constraints they might otherwise miss.",
        "timing": "2 minutes",
        "discussion": "What naming conventions or comment patterns does your team use to link related code?",
        "context": "This isn't about documenting everything—it's about creating discoverable paths between necessarily separated but related code.",
        "transition": "For genuinely high-risk code, we need stronger guardrails than bridges."
      }
    },
    {
      "type": "concept",
      "title": "Comments as Agent-Critical Sections",
      "content": [
        "Use for high-risk code: auth, crypto, payments, PII",
        "Write comments as prompts with imperatives",
        "Directives: NEVER, MUST, ALWAYS create friction",
        "Overuse is counterproductive—signal becomes noise"
      ],
      "speakerNotes": {
        "talkingPoints": "For genuinely high-risk code, write comments as prompts using imperative directives. These create deliberate friction that makes agents pause before modifying sensitive code. But use sparingly—if everything is CRITICAL, the signal becomes noise.",
        "timing": "2-3 minutes",
        "discussion": "What areas of your codebase would benefit from this protection? What areas would it slow down unnecessarily?",
        "context": "This is a protection mechanism, not a documentation pattern. It guards sensitive code from accidental modification by making the stakes explicit.",
        "transition": "Now let's discuss the most important role you play in this system."
      }
    },
    {
      "type": "concept",
      "title": "You Are the Quality Circuit Breaker",
      "content": [
        "Code review prevents negative compounding",
        "Accepting bad patterns lets them enter context",
        "Rejecting breaks the negative feedback loop",
        "Every acceptance decision affects future generations"
      ],
      "speakerNotes": {
        "talkingPoints": "Your critical role is breaking the cycle during code review. When you accept a random AI error, it becomes a pattern that gets copied. One hallucinated API call in iteration 1 becomes the template for 5 similar hallucinations by iteration 3.",
        "timing": "3 minutes",
        "discussion": "How rigorous is your team's code review process for AI-generated code vs. human-written code?",
        "context": "This connects back to Lesson 9 on code review. The stakes are higher with AI because errors compound faster through pattern amplification.",
        "transition": "There's one more anti-pattern we need to address: knowledge caching."
      }
    },
    {
      "type": "codeExecution",
      "title": "Knowledge Cache Anti-Pattern",
      "steps": [
        {
          "line": "Agent: Research phase - read source code",
          "highlightType": "execution",
          "annotation": "Agent extracts architectural knowledge from code"
        },
        {
          "line": "Knowledge extracted and held in context",
          "highlightType": "feedback",
          "annotation": "Knowledge stays fresh in agent's working memory"
        },
        {
          "line": "✅ Good Path: Plan and execute with fresh context",
          "highlightType": "prediction",
          "annotation": "Agent uses current knowledge, makes changes"
        },
        {
          "line": "Code updated - source remains single source of truth",
          "highlightType": "summary",
          "annotation": "No stale caches created"
        },
        {
          "line": "❌ Bad Path: Save ARCHITECTURE.md to codebase",
          "highlightType": "human",
          "annotation": "Committing extracted knowledge creates a cache"
        },
        {
          "line": "Code changes... but cache is now stale",
          "highlightType": "execution",
          "annotation": "Inevitable drift between code and documentation"
        },
        {
          "line": "Future agent finds BOTH: current code AND outdated cache",
          "highlightType": "feedback",
          "annotation": "Conflicting information pollutes grounding"
        },
        {
          "line": "Agent confusion: which source to trust?",
          "highlightType": "prediction",
          "annotation": "Cache invalidation problem - impossible to solve"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "The moment you commit extracted knowledge, every code change requires documentation updates you'll forget. Source code is your single source of truth. Code research tools extract architectural knowledge dynamically every time, fresh and accurate.",
        "timing": "3-4 minutes",
        "discussion": "Has your team ever created ARCHITECTURE.md or similar files that became outdated? What happened?",
        "context": "Document decisions and WHY (ADRs, high-level overviews). Don't document extracted WHAT that code research can regenerate on demand.",
        "transition": "Let's summarize the key patterns to remember."
      }
    },
    {
      "type": "comparison",
      "title": "What to Document vs. What to Skip",
      "left": {
        "label": "Don't Cache (Stale Risk)",
        "content": [
          "Extracted architecture diagrams",
          "Auto-generated API documentation",
          "Code structure summaries",
          "Implementation details"
        ]
      },
      "right": {
        "label": "Do Document (Stable Value)",
        "content": [
          "ADRs (Architecture Decision Records)",
          "High-level business domain concepts",
          "WHY decisions were made",
          "External integration contracts"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The distinction is between WHAT (which changes constantly and tools can extract) vs. WHY (which is stable and invisible in code). Trust grounding tools to re-extract structural knowledge on-demand.",
        "timing": "2-3 minutes",
        "discussion": "Review your team's documentation: what falls into each category?",
        "context": "This connects to Lesson 5 on grounding—code research tools like ChunkHound, semantic search, and Explore extract fresh architectural knowledge every time.",
        "transition": "Let's wrap up with the key takeaways from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents amplify existing code patterns",
        "Co-locate constraints for discoverability",
        "Code review breaks compounding cycles",
        "Avoid caching extracted knowledge"
      ],
      "speakerNotes": {
        "talkingPoints": "Remember: agents amplify patterns AND produce stochastic errors—both compound if accepted. Co-locate constraints so agents find them. Use imperative comments sparingly for high-risk code. You are the circuit breaker. Trust code research tools over cached documentation.",
        "timing": "2 minutes",
        "discussion": "What's one change you'll make to your codebase based on this lesson?",
        "context": "These patterns apply regardless of which AI coding assistant you use—Claude, Copilot, Cursor, or others.",
        "transition": "Next lesson, we'll cover advanced prompting techniques for complex multi-step tasks."
      }
    }
  ]
}