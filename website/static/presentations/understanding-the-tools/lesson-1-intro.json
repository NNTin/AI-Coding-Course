{
  "metadata": {
    "title": "Lesson 1: Introduction - Understanding AI Agents",
    "lessonId": "lesson-1-intro",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Understand LLMs as token prediction engines",
      "Recognize agents as LLM + execution layer",
      "Avoid anthropomorphizing AI tools",
      "Identify three critical operator errors"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Introduction: Operating AI Agents",
      "subtitle": "The Paradigm Shift in Software Engineering",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to the course on operating AI agents. This lesson establishes the foundational mental model. We're not learning to 'work with AI teammates'—we're learning to operate precision tools. The vocabulary matters. This is about paradigm shift similar to CNC machines in manufacturing.",
        "timing": "1-2 minutes",
        "discussion": "Ask students: 'How many of you have used GitHub Copilot or ChatGPT? What assumptions do you naturally make about how it works?'",
        "context": "This sets up the entire course philosophy: tool mindset, not teammate mindset. Many engineers approach AI assistants like junior developers, which causes frustration and ineffective usage.",
        "transition": "Let's start by understanding the fundamental machinery—what is an LLM actually doing?"
      }
    },
    {
      "type": "concept",
      "title": "The Manufacturing Analogy",
      "content": [
        "CNC machines: design → program → monitor → verify",
        "AI agents: architect → specify → monitor → verify",
        "Before: Manual craftsmanship, line-by-line control",
        "After: Autonomous execution, architecture focus",
        "Gain in bandwidth, repeatability, precision"
      ],
      "speakerNotes": {
        "talkingPoints": "The transformation in software engineering mirrors manufacturing. Just as CNC machines shifted from craft to engineering discipline, AI agents shift our focus from syntax to architecture. The gain isn't losing control—it's gaining bandwidth. We trade line-by-line implementation for architectural oversight.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What's harder—writing code or verifying architecture?' This primes them for why testing and verification matter more with agents.",
        "context": "In manufacturing, CNC operators still need tool knowledge and verification skills. Similarly, engineers using AI agents need to understand the execution model and build guardrails.",
        "transition": "This analogy works because it reframes the relationship. Now let's look at the machinery itself—what is an LLM?"
      }
    },
    {
      "type": "concept",
      "title": "LLM: The Brains (Token Prediction Engine)",
      "content": [
        "Predicts next token (word/sub-word) in sequence",
        "~200K tokens of working memory (context window)",
        "Samples from probability distributions",
        "Zero consciousness, intent, or feelings",
        "Sophisticated autocomplete pattern matcher"
      ],
      "speakerNotes": {
        "talkingPoints": "An LLM is not thinking. It's not understanding. It's doing one thing incredibly well: predicting the statistically probable next token. This is the core insight that prevents overestimating what it can do. The context window (~200K tokens) is working memory—it's limited, and that's critical for how we interact with agents.",
        "timing": "2-3 minutes",
        "discussion": "Real-world: 'GitHub Copilot can write a function, but it doesn't know about your authentication system. Why? It can only see context you provide.' This connects to the practical limitation.",
        "context": "Many engineers assume LLMs have understanding. They don't. This directly impacts error handling—you can't rely on the agent to 'know' if something is wrong. You need explicit guardrails.",
        "transition": "The distinction between metaphor and reality matters. Let's look at what we say vs. what's actually happening."
      }
    },
    {
      "type": "marketingReality",
      "title": "Marketing vs Reality: What's Actually Happening",
      "metaphor": {
        "label": "Marketing Speak",
        "content": [
          "The agent thinks",
          "The agent understands",
          "The agent learns",
          "The agent reasons"
        ]
      },
      "reality": {
        "label": "Technical Reality",
        "content": [
          "Token predictions via multi-head attention layers",
          "Pattern matching against training data",
          "Statistical weights fixed (trained once, not updated)",
          "Sequential token predictions building context"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This slide is critical. We use anthropomorphic language for convenience, but it creates mental models that lead to errors. The agent doesn't 'think'—it generates token predictions. This matters when you're debugging. If something goes wrong, you're not debugging reasoning; you're debugging context or constraints.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'An agent produces buggy code. Do you ask 'why didn't it understand?' or 'what context was missing?' The second question is productive.' Discuss how language shapes our debugging approach.",
        "context": "In production, teams that avoid anthropomorphizing build better guardrails. They don't expect the agent to 'catch' their error—they write tests that catch it.",
        "transition": "Now that we've separated metaphor from reality, let's look at the second component—the execution layer that turns predictions into action."
      }
    },
    {
      "type": "concept",
      "title": "Agent Software: The Body (Execution Layer)",
      "content": [
        "LLM generates text predictions (only)",
        "Agent framework executes deterministic actions:",
        "File operations: Read, Write, Edit",
        "Commands: Bash, git, npm, pytest",
        "Search & integration: Grep, Glob, API calls"
      ],
      "speakerNotes": {
        "talkingPoints": "The LLM is pure language. The agent framework wraps it with the ability to act. This separation is fundamental. The LLM predicts 'I should read auth.ts'—the agent framework actually reads it. One is probabilistic; one is deterministic. Both are necessary.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'If we only had the LLM without the execution layer, what could it do?' (Generate text). 'What's the minimum execution layer you need to be useful?' (Read files, execute code, see results).",
        "context": "Understanding this split prevents debugging mistakes. If the agent makes a wrong decision, it's usually because it didn't have the right context to read. If it executes the wrong command, that's a constraint issue.",
        "transition": "Let's visualize how these two pieces work together in an actual execution loop."
      }
    },
    {
      "type": "codeExecution",
      "title": "Agent Execution Loop: How They Work Together",
      "steps": [
        {
          "line": "Engineer specifies: 'Add authentication middleware to Express app'",
          "highlightType": "human",
          "annotation": "Explicit task with scope defined"
        },
        {
          "line": "LLM predicts: 'I should read existing auth patterns first'",
          "highlightType": "prediction",
          "annotation": "Token prediction drives next action"
        },
        {
          "line": "Agent executes: Read(src/middleware/auth.ts)",
          "highlightType": "execution",
          "annotation": "Deterministic tool call based on prediction"
        },
        {
          "line": "File content returned to context window",
          "highlightType": "feedback",
          "annotation": "LLM now has relevant context to work with"
        },
        {
          "line": "LLM analyzes patterns and predicts: 'I'll use JWT approach'",
          "highlightType": "prediction",
          "annotation": "New prediction informed by actual codebase"
        },
        {
          "line": "Agent executes: Edit(src/app.ts, old, new) + Bash(npm test)",
          "highlightType": "execution",
          "annotation": "Implementation and verification"
        },
        {
          "line": "Test output returned; loop continues until tests pass",
          "highlightType": "summary",
          "annotation": "Iteration continues with feedback"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the fundamental loop you'll be managing. The LLM predicts what to do next. The agent executes it. Results feed back as context. This repeats until the task is complete or you stop it. Your job is architecting the instructions and constraints to guide this loop effectively.",
        "timing": "4-5 minutes",
        "discussion": "Walk through a step at a time. Pause at the feedback step: 'Notice that the LLM can't see the file content until the agent reads it. So if you give vague instructions, the agent will make vague guesses.' This connects to later lessons on prompt engineering.",
        "context": "Production deployment: agents running this loop need guardrails—tests, type checking, linting—because the LLM predictions aren't guaranteed correct. It's probability, not logic.",
        "transition": "Understanding this loop prevents three critical operator errors. Let's look at each one."
      }
    },
    {
      "type": "concept",
      "title": "Error 1: Assuming the Agent 'Knows'",
      "content": [
        "Reality: Only sees current context (~200K tokens)",
        "Consequence: Makes vague guesses without explicit context",
        "Example: 'Add authentication' without showing existing patterns",
        "Your fix: Provide architectural context explicitly",
        "Production lesson: Context engineering is job #1"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the most common error. Engineers assume the agent can infer from names, patterns, or prior knowledge. It can't. It only sees what's in the current context window. If you don't show it the authentication code that exists, it invents one. If you don't specify the framework you're using, it guesses.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'You ask an agent to 'refactor authentication.' It produces something incompatible. Whose fault?' (Both—but yours is preventable by providing context).",
        "context": "In production: Junior engineers blame the agent. Senior engineers refactor their prompt or context. This is the mindset shift.",
        "transition": "Error 2 is related but different. It's about what the agent does with incomplete constraints."
      }
    },
    {
      "type": "concept",
      "title": "Error 2: Expecting the Agent to 'Care' About Outcomes",
      "content": [
        "Reality: Executes your literal instruction to completion",
        "Consequence: Follows vague requirements literally (dangerously)",
        "Example: 'Make it faster' → optimizes hot loop, breaks something else",
        "Your fix: Be precise with constraints and acceptance criteria",
        "Production lesson: Over-specification is safer than under-specification"
      ],
      "speakerNotes": {
        "talkingPoints": "The agent doesn't have goals. It has instructions. If you say 'optimize performance' without constraints, it'll do something—but maybe not what you wanted. It doesn't care about backwards compatibility, readability, or team conventions. You have to specify those.",
        "timing": "2-3 minutes",
        "discussion": "Real-world scenario: 'Agent produces minified code that breaks your build because you didn't specify that it should use your linting config. Did the agent fail? No—it executed your vague instruction.'",
        "context": "This mirrors CNC machines. You don't blame the machine for tolerances outside your spec. You blame your spec. Same here.",
        "transition": "Error 3 is the mindset error that causes both of these to persist."
      }
    },
    {
      "type": "concept",
      "title": "Error 3: Treating the Tool as a Teammate",
      "content": [
        "Reality: Precision instrument that speaks English",
        "Consequence: Expecting intuition, negotiation, or care",
        "Example: Vague requests with 'please' expecting it to infer intent",
        "Your fix: Use tool-operator mindset instead",
        "Production lesson: Precision beats politeness"
      ],
      "speakerNotes": {
        "talkingPoints": "You don't politely ask a CNC machine for 'a reasonably well-cut part.' You give exact coordinates and tolerances. Same with AI agents. Politeness doesn't help. Precision does. This is the mental model shift that separates frustration from flow.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How many of you say 'please' and 'thank you' to ChatGPT?' (Normal). 'Would you say that to a CNC machine?' (No). 'Why not apply the same mindset here?'",
        "context": "Teams that adopt the tool operator mindset iterate faster. They debug more effectively. They build better guardrails.",
        "transition": "These three errors stem from anthropomorphizing. Let's look at the power and limitations of what we're actually working with."
      }
    },
    {
      "type": "comparison",
      "title": "The Power and Limitation of 'Fancy Autocomplete'",
      "left": {
        "label": "The Limitation",
        "content": [
          "No model of correctness (only probability)",
          "Can't verify its own work",
          "Makes plausible-sounding mistakes",
          "Limited to training data patterns"
        ]
      },
      "right": {
        "label": "The Power",
        "content": [
          "Incredibly good at code patterns",
          "Fast iteration and generation",
          "Can synthesize novel combinations",
          "Fluent natural language interface"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is reductive but liberating. An LLM is fancy autocomplete. Don't expect it to be correct. Expect it to be fast and pattern-matching. Your job is building the verification layer (tests, types, lints) that catches the probabilistic errors. This inverts the mental model: the agent is optimized for generation speed, not correctness.",
        "timing": "3 minutes",
        "discussion": "Ask: 'If the agent can't verify its own work, whose job is that?' (Yours). 'How do you verify it?' (Tests, code review, types). This connects to the next lesson on architecture.",
        "context": "Production deployment: agents that have strong type systems, test suites, and linting pass more often. The verification layer is the architecture.",
        "transition": "This brings us to the core principle: you're not managing a junior developer; you're operating a sophisticated code generation tool that needs architectural guardrails."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways: The Operator Mindset",
      "content": [
        "LLMs = token prediction engines (not thinking)",
        "Agents = LLM + execution layer (brains + body)",
        "Avoid three errors: assuming 'knowing', expecting\n'caring', treating as teammate",
        "You're operating a precision tool, not managing a teammate",
        "Verification layer (tests, types, lints) is non-negotiable"
      ],
      "speakerNotes": {
        "talkingPoints": "We've covered the machinery and the mindset. The three operator errors are preventable. You avoid them by understanding what the tool actually is and building the right constraints and verification systems. This mental model foundation is the hardest part—everything else builds on it.",
        "timing": "2 minutes",
        "discussion": "Quick check: 'Can an agent understand ambiguous requirements?' (No—only see context). 'Should you blame it for missing context?' (No—specify it). 'Is it a teammate?' (No—a tool).",
        "context": "Many engineers starting this course are skeptical or over-enthusiastic. This lesson aims for realistic understanding. Neither 'AI will replace us' nor 'it's useless' is productive.",
        "transition": "Next lesson: Agent Architecture. We'll look at execution workflows and how your role evolves from implementer to orchestrator."
      }
    },
    {
      "type": "concept",
      "title": "Why This Matters: Three Prevention Strategies",
      "content": [
        "Prevent Error 1: Explicit context engineering (Lesson 3)",
        "Prevent Error 2: Precise specifications with constraints",
        "Prevent Error 3: Tool operator mindset (not teammate)",
        "Result: Faster iteration, fewer debugging loops",
        "Next: Learn the agent architecture and execution model"
      ],
      "speakerNotes": {
        "talkingPoints": "Each of these errors has a prevention strategy. We'll dive deep into them in subsequent lessons. But the foundation is understanding the machinery and adopting the right mental model. This sets you up to use agents effectively in production.",
        "timing": "1-2 minutes",
        "discussion": "Ask: 'In your role, which error seems most likely?' This surfaces where individuals might struggle and primes them for upcoming lessons.",
        "context": "Different roles struggle with different errors. Architects struggle with Error 1 (forgetting to specify constraints). Frontend engineers struggle with Error 2 (over-trusting the agent on UI patterns). QA struggles with Error 3 (treating agents like teammates).",
        "transition": "That's the foundation. In Lesson 2, we dive into agent architecture and workflows. You'll see how this mental model translates into operating patterns."
      }
    }
  ]
}
