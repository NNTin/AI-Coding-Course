{
  "metadata": {
    "title": "Understanding AI Agents: First Principles",
    "lessonId": "lesson-1-intro",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Understand AI agents as tools",
      "Learn token prediction mechanics",
      "Recognize three operator errors",
      "Adopt operator mindset for effectiveness"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding AI Agents",
      "subtitle": "First Principles for Senior Engineers",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Welcome to the AI Coding Course. This lesson establishes the foundational mental model for operating AI agents. We're not learning magic—we're learning to operate sophisticated tools. The goal is to shift your mindset from 'AI does things for me' to 'I operate AI tools effectively.' This course teaches you to orchestrate agents that autonomously execute tasks, freeing you to focus on architecture, verification, and decision-making.",
        "timing": "1 minute",
        "discussion": "Before we start: Who here has used GitHub Copilot or ChatGPT for coding? What surprised you most about what it could (or couldn't) do?",
        "context": "This is the first lesson in the 'Understanding the Tools' section. It sets the mental model for everything that follows.",
        "transition": "Let's start by looking at why this paradigm shift matters in software engineering—by drawing parallels to manufacturing."
      }
    },
    {
      "type": "concept",
      "title": "The Paradigm Shift",
      "content": [
        "Software engineering undergoing transformation similar to CNC/3D printers in manufacturing",
        "Shift from hands-on craftsmanship to orchestration and verification",
        "Engineers move from writing every line to configuring automated execution",
        "Gain in bandwidth, repeatability, and precision through proper tooling"
      ],
      "speakerNotes": {
        "talkingPoints": "Before CNC machines, lathe operators manually shaped every part. After CNC, they designed parts, programmed machines, and verified output. The same shift is happening in software. You're moving from writing every line to orchestrating agents. This isn't loss of control—it's a gain in bandwidth. A CNC operator doesn't lose control; they gain leverage.",
        "timing": "2 minutes",
        "discussion": "How does your role change if you're orchestrating instead of implementing? What new responsibilities emerge?",
        "context": "Production teams that embraced CNC didn't lose jobs—they became more valuable. Same principle applies here.",
        "transition": "Let's make this concrete by breaking down what AI agents actually are."
      }
    },
    {
      "type": "concept",
      "title": "What is an AI Agent?",
      "content": [
        "LLM (brain) = Token prediction engine, ~200K tokens context",
        "Agent software (body) = File ops, bash, code search, API calls",
        "LLM predicts next action, agent software executes deterministically",
        "No consciousness, intent, or learning during use—just probability"
      ],
      "speakerNotes": {
        "talkingPoints": "An AI agent is two components working together. The LLM is the brains—a statistical pattern matcher that predicts the next most probable token in a sequence. It's sophisticated autocomplete. The agent software is the body—deterministic tools that read files, execute commands, and fetch data. The LLM generates predictions; the software executes them. This separation is critical for understanding how agents work and what can go wrong.",
        "timing": "3-4 minutes",
        "discussion": "If the LLM is just predicting tokens, how does it produce coherent code? (Answer: Because code patterns are common in training data, and token-by-token prediction preserves coherence.)",
        "context": "Understanding this separation prevents anthropomorphizing. You're not talking to a developer; you're operating a tool.",
        "transition": "Let's look at the first component: what an LLM actually is."
      }
    },
    {
      "type": "codeExecution",
      "title": "Marketing vs Reality: What We Say vs What Happens",
      "steps": [
        {
          "line": "We say: 'The agent thinks'",
          "highlightType": "human",
          "annotation": "Metaphorical language we use"
        },
        {
          "line": "Reality: LLM generates token predictions through multi-head attention layers",
          "highlightType": "prediction",
          "annotation": "Actual mechanism: statistical pattern matching"
        },
        {
          "line": "We say: 'The agent understands'",
          "highlightType": "human",
          "annotation": "Implies comprehension"
        },
        {
          "line": "Reality: Pattern matching against training data produces contextually probable output",
          "highlightType": "feedback",
          "annotation": "No understanding, just probability"
        },
        {
          "line": "We say: 'The agent learns'",
          "highlightType": "human",
          "annotation": "Implies real-time learning"
        },
        {
          "line": "Reality: Weights updated during training (not during your conversation)",
          "highlightType": "feedback",
          "annotation": "Learning happened before you started"
        },
        {
          "line": "We say: 'The agent reasons'",
          "highlightType": "human",
          "annotation": "Implies logical thinking"
        },
        {
          "line": "Reality: Breaking down problems into sequential token predictions that build on each other",
          "highlightType": "prediction",
          "annotation": "Sequential generation, not logical reasoning"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "Marketing language makes LLMs sound magical. But understanding the reality is liberating. The LLM doesn't think—it predicts tokens. It doesn't understand—it matches patterns. It doesn't learn from your conversation—weights are frozen after training. It doesn't reason—it generates sequences of probable tokens. This might sound reductive, but it's the key to using these tools effectively. Once you stop expecting magic, you can build systems that work.",
        "timing": "4-5 minutes",
        "discussion": "Can you think of a time you expected the agent to 'understand' something and it failed? What was actually missing?",
        "context": "This is the most important mental model shift. Senior engineers often over-attribute capability. Recognize the tool's actual mechanism.",
        "transition": "Now that we understand the LLM, let's talk about the agent software that wraps it."
      }
    },
    {
      "type": "concept",
      "title": "Agent Software: The Execution Layer",
      "content": [
        "LLM generates predictions; agent software executes them deterministically",
        "File operations: Read, Write, Edit files with precision",
        "Command execution: Bash, git, npm, pytest for automation",
        "Code search: Grep, Glob for context gathering",
        "External resources: API calls, documentation fetching"
      ],
      "speakerNotes": {
        "talkingPoints": "The LLM is the brains, but it's trapped in text. It can only output text predictions. The agent software is the body—it translates those text predictions into deterministic actions. When the LLM generates 'I should read the auth middleware,' the agent software executes Read(src/auth.ts). When the LLM generates code changes, the agent executes Edit. This separation is clean: the LLM predicts, the software executes. You control what tools the agent has access to, which constrains what it can do.",
        "timing": "3 minutes",
        "discussion": "What tools have you used recently that match this pattern? (IDE autocomplete, git hooks, CI/CD pipelines all follow this pattern.)",
        "context": "This is why agent architecture matters. You're not giving the agent superpowers; you're designing which tools it can invoke.",
        "transition": "Let's see this in action with a real execution loop."
      }
    },
    {
      "type": "codeExecution",
      "title": "Agent Execution Loop: Real Example",
      "steps": [
        {
          "line": "Engineer specifies: 'Add JWT authentication middleware to express app'",
          "highlightType": "human",
          "annotation": "Clear task with context"
        },
        {
          "line": "LLM predicts: 'I should first understand the existing auth patterns'",
          "highlightType": "prediction",
          "annotation": "Token prediction identifies what to do next"
        },
        {
          "line": "Agent executes: Read(src/middleware/auth.ts)",
          "highlightType": "execution",
          "annotation": "Deterministic file read"
        },
        {
          "line": "File content returned to context (~500 tokens)",
          "highlightType": "feedback",
          "annotation": "Operation result available for next prediction"
        },
        {
          "line": "LLM analyzes patterns: 'I see we use session-based auth. I'll implement JWT as a new approach'",
          "highlightType": "prediction",
          "annotation": "Prediction incorporates new context"
        },
        {
          "line": "Agent executes: Read(package.json) to check dependencies",
          "highlightType": "execution",
          "annotation": "Gather more context to inform implementation"
        },
        {
          "line": "LLM predicts: 'I'll use jsonwebtoken library and add middleware to app.ts'",
          "highlightType": "prediction",
          "annotation": "Decision made with full context"
        },
        {
          "line": "Agent executes: Edit(src/app.ts, old_import, new_import)",
          "highlightType": "execution",
          "annotation": "Code modification"
        },
        {
          "line": "Agent executes: Bash('npm test') to verify",
          "highlightType": "execution",
          "annotation": "Automated verification"
        },
        {
          "line": "Test output returned: '4 new tests passing, middleware integrated'",
          "highlightType": "feedback",
          "annotation": "Verification result"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the actual loop: engineer provides task, LLM predicts action, agent executes, result fed back to LLM. No magic—just probability-driven tool invocation. The LLM doesn't 'know' the codebase; it sees current context. It doesn't 'care' about tests; it generates code based on patterns. Notice how each step adds context that informs the next prediction. This is why context management matters—more context means better predictions.",
        "timing": "5-6 minutes",
        "discussion": "Where could this loop fail? (Vague task → bad prediction, missing context → hallucinated code, no verification → broken output.)",
        "context": "This loop repeats thousands of times in a real agent session. Understanding it is fundamental.",
        "transition": "Now let's talk about the three critical mistakes engineers make when operating agents."
      }
    },
    {
      "type": "concept",
      "title": "Three Critical Operator Errors",
      "content": [
        "Error 1: Assuming the agent 'knows' things (it only sees ~200K tokens of context)",
        "Error 2: Expecting the agent to 'care' about outcomes (it executes literal instructions)",
        "Error 3: Treating it like a teammate instead of a tool (it speaks English but isn't human)",
        "Fix: Provide explicit context, precise constraints, and architectural guardrails"
      ],
      "speakerNotes": {
        "talkingPoints": "These three errors stem from anthropomorphizing the tool. You wouldn't expect a CNC machine to 'know' what part you want if you gave vague coordinates. You'd provide exact specifications. Same with LLMs. These three errors are costly because they lead to iterations, hallucinations, and broken code.",
        "timing": "2 minutes",
        "discussion": "Which of these three have you experienced? What happened?",
        "context": "These errors are the primary source of frustration when using AI agents. Recognizing them prevents most problems.",
        "transition": "Let's examine each error in detail."
      }
    },
    {
      "type": "concept",
      "title": "Error 1: Assuming the Agent Knows",
      "content": [
        "Agent only sees current context window (~200K tokens / ~50K lines of code)",
        "If your codebase is larger, most of it is invisible to the agent",
        "Agent can't 'look something up' unless you explicitly provide it",
        "Fix: Grounding principle—provide relevant context before asking",
        "Example: 'In our UserService at src/services/user.ts, we validate emails with regex X. Add similar validation to newUser() method.'"
      ],
      "speakerNotes": {
        "talkingPoints": "The agent operates with tunnel vision. It sees only the context you provide. If you ask it to 'follow our existing patterns' without showing those patterns, it will hallucinate something that sounds reasonable but doesn't match your codebase. This is the most common source of 'the agent didn't understand our architecture' complaints. The fix is simple: show it what you want it to follow before asking.",
        "timing": "3 minutes",
        "discussion": "How does this change how you phrase requests? What additional context do you need to provide?",
        "context": "Production teams that struggle with agents usually skip this step. Grounding is not optional.",
        "transition": "The second error is about operator responsibility."
      }
    },
    {
      "type": "concept",
      "title": "Error 2: Expecting the Agent to Care",
      "content": [
        "Agent executes your literal instruction to completion—nothing more, nothing less",
        "It doesn't have 'goals' or 'intent'—it generates tokens based on probability",
        "If you say 'add validation' without constraints, it adds validation somewhere",
        "Fix: Include explicit constraints in every request",
        "Example: 'Add validation to the POST /users endpoint (no GET changes). Validate email format and age > 18. Return 400 with specific error messages.'"
      ],
      "speakerNotes": {
        "talkingPoints": "This error manifests as 'the agent over-engineered the solution' or 'it changed things I didn't ask for.' The agent isn't trying to be helpful; it's completing your prompt. If your prompt is vague about scope, the agent's probability distributions will generate changes you didn't expect. Precision in requests isn't pedantry—it's necessary for correct behavior.",
        "timing": "3 minutes",
        "discussion": "How would you rewrite a vague request to be constrained and testable?",
        "context": "This is why the Prompting 101 lesson focuses on specificity. It's not a style preference; it's operational necessity.",
        "transition": "The third error is about mental framing."
      }
    },
    {
      "type": "concept",
      "title": "Error 3: Treating Like a Teammate",
      "content": [
        "Agent doesn't understand context clues or implied requirements",
        "It doesn't infer your actual goal from hints or story",
        "It generates the most probable continuation of your text, not what you meant",
        "Fix: Use tool mindset—explicit instructions, measurable outcomes, verification",
        "Example: Instead of 'make the auth better,' write 'implement JWT authentication. Add tests for token expiration. Verify no plaintext passwords are logged.'"
      ],
      "speakerNotes": {
        "talkingPoints": "You wouldn't tell a CNC machine 'make this look nice' and expect beautiful results. You'd provide exact dimensions and tolerances. Same with agents. They need explicit, measurable outcomes. This doesn't make them inflexible—it makes them precise. A teammate can infer 'we probably want error handling.' An agent won't unless you ask.",
        "timing": "3 minutes",
        "discussion": "What's the difference between a 'teammate prompt' and an 'operator prompt' for a real task you're working on?",
        "context": "Senior engineers often resist being this explicit because they've trained juniors to infer intent. You're not training this tool; you're configuring it.",
        "transition": "Now let's look at the power and limitation of this system."
      }
    },
    {
      "type": "concept",
      "title": "The Power and Limitation",
      "content": [
        "Power: Token prediction engines are incredibly good at generating code patterns",
        "Limitation: No model of correctness—only probability distributions",
        "Implication: You must build verification systems that catch probabilistic errors",
        "Your role: Architect guardrails (tests, types, lints) that catch mistakes",
        "Analogy: You're not managing a junior—you're operating a code generation tool"
      ],
      "speakerNotes": {
        "talkingPoints": "Here's the liberating insight: You don't need to trust the agent's judgment. You need to verify its output. The power is that it generates code fast. The limitation is that generation isn't verification. Your job as an operator is to design systems that catch mistakes. Tests, type checking, linting—these aren't optional. They're your verification layer. This shifts your mental model from 'trust the agent' to 'trust the process.'",
        "timing": "3-4 minutes",
        "discussion": "How does this change the importance of testing and type safety? What guardrails matter most in your domain?",
        "context": "Teams that succeed with agents are those with strong testing practices. Teams that struggle usually have weak verification.",
        "transition": "Let's wrap up with the key takeaways."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "AI agents = LLM (brains) + agent software (body). The LLM predicts; the software executes",
        "Understand token prediction, not magic. No consciousness, intent, or learning during use",
        "Avoid three critical operator errors: assuming knowledge, expecting care, teammate mentality",
        "Build verification systems (tests, types, lints). You're operating a tool, not managing a person",
        "Bandwidth gain comes from orchestration, not magic. Your role evolves; you don't disappear"
      ],
      "speakerNotes": {
        "talkingPoints": "You're not learning to use magic. You're learning to operate sophisticated tools that speak English. Everything we covered—token prediction, the three errors, verification—follows from understanding what these tools actually are. The next lesson dives deeper into agent architecture and how execution workflows differ from traditional development.",
        "timing": "2-3 minutes",
        "discussion": "Based on this mental model, what questions do you have before we move to agent architecture?",
        "context": "This foundation is critical. If anyone in the class is skeptical, this is the moment to address it.",
        "transition": "Next lesson: Lesson 2 covers agent architecture and execution workflows. We'll see how this mental model applies to real agent behavior."
      }
    }
  ]
}
