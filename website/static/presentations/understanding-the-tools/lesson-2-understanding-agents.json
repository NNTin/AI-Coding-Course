{
  "metadata": {
    "title": "Understanding Agents",
    "lessonId": "lesson-2-understanding-agents",
    "estimatedDuration": "35-50 minutes",
    "learningObjectives": [
      "Recognize agent loops close feedback",
      "Understand context as agent world",
      "Control behavior via context engineering",
      "Leverage stateless for unbiased review"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding Agents",
      "subtitle": "From Text Loops to Autonomous Systems",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "In Lesson 1, we established that LLMs are token prediction engines and frameworks are execution layers. Today we're diving into how these combine into autonomous agents. By the end of this lesson, you'll understand the feedback loop that makes agents powerful, the textual nature that demystifies their behavior, and how to engineer context for predictable outcomes.",
        "timing": "1 minute",
        "discussion": "Ask: Who's used an AI coding assistant? Did it feel like a chat interface or an autonomous agent? What's the difference?",
        "context": "This lesson is the bridge between understanding components (Lesson 1) and applying them strategically (Lesson 3+).",
        "transition": "Let's start by understanding what makes an agent different from a chat interface—the feedback loop."
      }
    },
    {
      "type": "concept",
      "title": "The Agent Execution Loop",
      "content": [
        "Perceive: Read context (files, state, history)",
        "Reason: LLM generates plan and next action",
        "Act: Execute tool (Read, Edit, Bash, Grep)",
        "Verify: Check if goal achieved or iterate",
        "Loop closes autonomously—no manual intervention"
      ],
      "speakerNotes": {
        "talkingPoints": "A chat interface requires you to manually execute actions between prompts. You ask, you get code, you edit, you come back with the result. An agent loops through this entire cycle automatically. You specify the goal once, and the agent perceives the current state, reasons about what to do, acts on files or commands, observes the result, and either iterates or completes. This is the fundamental difference between talking to an LLM and working with an autonomous system.",
        "timing": "3-4 minutes",
        "discussion": "Have students describe a task they've done with ChatGPT—count how many manual steps were in the loop. Then ask how many steps an agent could handle automatically.",
        "context": "Real-world example: Adding authentication middleware. With chat, you're manually testing, asking for fixes, applying edits. With agents, you specify once and it runs tests, analyzes failures, fixes code, retests.",
        "transition": "Let me show you a concrete example—how this loop plays out when you ask an agent to implement a feature."
      }
    },
    {
      "type": "comparison",
      "title": "Chat Interface vs Agent Workflow",
      "left": {
        "label": "Chat Interface (Manual Loop)",
        "content": [
          "You: 'How to add auth?'",
          "LLM: 'Here's the code...'",
          "You manually edit files",
          "You: 'Got an error...'",
          "LLM: 'Try this fix...'",
          "Repeat: You close the loop manually"
        ]
      },
      "right": {
        "label": "Agent (Autonomous Loop)",
        "content": [
          "You: 'Add authentication middleware'",
          "Agent reads API files",
          "Plans implementation approach",
          "Edits files, runs tests",
          "Analyzes results, fixes failures",
          "Loop completes without intervention"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The chat interface is synchronous and requires human participation at each step. You're the missing link in the feedback loop. Agents are asynchronous and self-contained—they work independently until the task completes or they genuinely need your input. The autonomy is what enables the agent to test, debug, and iterate without context-switching back to you.",
        "timing": "2-3 minutes",
        "discussion": "Ask: In your current workflow, how many times do you manually test code after an LLM suggestion? How much time would agents save?",
        "context": "Productivity impact: Chat-driven feature implementation might involve 5-7 back-and-forth cycles. Agent-driven implementation typically completes in 1-2 cycles with human oversight.",
        "transition": "Now that you understand the loop structure, let's demystify what's actually happening inside—it all comes down to text."
      }
    },
    {
      "type": "concept",
      "title": "The Fundamental Truth: It's All Just Text",
      "content": [
        "No magic, no separate reasoning engine",
        "No hidden state—only context window",
        "Everything is text: instructions, tasks,\ntool calls, results, responses",
        "Agent interaction = conversation in a\nlarge text buffer",
        "What's in context = what agent knows"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the insight that removes mystique from agents. There's no hidden reasoning layer, no separate 'thinking' mode. When you see the agent reasoning aloud ('I should check validation logic'), that's text being generated in real-time, visible to you and the LLM itself. The agent's entire knowledge is what's in the context window. Once text scrolls out of context, the agent has no access to it.",
        "timing": "2-3 minutes",
        "discussion": "Ask: If the agent forgets something midway through a task, where is that knowledge? (Scrolled out of context.) How would you prevent this?",
        "context": "Extended thinking modes complicate this: providers now offer hidden reasoning tokens. You see a summary of the chain-of-thought, but the full reasoning is opaque. You're billed for all reasoning, visible and hidden.",
        "transition": "Let me show you exactly what flows through that context window with a concrete example."
      }
    },
    {
      "type": "codeExecution",
      "title": "Agent Context Flow: Add Email Validation",
      "steps": [
        {
          "line": "SYSTEM: System prompt defines tools\nand agent behavior",
          "highlightType": "human",
          "annotation": "Foundation: agent capabilities and rules"
        },
        {
          "line": "USER: 'Add email validation to\nregistration endpoint'",
          "highlightType": "human",
          "annotation": "Explicit task specification"
        },
        {
          "line": "ASSISTANT: 'I'll search for the\nregistration endpoint first'",
          "highlightType": "prediction",
          "annotation": "LLM decides next action based on task"
        },
        {
          "line": "AGENT: Executes Grep to find\nregistration handler",
          "highlightType": "execution",
          "annotation": "Tool call—deterministic action"
        },
        {
          "line": "TOOL_RESULT: 'Found in\nsrc/handlers/user.ts:45'",
          "highlightType": "feedback",
          "annotation": "Operation result becomes context input"
        },
        {
          "line": "ASSISTANT: 'Found the endpoint.\nReading handler now.'",
          "highlightType": "prediction",
          "annotation": "LLM predicts next action with new data"
        },
        {
          "line": "AGENT: Executes Read on\nsrc/handlers/user.ts",
          "highlightType": "execution",
          "annotation": "Reads current code"
        },
        {
          "line": "TOOL_RESULT: File contents showing\nno email validation",
          "highlightType": "feedback",
          "annotation": "Code context feeds LLM decision-making"
        },
        {
          "line": "ASSISTANT: 'No validation present.\nAdding Zod schema...'",
          "highlightType": "prediction",
          "annotation": "Decision: which validation library to use"
        },
        {
          "line": "AGENT: Executes Edit to add\nvalidation schema",
          "highlightType": "execution",
          "annotation": "Code modification"
        },
        {
          "line": "TOOL_RESULT: 'Edit successful'",
          "highlightType": "feedback",
          "annotation": "Confirmation allows next step"
        },
        {
          "line": "AGENT: Executes Bash to run tests",
          "highlightType": "execution",
          "annotation": "Verification step"
        },
        {
          "line": "TOOL_RESULT: Test output shows\n✓ all validation tests pass",
          "highlightType": "feedback",
          "annotation": "Success signal—loop can complete"
        },
        {
          "line": "ASSISTANT: 'Email validation\nsuccessfully added.'",
          "highlightType": "prediction",
          "annotation": "Summary and completion"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the actual conversation that happens in the context window. Every step—system instructions, your task, tool calls, results, LLM responses—is text. The agent sees each tool result as text added to the conversation, which informs the next prediction. Notice how each step builds on the previous one. The Grep result tells the agent WHERE the code is. The Read result shows WHAT the code looks like. The test result shows WHETHER it works. This is feedback flowing through pure text.",
        "timing": "4-5 minutes—this is critical material",
        "discussion": "Ask: At which point does the agent know the task is complete? (When test output shows success.) If tests had failed, what would happen next? (Agent would analyze the failure and iterate.) What if the context window was too small to fit all this conversation? (Some early steps would scroll out and the agent would lose context.)",
        "context": "Real-world impact: Understanding this flow explains why clear task specs matter (more relevant context), why long-running tasks fail (context overflow), and why agents sometimes 'forget' what they were doing (scrolled out).",
        "transition": "This textual flow explains a critical advantage—the LLM is completely stateless. Your context is its entire world."
      }
    },
    {
      "type": "concept",
      "title": "The Stateless Advantage",
      "content": [
        "LLM has no hidden internal state—only\ncurrent context window",
        "Clean-slate exploration: No bias from\nprevious decisions",
        "Fresh context = fresh thinking without\ndefending earlier choices",
        "Unbiased code review: Agent audits its\nown code objectively",
        "You control behavior by controlling what's\nin the context"
      ],
      "speakerNotes": {
        "talkingPoints": "This seems like a limitation but it's actually powerful. The LLM doesn't 'remember' previous conversations or defend earlier decisions. Each new context is evaluated on merit. You can ask the agent to implement authentication with JWT in one conversation and sessions in another—each gets objective evaluation without defending the first choice. For code review, you can have the agent write code, then in a fresh conversation ask it to audit the same code without revealing authorship. The fresh context removes defensive bias.",
        "timing": "3-4 minutes",
        "discussion": "Scenario: You ask the agent to implement JWT auth and it makes a choice. In a new conversation, ask it to implement session-based auth. Ask: Would the agent defend JWT? No—fresh context, fresh evaluation. When might this be valuable?",
        "context": "Example from real work: Generate implementation in one context. Then: 'Security engineer: Review this code for vulnerabilities.' Fresh context triggers thorough scrutiny. Same code that passed 'looks sound overall' gets flagged for XSS vulnerabilities.",
        "transition": "This context engineering is the core skill you'll develop. But first, you need to understand the tools that agents use to manipulate context."
      }
    },
    {
      "type": "visual",
      "component": "AbstractShapesVisualization",
      "caption": "Fresh contexts enable unbiased code review",
      "speakerNotes": {
        "talkingPoints": "This visualization shows how the stateless nature creates opportunity. The same code flows through different contexts—sometimes it's 'write this feature,' sometimes it's 'security audit this code,' sometimes it's 'optimize this algorithm.' Each context brings fresh perspective without defensive bias. This isn't a workaround; it's a feature. You're designing different evaluation frames for the same code.",
        "timing": "1-2 minutes",
        "discussion": "Ask: What other evaluation frames might you design? (Performance review, accessibility audit, maintainability analysis, API compatibility check)",
        "context": "Workflow pattern: 1) Generate code, 2) Fresh context security review, 3) Fresh context performance review, 4) Merge best insights. Multi-perspective analysis.",
        "transition": "Now that you understand context as the agent's world, let's look at the tools it uses to read and manipulate that context."
      }
    },
    {
      "type": "concept",
      "title": "Tools: The Agent's Interface to Reality",
      "content": [
        "Built-in tools: Read, Edit, Bash, Grep,\nWrite, Glob optimized for code",
        "Not simple shell wrappers—includes edge\ncase handling and safety guardrails",
        "External tools via MCP (Model Context\nProtocol) for custom integrations",
        "Database clients, APIs, cloud platforms\nall accessible as tools",
        "Agent discovers available tools at\nruntime from configuration"
      ],
      "speakerNotes": {
        "talkingPoints": "Tools are how the agent interacts with the world outside the context window. Read, Edit, Bash—these aren't simple shell command wrappers. They include logic for handling edge cases, returning LLM-friendly output, and applying safety guardrails. MCP is the standardization layer that lets you add custom tools. A Postgres client becomes a tool. A Stripe API integration becomes a tool. The agent discovers what tools are available from your configuration and can call any of them.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What custom tools would be valuable in your projects? Database queries? API calls? File system operations beyond basic read/write?",
        "context": "Production example: Adding a database tool means the agent can query your schema, suggest migrations, validate data models. Without the tool, it's working blind.",
        "transition": "Built-in tools are powerful, but the real advantage of CLI agents over chat interfaces is what happens when you run multiple agents simultaneously."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs Chat Interfaces vs IDE Agents",
      "left": {
        "label": "Chat & IDE Agents",
        "content": [
          "Single conversation or window",
          "Blocked until task completes",
          "Context resets between sessions",
          "Manual copy-paste of code",
          "Sequential work only"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Multiple terminal tabs simultaneously",
          "Independent concurrent execution",
          "Persistent context per project",
          "Direct file system integration",
          "Parallel work across projects"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is where CLI agents shine. Open three terminal tabs, run agents on three projects simultaneously. They work independently without blocking each other. Chat interfaces and IDE agents are tightly coupled to a single window or conversation—you're blocked until they complete. CLI agents unlock parallelism. You can refactor project-a while debugging project-b while implementing project-c, each with its own agent, independent context, and concurrent execution. This is a game-changer for productivity at scale.",
        "timing": "2-3 minutes",
        "discussion": "Ask: How many times today did you wait for code completion? How many times did you context-switch because you were blocked? How would parallel agents change your workflow?",
        "context": "Real productivity gain: Instead of sequential back-and-forth cycles on one project, you run three agents in parallel. Time to completion is dramatically reduced.",
        "transition": "This brings us to the core theme: effective AI-assisted coding is about engineering context. You control behavior by controlling what's in that context window."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering: The Core Skill",
      "content": [
        "Context window is agent's entire world—\neverything it knows comes from text",
        "You control: system prompts, instructions,\ntask specifications, tool results",
        "Vague context → wandering behavior",
        "Precise, scoped context → predictable,\nsteered behavior",
        "Steer upfront with specificity or\ndynamically during execution"
      ],
      "speakerNotes": {
        "talkingPoints": "This is system design applied to text. You're already good at designing interfaces and contracts. Agent steering is the same principle: engineer the inputs (context) to produce predictable outputs (behavior). Vague instructions produce vague code. Specific constraints and clear requirements steer the agent exactly where you need it. You can engineer context upfront with clear specifications, or you can steer dynamically mid-task when the agent starts drifting. The stateless nature means you can even create a fresh context to objectively review earlier work.",
        "timing": "3-4 minutes",
        "discussion": "Ask: What makes a task spec clear vs vague? (Clear: specific files, constraints, expected outputs. Vague: generic goals, no scope.) Have students practice: take a vague request and make it precise.",
        "context": "Example: 'Improve the auth system' (vague) vs 'Add JWT refresh token support to AuthService.ts, maintain backward compatibility with session auth, add tests for token expiration edge cases' (precise). Which gets better results?",
        "transition": "This is the foundation for everything that follows. The rest of the course teaches how to apply context engineering to real coding scenarios."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents autonomously loop: perceive, reason,\nact, verify—no manual intervention",
        "Everything is text: understand context flow\nto predict agent behavior",
        "Stateless = opportunity: fresh contexts\nenable unbiased review and multi-perspective\nanalysis",
        "Context engineering is system design:\ncontrol inputs to steer behavior"
      ],
      "speakerNotes": {
        "talkingPoints": "You now understand agents at a fundamental level. Not as magic, but as systems: feedback loops powered by text, driven by token prediction, steered by context engineering. The LLM brain is stateless—it has no memory, only the current context window. That's your lever. Everything the agent does flows from the text in that context. The feedback loop is what makes agents autonomous. System prompts, tool results, and your instructions are all just text you're adding to the conversation. Master context engineering, and you master agent behavior.",
        "timing": "2 minutes",
        "discussion": "Ask students to explain the agent loop in their own words. Ask: Why is understanding the textual nature important? What does it tell you about context window limits?",
        "context": "These four points are the mental models for Lessons 3+ where we apply them to real scenarios.",
        "transition": "Next lesson: High-Level Methodology. We'll take these mental models and structure them into systematic approaches for different coding tasks."
      }
    }
  ]
}
