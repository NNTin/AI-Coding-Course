{
  "metadata": {
    "title": "Understanding Agents",
    "lessonId": "lesson-2-understanding-agents",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand agent feedback loops",
      "Recognize agents as textual systems",
      "Leverage statelessness strategically",
      "Engineer context to steer behavior"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Understanding Agents",
      "subtitle": "Feedback loops, textual systems, and context engineering",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "In Lesson 1, we established that LLMs are brains (token prediction engines) and agent frameworks are bodies (execution layers). Now we're diving into how these actually work together. By the end of this lesson, you'll understand agents not as magic black boxes, but as straightforward feedback loops operating on text. This mental model is the foundation for everything we'll do with agents.",
        "timing": "1 minute",
        "discussion": "Ask: 'Who here has used ChatGPT for coding? Who has used a CLI agent like Claude Code?' What differences have you noticed?",
        "context": "This lesson is where the abstraction becomes concrete. We're moving from 'what is an agent' to 'how does an agent actually work.'",
        "transition": "Let's start by understanding the core mechanism that makes agents different from chat."
      }
    },
    {
      "type": "concept",
      "title": "Agent Execution Loop",
      "content": [
        "Perceive: Observe current state (read files, run tests)",
        "Reason: Plan next action based on observations",
        "Act: Execute tools (edit files, run commands)",
        "Verify: Check if task is complete",
        "Iterate: Loop until goal is reached"
      ],
      "speakerNotes": {
        "talkingPoints": "The fundamental difference between a chat interface and an agent is this loop. When you use ChatGPT, YOU are the execution layer - you read the code, you edit files, you run tests, you ask follow-up questions. The agent closes this loop automatically. The LLM doesn't stop and wait for you; it generates text predicting the next action, executes it, observes the result, and continues. This is why agents can complete tasks autonomously.",
        "timing": "3-4 minutes",
        "discussion": "Ask students: 'Have you manually closed loops like this? Code suggestion → you edit → you test → you ask for fixes?' That's exactly what agents automate. Walk through a real example: 'Add authentication to this API' - the agent perceives the current API structure, reasons about authentication patterns, acts by editing files, verifies with tests, and if tests fail, iterates.",
        "context": "This is the mental model that separates agents from chat. Emphasize that this is happening automatically, without human intervention at each step.",
        "transition": "Now let's see this in action with a concrete example of chat vs. agent workflow."
      }
    },
    {
      "type": "comparison",
      "title": "Chat Interface vs Agent Workflow",
      "left": {
        "label": "Chat Interface (Manual Loop)",
        "content": [
          "You ask: 'Add authentication'",
          "LLM responds with code",
          "You manually edit files",
          "You run tests manually",
          "You ask: 'Fix this error'",
          "Repeat until done"
        ]
      },
      "right": {
        "label": "Agent Workflow (Automatic Loop)",
        "content": [
          "You ask: 'Add authentication'",
          "Agent perceives API files",
          "Agent reasons about patterns",
          "Agent edits files automatically",
          "Agent runs tests automatically",
          "Agent iterates until tests pass"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Left side is what you do with ChatGPT - you're in the loop making decisions. Right side is what happens with agents - the LLM is making predictions that drive tool calls, observing results, and continuing. The agent is autonomous because it's generating text that predicts tool calls and interpreting tool results as text back into the context.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How many iterations before you get it right with chat?' Usually 4-7. With agents? Usually 1-2. Why? Because the agent can see failures and self-correct immediately without losing context or waiting for you to react.",
        "context": "In production teams, chat interfaces become asynchronous workflows where one engineer writes prompt, another reviews code, another tests. Agents compress this into seconds.",
        "transition": "This automation is possible because of something fundamental: agents operate entirely on text."
      }
    },
    {
      "type": "concept",
      "title": "The Core Truth: Everything is Text",
      "content": [
        "No magic, no hidden reasoning engine",
        "Agent interaction = conversation in text buffer",
        "System prompts, your task, tool calls, results",
        "All flowing through single context window",
        "Agent only knows what's currently visible"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the demystifying moment. When people think of AI agents, they imagine some sophisticated reasoning system. The reality is simpler and more powerful: everything is text. Your task description is text. The agent's reasoning is text. Tool calls are text. File contents are text. Test results are text. It all flows through one large buffer. The LLM doesn't have secret internal state or hidden reasoning - it's just predicting the next token based on what's visible in the context.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'What happens if the agent \"forgets\" something?' The answer: it scrolled out of context. If you want it to remember, you have to keep it in the current window. This is why context engineering matters.",
        "context": "This textual view explains why agents sometimes seem to lose track of constraints or earlier decisions. It's not that they're being forgetful - it's token economics. Long context windows cost more per request. The agent has to decide what to keep visible.",
        "transition": "Let's look at what actually flows through the context in a real agent interaction."
      }
    },
    {
      "type": "codeExecution",
      "title": "What Context Actually Looks Like",
      "steps": [
        {
          "line": "SYSTEM: You are an expert developer. Use tools to implement features, fix bugs, and run tests.",
          "highlightType": "human",
          "annotation": "System instructions establish the agent's role"
        },
        {
          "line": "USER: Add email validation to the registration endpoint",
          "highlightType": "human",
          "annotation": "Your task specification enters the context"
        },
        {
          "line": "ASSISTANT: I'll examine the current registration code and add validation. Let me start by reading the endpoint.",
          "highlightType": "prediction",
          "annotation": "LLM predicts its next action"
        },
        {
          "line": "TOOL_CALL: Read src/routes/auth.ts",
          "highlightType": "execution",
          "annotation": "Agent executes the predicted tool"
        },
        {
          "line": "TOOL_RESULT: [File contents showing registration function...]",
          "highlightType": "feedback",
          "annotation": "Tool result becomes context for next prediction"
        },
        {
          "line": "ASSISTANT: I see the endpoint accepts email without validation. I'll add RFC 5322 validation and test it.",
          "highlightType": "prediction",
          "annotation": "Agent analyzes the file and predicts next steps"
        },
        {
          "line": "TOOL_CALL: Edit src/routes/auth.ts [add validation code]",
          "highlightType": "execution",
          "annotation": "Agent modifies the file"
        },
        {
          "line": "TOOL_RESULT: File modified successfully",
          "highlightType": "feedback",
          "annotation": "Confirmation returned to context"
        },
        {
          "line": "TOOL_CALL: Bash npm test --testNamePattern=registration",
          "highlightType": "execution",
          "annotation": "Agent runs tests to verify changes"
        },
        {
          "line": "TOOL_RESULT: PASS: registration endpoint validates emails correctly",
          "highlightType": "feedback",
          "annotation": "Test results inform final decision"
        },
        {
          "line": "ASSISTANT: Email validation added successfully. Tests pass. Implementation uses RFC 5322 standard.",
          "highlightType": "prediction",
          "annotation": "Agent concludes and summarizes"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is exactly what flows through the agent's context. Every line is text. The agent isn't thinking in some hidden layer - it's generating text that becomes visible in the context. Notice how tool results become inputs to the next prediction. The agent reads the result, incorporates it into its reasoning (as text), and predicts the next action. This is the complete loop in one text stream.",
        "timing": "4-5 minutes",
        "discussion": "Ask: 'Where does the agent's reasoning happen?' (In the visible text it generates.) 'What if it makes a wrong prediction?' (The tool result will show the error, and it predicts the correction.) 'What if it needs to remember context from step 3 when we're at step 11?' (It has to be in the current window - if it scrolled off, it's gone.)",
        "context": "This is pedagogically critical. This flow demystifies what 'context' actually means. It's not abstract - it's concrete text. Understanding this flow is the foundation for context engineering.",
        "transition": "Now, here's a crucial insight about how LLMs work in this context: they have no memory between requests."
      }
    },
    {
      "type": "concept",
      "title": "LLM Statelessness: A Superpower",
      "content": [
        "LLM has no hidden state or memory",
        "Only knows what's in current context window",
        "Previous responses are just text, not memories",
        "Each response generated purely from visible text",
        "You control behavior by controlling context"
      ],
      "speakerNotes": {
        "talkingPoints": "This sounds like a limitation but it's actually a massive advantage. The LLM doesn't 'remember' conversations. It doesn't have persistent internal state. Every response is generated from scratch based on what's currently visible in the context. What does this mean? It means you can run the same agent on the same problem in two different contexts and get different answers based on what you include. More importantly, it means the agent can objectively review its own work without defending previous decisions.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Why would you want an AI to NOT remember previous decisions?' The answer: defensive bias. If the agent generated code earlier and you tell it 'this approach is working well,' it will defend that choice. But if you give it the code without authorship context and say 'audit this for security issues,' it will give honest feedback. Statelessness enables unbiased code review.",
        "context": "In production, this manifests as: 'Hey Claude Code, implement authentication with JWT' in one terminal. 'Hey Claude Code, implement authentication with sessions' in another terminal. Each gets evaluated on merit. The agent doesn't argue that 'but we chose JWT earlier.'",
        "transition": "This statelessness combined with context control creates a powerful tool for directing agent behavior. Let's see this visually."
      }
    },
    {
      "type": "visual",
      "component": "AbstractShapesVisualization",
      "caption": "Clean context prevents agent confusion and hallucinations.",
      "speakerNotes": {
        "talkingPoints": "This visualization shows how context composition affects agent output. The key insight: the same code can trigger different analyses depending on what else is in the context. In one context where you prime the agent with security concerns, it produces 'Critical security vulnerabilities: XSS exposure.' In another context focused on performance, it produces 'Query N+1 pattern detected.' The agent isn't inconsistent - it's responding to the textual environment you've created. This is context engineering.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Have you seen an AI give different answers to the same question depending on how you asked it?' That's textual context at work. 'How could you use this intentionally?' Answer: by structuring prompts, system instructions, and conversation history to steer the agent toward the analysis you need.",
        "context": "This is the bridge between understanding 'how agents work' and 'how to work effectively with agents.' You're engineering the textual environment that shapes output.",
        "transition": "Now let's look at the actual tools that make agents useful."
      }
    },
    {
      "type": "concept",
      "title": "Tools: The Agent's Interface to the World",
      "content": [
        "Built-in tools: Read, Edit, Bash, Grep, Write, Glob",
        "Engineered for LLM efficiency, not just shell convenience",
        "External tools via MCP protocol",
        "Discover and configure at runtime",
        "Connect to databases, APIs, cloud platforms"
      ],
      "speakerNotes": {
        "talkingPoints": "Tools are how agents interact with the world. Built-in tools like Read and Edit aren't just wrappers around shell commands - they're designed specifically for LLM output. They handle edge cases, format output to be token-efficient, include safety guardrails. For example, Read doesn't dump the entire file - it shows line numbers, handles large files, respects encoding. Edit validates old_string uniqueness before replacing. These design decisions matter because you're paying per token. MCP (Model Context Protocol) extends this with pluggable tools for external systems - connect to databases, call APIs, deploy to cloud platforms.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'What tools would you want to give an AI agent in YOUR project?' Database connection? API calls? File system access? That's what MCP enables. The agent can discover these tools at runtime and use them as needed.",
        "context": "In practice, teams build custom MCP servers for their internal tools. A custom Postgres MCP server lets the agent directly query your database. A GitHub MCP server lets it read issues and create PRs. This multiplies agent usefulness for domain-specific work.",
        "transition": "Now, where should you run agents? CLI agents have significant advantages over chat and IDE agents."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents vs Chat and IDE Agents",
      "left": {
        "label": "Chat / IDE Agents",
        "content": [
          "Single window, single project, single context",
          "Coupled to UI, interrupt-driven workflows",
          "Limited to visual editing metaphor",
          "Manual context management between conversations",
          "Context resets on new conversation"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Multiple terminals, parallel workflows",
          "Command-driven, scripted interactions",
          "Full filesystem and tool access",
          "Persistent context across commands",
          "Context survives agent completion"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is a practical distinction that matters for productivity. With IDE agents like Cursor or Copilot, you're tied to that window. You want to refactor project-a while the agent is working on project-b? Tough. You're context-switching and losing focus. CLI agents in parallel terminals let you spawn multiple agents, each in its own context, all running simultaneously. You can supervise three agents implementing different features. More importantly, CLI agents don't reset context when they complete. The conversation persists. You can ask follow-up questions without losing the interaction history.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Who would benefit from parallel agents?' Teams with multiple projects, developers context-switching between tasks, multi-person work. 'How do you coordinate?' That's Lesson 7 - Planning & Execution. For now, just recognize that parallelism is a game-changer.",
        "context": "Real production example: one agent refactoring authentication (5-10 minutes), another implementing a new endpoint (5-10 minutes), another writing tests (5-10 minutes). Serial: 15-30 minutes. Parallel in separate terminals: 5-10 minutes.",
        "transition": "Okay, so agents are textual systems operating on context. Tools give them access. CLI agents enable parallelism. The core skill is context engineering."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering: The Core Skill",
      "content": [
        "Context is the agent's entire world",
        "You control what the agent knows by controlling visible text",
        "Vague context = wandering behavior",
        "Precise, scoped context = directed execution",
        "Steer upfront with prompts or dynamically mid-conversation"
      ],
      "speakerNotes": {
        "talkingPoints": "Everything we've learned leads to this: effective AI-assisted coding is about engineering context. The agent's perception, reasoning, and actions are all driven by text. You have multiple levers: system prompts (role definition), your task description (specificity), tool results (what information to include), conversation history (what to keep visible). An engineer skilled at designing interfaces and contracts is already thinking like a system architect. Context engineering is applying those same principles to text. Clear contracts, focused scope, explicit constraints - these patterns translate directly.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What makes a vague prompt like \"improve this code\" ineffective?' (No target, no constraints, no criteria.) 'What makes a specific prompt work?' (Clear goal, explicit constraints, expected output format.) 'How is this like API design?' (Tight contracts, explicit invariants, clear error conditions.)",
        "context": "This is the conceptual bridge from 'understanding agents' to 'working effectively with agents.' All the next lessons are about applying this principle in different contexts.",
        "transition": "Let's wrap up with what you can apply immediately."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents are feedback loops: perceive, reason, act, verify, iterate",
        "Everything is text flowing through context—no magic",
        "Statelessness is an advantage: enables unbiased review and clean-slate exploration",
        "Tools (built-in and MCP) extend agent capabilities",
        "CLI agents enable parallel work on multiple projects simultaneously",
        "Context engineering—controlling visible text—is the core skill"
      ],
      "speakerNotes": {
        "talkingPoints": "You now understand how agents actually work. Not as magic, but as straightforward systems that loop through perception, reasoning, and action. The key insight: everything is text. That demystifies what seemed complex and reveals a powerful principle: you control behavior by controlling context. This is your mental model for all future work with agents.",
        "timing": "2 minutes",
        "discussion": "Ask: 'What's one thing that surprised you about how agents actually work?' Let them surface misconceptions. Address them directly.",
        "context": "Preview the next lesson: Lesson 3 goes deep into High-Level Methodology - how to structure your thinking about AI-assisted work at the macro level. This lesson was 'how,' Lesson 3 is 'why and when.'",
        "transition": "Next: High-Level Methodology."
      }
    }
  ]
}
