{
  "metadata": {
    "title": "How Agents Work",
    "lessonId": "lesson-2-how-agents-work",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Understand agent execution loops",
      "Recognize context as text",
      "Leverage LLM statelessness strategically",
      "Choose appropriate agent tools"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "How Agents Work",
      "subtitle": "From Token Prediction to Autonomous Execution",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Building on Lesson 1 where we established LLMs as brains and frameworks as bodies, we now explore how they work together. By the end, you'll understand the execution loop, why everything is just text, and how statelessness is actually an advantage.",
        "timing": "1 minute",
        "discussion": "Quick check: Who has used a CLI coding agent vs only chat interfaces?",
        "context": "This lesson demystifies agent behavior so you can predict and control it.",
        "transition": "Let's start with the fundamental loop that drives all agent behavior."
      }
    },
    {
      "type": "concept",
      "title": "The Agent Execution Loop",
      "content": [
        "Perceive: Read context, files, state, history",
        "Reason: LLM generates plan and next action",
        "Act: Execute tool (Read, Edit, Bash, etc.)",
        "Observe: Process tool output",
        "Verify & Iterate: Goal achieved? Loop or complete"
      ],
      "speakerNotes": {
        "talkingPoints": "An agent isn't just an LLM responding to prompts. It's a feedback loop combining reasoning with action. The key distinction from chat: agents autonomously loop through this cycle without requiring manual intervention at each step.",
        "timing": "3 minutes",
        "discussion": "Think about a recent coding task. How many manual iterations did you do between prompts?",
        "context": "In production, this loop can run 10-50+ iterations for complex features, handling errors and edge cases automatically.",
        "transition": "Let's see this loop in action comparing chat vs agent workflows."
      }
    },
    {
      "type": "comparison",
      "title": "Chat Interface vs Agent Workflow",
      "left": {
        "label": "Chat Interface",
        "content": [
          "You ask: 'How should I add auth?'",
          "LLM responds with code",
          "You manually edit files",
          "You report errors back",
          "Repeat manual cycle"
        ]
      },
      "right": {
        "label": "Agent Workflow",
        "content": [
          "You say: 'Add authentication'",
          "Agent reads → plans → edits",
          "Agent runs tests automatically",
          "Agent fixes failures itself",
          "Reports when complete"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "The chat interface requires you to be the execution layer - copying code, running commands, reporting results. The agent closes the loop automatically, handling the full perceive-reason-act-verify cycle without requiring your intervention at each step.",
        "timing": "2-3 minutes",
        "discussion": "How much time do you spend on the manual execution part of chat-based workflows?",
        "context": "For a typical feature, chat might need 5-10 manual round trips. An agent completes it in one autonomous session.",
        "transition": "Now let's look under the hood at what's actually happening."
      }
    },
    {
      "type": "concept",
      "title": "The Fundamental Truth: Everything is Text",
      "content": [
        "No magic, no separate reasoning engine",
        "No hidden state outside the context window",
        "System prompt + your task + tool calls + results",
        "All exist as one continuous text stream",
        "The LLM predicts next tokens from this text"
      ],
      "speakerNotes": {
        "talkingPoints": "This is the insight that demystifies AI coding agents. When you see the agent 'thinking' or 'reasoning', that's not internal thought - it's text being generated in the context, visible to both you and the LLM itself. Extended thinking modes add hidden reasoning tokens, but the core remains: text in, text out.",
        "timing": "2-3 minutes",
        "discussion": "If you could see the raw context window during agent execution, what would you expect to find?",
        "context": "Understanding this helps you debug agent confusion - if it forgets something, it probably scrolled out of context.",
        "transition": "Let me show you exactly what flows through the context."
      }
    },
    {
      "type": "codeExecution",
      "title": "What the Context Actually Contains",
      "steps": [
        {
          "line": "SYSTEM: You are Claude Code, an AI coding assistant.\nYou have access to these tools:\n- Read(file_path): Read a file\n- Edit(file_path, old_string, new_string): Edit a file\n- Bash(command): Run a shell command\n[... more tool descriptions ...]",
          "highlightType": "human",
          "annotation": "System prompt defines agent behavior and available tools"
        },
        {
          "line": "USER: Add email validation to the registration endpoint",
          "highlightType": "human",
          "annotation": "Your task becomes part of the text stream"
        },
        {
          "line": "ASSISTANT: I'll search for the registration endpoint first.\n<tool_use>Grep(pattern='register', path='**/*.ts')</tool_use>",
          "highlightType": "prediction",
          "annotation": "LLM predicts next action based on context"
        },
        {
          "line": "TOOL_RESULT: Found in:\n- src/routes/auth.ts:12: router.post('/register', ...)\n- src/handlers/user.ts:45: export async function register(...)",
          "highlightType": "feedback",
          "annotation": "Tool output appended to context as text"
        },
        {
          "line": "ASSISTANT: Found the endpoint. Let me read the handler.\n<tool_use>Read('src/handlers/user.ts')</tool_use>",
          "highlightType": "prediction",
          "annotation": "LLM reasons about results, plans next step"
        },
        {
          "line": "TOOL_RESULT: [file contents showing no email validation]",
          "highlightType": "feedback",
          "annotation": "File content becomes available for analysis"
        },
        {
          "line": "ASSISTANT: No validation present. I'll add email validation.\n<tool_use>Edit(file, old_code, new_validation_code)</tool_use>",
          "highlightType": "prediction",
          "annotation": "LLM decides on implementation approach"
        },
        {
          "line": "TOOL_RESULT: Edit successful",
          "highlightType": "feedback",
          "annotation": "Confirmation added to context"
        },
        {
          "line": "ASSISTANT: Validation added. Running tests to verify.\n<tool_use>Bash('npm test user.test.ts')</tool_use>",
          "highlightType": "execution",
          "annotation": "Agent verifies its own work"
        },
        {
          "line": "TOOL_RESULT: PASS ✓ rejects invalid email ✓ rejects short password",
          "highlightType": "feedback",
          "annotation": "Test results confirm success"
        },
        {
          "line": "ASSISTANT: Email validation successfully added to the registration endpoint.",
          "highlightType": "summary",
          "annotation": "Task complete - loop exits"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This is the exact text flowing through the context window. System instructions, your task, every tool call and result - all as one continuous stream. The LLM sees its previous responses as text, not as memories. This is why context management is critical.",
        "timing": "4-5 minutes",
        "discussion": "Notice how each TOOL_RESULT becomes input for the next prediction. What happens if that result is wrong or truncated?",
        "context": "In production, this stream can grow to 100K+ tokens for complex tasks. Understanding this helps you structure work to avoid context overflow.",
        "transition": "This textual nature leads to a crucial insight about statelessness."
      }
    },
    {
      "type": "concept",
      "title": "The Stateless Advantage",
      "content": [
        "LLM has no memory between conversations",
        "Each response generated solely from current context",
        "You control what the agent knows",
        "Fresh context = unbiased evaluation",
        "Same code can get different reviews"
      ],
      "speakerNotes": {
        "talkingPoints": "This is often seen as a limitation, but it's actually a massive advantage. Clean-slate exploration means no bias from previous decisions. You can ask for JWT auth in one context, sessions in another - each evaluated on merit. Unbiased code review becomes possible.",
        "timing": "2-3 minutes",
        "discussion": "How could you use statelessness to get a more objective code review?",
        "context": "Teams use this for generate-then-review workflows: one context writes code, a fresh context audits it with full scrutiny.",
        "transition": "Let's see how context shapes agent behavior."
      }
    },
    {
      "type": "visual",
      "title": "Context and Agent Behavior",
      "component": "AbstractShapesVisualization",
      "caption": "Clean context prevents agent hallucinations and defensive bias.",
      "speakerNotes": {
        "talkingPoints": "The same code that gets 'looks sound overall' in one context can trigger 'Critical security vulnerabilities' in a fresh context. This enables generate-review-iterate workflows where the agent objectively audits its own work.",
        "timing": "2 minutes",
        "discussion": "What contexts would you create for security review vs performance review?",
        "context": "Multi-perspective analysis is a production pattern: security audit context, performance review context, maintainability review context.",
        "transition": "Now let's look at the tools that make this interaction possible."
      }
    },
    {
      "type": "comparison",
      "title": "Built-In vs External Tools",
      "left": {
        "label": "Built-In Tools",
        "content": [
          "Read, Edit, Bash, Grep, Write, Glob",
          "Engineered for LLM-friendly output",
          "Safety guardrails built-in",
          "Token-efficient formatting",
          "Optimized edge case handling"
        ]
      },
      "right": {
        "label": "MCP External Tools",
        "content": [
          "Database clients (Postgres, MongoDB)",
          "API integrations (Stripe, GitHub)",
          "Cloud platforms (AWS, GCP, Azure)",
          "Standardized plugin protocol",
          "Discovered at runtime"
        ]
      },
      "neutral": true,
      "speakerNotes": {
        "talkingPoints": "Built-in tools are purpose-built for common workflows - they're not just shell wrappers. MCP (Model Context Protocol) is a standardized way to add custom tools. Configure MCP servers and the agent discovers their capabilities at runtime.",
        "timing": "2-3 minutes",
        "discussion": "What external systems would you want your agent to connect to?",
        "context": "Teams often start with built-in tools and gradually add MCP servers as needs grow - database access, CI/CD integration, monitoring systems.",
        "transition": "Let me show you how MCP configuration looks."
      }
    },
    {
      "type": "code",
      "title": "MCP Server Configuration",
      "language": "json",
      "code": "// ~/.claude/mcp_settings.json\n{\n  \"servers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@modelcontextprotocol/server-postgres\",\n        \"postgresql://localhost/mydb\"\n      ]\n    }\n  }\n}",
      "caption": "External tools extend agent capabilities via standardized protocol",
      "speakerNotes": {
        "talkingPoints": "This configuration makes a Postgres database accessible to your agent. The agent discovers the available tools at startup and can query, insert, or modify data as part of its workflow. Same pattern works for any MCP-compatible server.",
        "timing": "2 minutes",
        "discussion": "What security considerations come to mind with database access from an agent?",
        "context": "Production setups typically use read-only connections or scoped permissions to limit blast radius.",
        "transition": "Now let's discuss why CLI agents are the preferred interface for serious development work."
      }
    },
    {
      "type": "comparison",
      "title": "CLI Agents: The Concurrent Work Advantage",
      "left": {
        "label": "Chat/IDE Agents",
        "content": [
          "Tightly coupled to single window",
          "Blocked until agent completes",
          "Context resets each conversation",
          "Manual copy-paste of code",
          "One project at a time"
        ]
      },
      "right": {
        "label": "CLI Agents",
        "content": [
          "Multiple terminal tabs = parallel agents",
          "Context-switch freely between projects",
          "Each agent works independently",
          "No IDE lock-in or window management",
          "True parallelism unlocked"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Open three terminal tabs, run agents on different projects simultaneously. Refactoring in project-a, debugging in project-b, implementing in project-c. Each agent keeps working independently while you context-switch freely.",
        "timing": "2-3 minutes",
        "discussion": "How many projects do you typically juggle in a day? How would parallel agents change your workflow?",
        "context": "Power users run 3-5 agents concurrently, checking in on each as needed. IDE agents block you to a single workflow.",
        "transition": "This brings us to the core skill: context engineering."
      }
    },
    {
      "type": "concept",
      "title": "Context Engineering: The Core Skill",
      "content": [
        "Context window is the agent's entire world",
        "You control what flows through it",
        "Vague context → wandering behavior",
        "Precise context → targeted execution",
        "Steer upfront or dynamically mid-conversation"
      ],
      "speakerNotes": {
        "talkingPoints": "Understanding agents as textual systems and LLMs as stateless reveals the core truth: effective AI-assisted coding is about engineering context to steer behavior. System prompts, your instructions, tool results - you control all of it. This is system design thinking applied to text.",
        "timing": "2-3 minutes",
        "discussion": "You're already good at designing interfaces and contracts. How might those skills apply to prompt engineering?",
        "context": "The rest of this course teaches how to apply system design skills to engineer context across real coding scenarios.",
        "transition": "Let's summarize the key insights from this lesson."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agents loop: perceive-reason-act-verify",
        "Everything is just text",
        "Statelessness enables unbiased review",
        "CLI agents unlock parallelism",
        "Context engineering steers behavior"
      ],
      "speakerNotes": {
        "talkingPoints": "The agent execution loop closes automatically what chat requires manually. Understanding the textual nature helps you debug and predict behavior. Statelessness is an advantage for objective analysis. CLI agents enable concurrent workflows. And context engineering is the core skill for effective AI-assisted development.",
        "timing": "2 minutes",
        "discussion": "Which insight changes how you'll approach your next agent interaction?",
        "context": "These fundamentals apply to all AI coding tools - Claude Code, Cursor, Copilot, and future tools.",
        "transition": "Next lesson, we'll cover the high-level methodology for applying these concepts in practice."
      }
    }
  ]
}