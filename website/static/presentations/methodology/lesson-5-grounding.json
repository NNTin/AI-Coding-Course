{
  "metadata": {
    "title": "Grounding: Injecting Reality Into Agents",
    "lessonId": "lesson-5-grounding",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Prevent agent hallucinations with grounding",
      "Choose tools by codebase scale",
      "Master sub-agent architectures",
      "Combine code and web grounding"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Grounding: Injecting Reality Into Agents",
      "subtitle": "How to prevent hallucinations and anchor agents in your actual system",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson covers the engineering techniques that keep agents grounded in reality instead of hallucinating plausible but wrong solutions. Without grounding, agents work from frozen training data and guess your architecture. With grounding, agents see your actual codebase and current documentation.",
        "timing": "1 minute",
        "discussion": "Ask students: 'Have you experienced an AI assistant confidently suggesting code that doesn't match your system?'",
        "context": "This is the capstone of the methodology module. Understanding grounding is essential for production AI systems.",
        "transition": "Let's start with the fundamental problem: Why do agents hallucinate?"
      }
    },
    {
      "type": "concept",
      "title": "The Hallucination Problem",
      "content": [
        "Agent generates JWT auth using training patterns (Jan 2025 knowledge)",
        "Your codebase actually uses sessions, not JWTs",
        "Solution is statistically plausible but completely wrong",
        "Agent confidently suggests something that doesn't exist in your system"
      ],
      "speakerNotes": {
        "talkingPoints": "The core issue is simple: agents don't know your codebase exists. The context window is the agent's entire world. Without explicit grounding, agents generate solutions based on generic patterns from training data, not your actual architecture. These solutions sound right but don't fit your system.",
        "timing": "2-3 minutes",
        "discussion": "Share the JWT vs sessions example. Ask: 'Why do you think the agent was confident even though it was wrong?'",
        "context": "This is the mental model that underlies everything in this lesson. The context window is the boundary between what the agent knows and doesn't know.",
        "transition": "So how do we solve this? The answer is grounding—injecting your actual system into the context window."
      }
    },
    {
      "type": "concept",
      "title": "Grounding: The Solution",
      "content": [
        "Without: Generic patterns, hallucinations, wrong solutions",
        "With: Actual codebase, current docs, confident correct answers",
        "Grounding injects external information into context before generation",
        "External sources: your code, docs, best practices, security advisories"
      ],
      "speakerNotes": {
        "talkingPoints": "Grounding is the operational technique for anchoring agents in reality. You retrieve relevant external information—your actual codebase patterns, current documentation, best practices—and feed it into the context window before the agent generates. This shifts the agent from training-data-based generation to context-aware generation.",
        "timing": "2 minutes",
        "discussion": "Ask: 'What information do you think the agent needs to know about your system to solve problems correctly?'",
        "context": "Grounding is about information architecture, not magic. You control what the agent sees, so you control what it can generate.",
        "transition": "Now we need to understand how agents discover your codebase. That's agentic search."
      }
    },
    {
      "type": "concept",
      "title": "Agentic Search: Discovery at Scale",
      "content": [
        "Agent autonomously calls Grep, Read, Glob tools",
        "Works beautifully under 10,000 LOC (2-3 searches = clean context)",
        "Breaks at scale: 'authentication' search returns 80+ files",
        "Results flood context, pushing constraints into ignored middle"
      ],
      "speakerNotes": {
        "talkingPoints": "Agentic search is how agents discover your codebase. The agent decides what to search, reads files, and builds a mental model. In small projects (under 10K lines), two or three Grep searches return 5-10 files and 15K tokens total—highly efficient. At scale (100K lines), a single search returns 80+ files and 60K+ tokens, consuming half your effective context before the agent even starts reasoning.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'What happens to your constraints when search results consume 60,000 tokens?'",
        "context": "This is the inflection point where agentic search stops working. Understanding this helps students recognize when to switch tools.",
        "transition": "This brings us to the context window reality: what you see advertised is not what you get."
      }
    },
    {
      "type": "visual",
      "component": "UShapeAttentionCurve",
      "title": "The Context Window Illusion",
      "caption": "Middle context gets skimmed; critical constraints disappear.",
      "speakerNotes": {
        "talkingPoints": "Claude Sonnet advertises 200,000 tokens. Reality: reliable attention spans 60,000-120,000 tokens (30-60% of advertised). This is the U-shaped attention curve—transformer architecture under realistic constraints. Beginning and end get strong attention. Middle gets skimmed or missed entirely. This isn't a bug; it's how transformers work. If you fill your context and push constraints into the middle, the agent doesn't see them.",
        "timing": "2 minutes",
        "discussion": "Ask: 'If you put your critical requirements at position 50,000 tokens and then load 80,000 tokens of search results before it, what happens?' This builds intuition for why context management matters.",
        "context": "The U-curve explains why naive agentic search fails at scale. The more you search, the more results push your original task deeper into the ignored middle.",
        "transition": "So we have three approaches to solve this: semantic search, sub-agents, and architectural choices. Let's explore each."
      }
    },
    {
      "type": "concept",
      "title": "Solution 1: Semantic Search",
      "content": [
        "Query by meaning, not keywords—'auth middleware' finds JWT, sessions, OAuth",
        "Vector embeddings capture semantic relationships",
        "IDE assistants include it; CLI agents need MCP servers",
        "Extends scale to 100,000+ LOC with fewer false positives"
      ],
      "speakerNotes": {
        "talkingPoints": "Semantic search is fundamentally different from keyword matching. Your code gets converted to high-dimensional vectors (768-1536 dimensions) capturing semantic meaning. 'Auth middleware', 'login verification', and 'JWT validation' map to nearby vectors—the system understands semantic relationships. You ask 'Find authentication middleware' and get relevant code even if it never mentions those exact terms. IDE assistants (Cursor, Windsurf) include this out-of-the-box. CLI agents need MCP servers like ChunkHound or Claude Context.",
        "timing": "2 minutes",
        "discussion": "Ask: 'Why would semantic search return better results than just searching for the word \"auth\"?'",
        "context": "Semantic search is a tactical improvement. It extends scale but doesn't solve the fundamental context pollution problem. You still load results into your main context.",
        "transition": "For larger codebases, we need architectural isolation. That's where sub-agents come in."
      }
    },
    {
      "type": "concept",
      "title": "Solution 2: Sub-Agents for Isolation",
      "content": [
        "Orchestrator delegates research task to isolated sub-agent context",
        "Sub-agent processes 50,000-150,000 tokens internally",
        "Returns 2,000-5,000 token synthesis to orchestrator",
        "Costs more tokens but delivers first-iteration accuracy"
      ],
      "speakerNotes": {
        "talkingPoints": "A sub-agent is an agent invoked by another agent, like a function call for agents. The orchestrator writes a research prompt: 'Find all JWT authentication code and explain the implementation.' The sub-agent runs in isolated context with its own tools, processing 50K-150K tokens of searches and file reads. When done, it returns a concise synthesis (2K-5K tokens) summarizing findings. The orchestrator's context stays clean throughout. You pay to process tokens in both contexts, increasing total cost, but you get first-iteration accuracy instead of multiple correction cycles from context pollution.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Why would you pay more tokens to get fewer tokens back?' Guide toward: accuracy and clean reasoning in the main agent.",
        "context": "Sub-agents are the architectural pattern that scales to production. Claude Code includes Explore (autonomous). ChunkHound (structured via MCP) is the only option for other CLI agents.",
        "transition": "Sub-agents come in two flavors: autonomous and structured. Let's understand the trade-offs."
      }
    },
    {
      "type": "comparison",
      "title": "Sub-Agent Architectures",
      "left": {
        "label": "Autonomous",
        "content": [
          "Agent decides search strategy and sequences",
          "Flexible across different tasks",
          "Cheaper to build and run",
          "Degrades in large codebases (suboptimal exploration)",
          "Example: Claude Code's Explore agent"
        ]
      },
      "right": {
        "label": "Structured",
        "content": [
          "Deterministic control plane defines algorithm",
          "LLM makes tactical decisions within structure",
          "Reliable at extreme scale",
          "Higher cost to build and run",
          "Example: ChunkHound with BFS traversal"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Autonomous agents are simpler: you give them tools and let them decide what to do. They're flexible and cheap but make suboptimal choices in huge codebases. Structured agents define the search algorithm explicitly (like breadth-first traversal through architecture), and the LLM makes tactical decisions within that structure (relevance ranking, expansion criteria). More complex to build, but maintains consistency at million-line scales.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'When would you prefer flexibility over consistency? When would you prefer the opposite?'",
        "context": "This is the architectural trade-off. Choose based on your codebase size and consistency requirements.",
        "transition": "Now let's build the decision matrix for choosing tools by your specific scale."
      }
    },
    {
      "type": "concept",
      "title": "Choosing Tools by Scale",
      "content": [
        "Under 10,000 LOC: Agentic search works reliably",
        "10,000-100,000 LOC: Add semantic search or Explore agent",
        "Over 100,000 LOC: Use structured research (ChunkHound)",
        "At 1,000,000+ LOC: ChunkHound becomes essential—progressive aggregation"
      ],
      "speakerNotes": {
        "talkingPoints": "The tool you need depends entirely on your codebase size. Under 10K lines, agentic search is sufficient—searches return manageable results. Around 10K lines, you hit the inflection point where semantic search or Claude Code's Explore agent starts adding value. At 100K lines, autonomous exploration misses architectural connections. You need structured research like ChunkHound's multi-hop BFS with hybrid semantic + symbol search. At 1M lines, it's essential—it's the only approach with progressive aggregation across modules.",
        "timing": "2 minutes",
        "discussion": "Ask students to estimate their codebase size. 'What tool would you start with? What would trigger switching?'",
        "context": "This decision matrix is the practical output of everything we've covered. Use codebase scale as your decision point.",
        "transition": "Code grounding is half the picture. You also need to ground in current external knowledge. Let's discuss web grounding."
      }
    },
    {
      "type": "concept",
      "title": "Web Grounding: Current Knowledge",
      "content": [
        "Built-in search works but pollutes context (8K-15K tokens per query)",
        "Synthesis tools (Perplexity) compress to 3K-8K tokens per result",
        "Sub-agents (ArguSeek) maintain state across 100+ sources",
        "Same progression as code grounding—scale determines your tool"
      ],
      "speakerNotes": {
        "talkingPoints": "Web grounding follows the exact same pattern as code grounding. You need current information: API docs, best practices, security advisories, research. Built-in web search works initially but hits context limits—each search consumes 8K-15K tokens, each page fetch adds 3K-10K tokens. Synthesis tools like Perplexity compress raw results but hit context limits after 3-5 queries and use Bing instead of Google. ArguSeek is a web research sub-agent with isolated context and semantic state management. It processes 12-30 sources per call, supports tens of calls per task, and uses Google Search API for quality.",
        "timing": "2 minutes",
        "discussion": "Ask: 'If you're researching a new library, how many sources do you think you need to understand it?'",
        "context": "Most students underestimate how much research is needed for production decisions. This shows why sub-agents are valuable.",
        "transition": "So code grounding and web grounding are both essential. Let's see how they work together in production."
      }
    },
    {
      "type": "concept",
      "title": "Production Pattern: Combined Grounding",
      "content": [
        "Code grounding: Your architecture, existing patterns, current state",
        "Web grounding: Latest APIs, security advisories, best practices",
        "Together: Solutions that work for your system using current standards",
        "Prevents: Outdated patterns OR solutions that don't fit your architecture"
      ],
      "speakerNotes": {
        "talkingPoints": "In production, you never use code grounding alone or web grounding alone. Code-only prevents hallucinations but risks outdated patterns. Web-only gives current best practices but doesn't fit your architecture. Combined, you ground solutions in your actual system using current standards. Before implementing authentication, you ground in your codebase patterns AND current security advisories about JWT vulnerabilities. Before choosing a new database library, you ground in your existing data layer architecture AND current benchmarks for that library.",
        "timing": "2-3 minutes",
        "discussion": "Walk through an example: 'We're adding user notifications. What would you ground in code? What would you ground in web?'",
        "context": "This is the operational reality of production systems. Single-source grounding is a recipe for either outdated or misfit solutions.",
        "transition": "Let's summarize the core principles before wrapping up."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Agent hallucination happens because the context window is their entire world",
        "The U-curve means middle context gets ignored—manage information placement strategically",
        "Choose grounding tools by codebase scale, not feature richness",
        "Production systems combine code and web grounding for solutions that work and scale"
      ],
      "speakerNotes": {
        "talkingPoints": "These four points are the operational foundation for grounding in production. First, internalize that agents can only work with what's in their context. Second, understand that advertised context limits are illusions—the U-curve governs real attention. Third, make tool decisions based on your specific scale—don't over-engineer or under-invest. Fourth, always combine sources—your system's patterns plus current best practices.",
        "timing": "2 minutes",
        "discussion": "Ask: 'Which of these four points most changes how you think about using AI agents?'",
        "context": "These takeaways connect back to the lesson's opening problem. Now students have the framework to prevent those hallucinations.",
        "transition": "This completes the methodology module. You now have planning (Lesson 3), communication (Lesson 4), and context management (Lesson 5). These are the foundations for production AI systems."
      }
    }
  ]
}
