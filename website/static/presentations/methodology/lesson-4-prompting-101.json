{
  "metadata": {
    "title": "Lesson 4: Prompting 101",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Understand prompting as pattern completion, not conversation",
      "Write specific, action-oriented prompts with clear constraints",
      "Use Chain-of-Thought for complex multi-step tasks",
      "Apply structure and personas strategically",
      "Avoid common failure modes: negation and math"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson challenges how engineers think about prompts. They're not requests to a conversational partner—they're initializations of a pattern completion engine. The AI model predicts what comes next based on statistical patterns from training data. Understanding this distinction changes everything about how you write prompts. We'll explore specific techniques that leverage this mental model for production-quality code.",
        "timing": "1-2 minutes",
        "discussion": "Ask: Has anyone used an AI assistant and felt surprised by what it generated? We'll see why that happens.",
        "context": "Most engineers approach prompting like they're emailing a colleague—polite, conversational, indirect. This inefficiency compounds in production. Better understanding = better results.",
        "transition": "Let's start with the fundamental mental model: what's actually happening when you write a prompt."
      }
    },
    {
      "type": "concept",
      "title": "Pattern Completion: The Core Model",
      "content": [
        "AI is a sophisticated text predictor, not a reasoner",
        "Your prompt is the pattern start; model predicts what comes next",
        "Specificity constrains the completion space",
        "Every token in your prompt influences what the model generates",
        "This is why vague prompts produce vague results"
      ],
      "speakerNotes": {
        "talkingPoints": "The moment you stop thinking of the AI as a conversational partner and start thinking of it as a pattern completion engine, your prompting improves dramatically. When you write a prompt, you're drawing the beginning of a pattern. The model's job is to predict what naturally follows based on similar patterns it saw during training. More specific patterns = more constrained completion space. A vague pattern has thousands of plausible continuations; a specific pattern has maybe tens. This is purely mechanical—no reasoning, no understanding, just statistical prediction.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why do you think a vague prompt produces vague results? Think about the math of it.",
        "context": "In production, the difference between vague and specific prompts often means 1-2 iteration cycles vs 5+ cycles. Token efficiency improves. Quality improves. Debugging becomes straightforward.",
        "transition": "Now let's see how this plays out in practice. The first technique is clear, imperative commands."
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands: The Foundation",
      "leftCode": {
        "label": "Ineffective",
        "language": "text",
        "code": "Could you help me write a function\nto validate email addresses?\nThanks in advance!"
      },
      "rightCode": {
        "label": "Effective",
        "language": "text",
        "code": "Write a TypeScript function that\nvalidates email addresses per\nRFC 5322. Handle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\n\nReturn { valid: boolean,\n  reason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The left side treats the AI like a colleague—polite, conversational, vague. The right side treats it like a pattern completion engine—direct, specific, structured. Notice the right side doesn't ask; it commands. It's imperative: 'Write a TypeScript function.' This establishes the pattern start immediately. The ineffective version forces the model to guess: What language? What standard? What edge cases? Each guess adds uncertainty.",
        "timing": "2-3 minutes",
        "discussion": "Ask: What's the model actually completing when it sees 'Could you help me'? What about when it sees 'Write a TypeScript function'?",
        "context": "In production code reviews, vague prompts lead to code that compiles but doesn't meet requirements. You iterate 5+ times. Specific prompts reduce that to 1-2 iterations. The time difference is measurable.",
        "transition": "Let's look at how specificity actually constrains the completion. This is where action verbs become critical."
      }
    },
    {
      "type": "code",
      "title": "Pattern Start Drives Completion",
      "language": "typescript",
      "code": "function authMiddleware(\n  req: Request,\n  res: Response,\n  next: NextFunction\n): void {\n  // Model completes based on\n  // pattern: middleware function",
      "caption": "Specific pattern start constrains completion space to matching code",
      "speakerNotes": {
        "talkingPoints": "When you write 'function authMiddleware(req, res, next)' you're drawing the beginning of a middleware pattern. The model recognizes this pattern shape and completes with code that fits: check for auth header, verify token, extract payload, call next(). Compare this to just writing 'authentication function' — the completion space expands enormously. The model might generate auth logic for a database layer, API client, CLI tool, etc. Pattern specificity = constrained completion.",
        "timing": "1-2 minutes",
        "discussion": "Ask: What patterns do you frequently use in your codebase? How would you write them specifically enough to get consistent completions?",
        "context": "This principle underlies all the techniques we'll see: action verbs, constraints, personas, CoT. They all work by making the pattern more specific.",
        "transition": "Now let's look at action verbs—the words you use matter more than you'd expect."
      }
    },
    {
      "type": "comparison",
      "title": "Action Verbs: Weak vs Strong",
      "left": {
        "label": "Weak Verbs",
        "content": [
          "Make a function",
          "Fix the bug",
          "Update the docs",
          "Improve performance"
        ]
      },
      "right": {
        "label": "Strong Verbs",
        "content": [
          "Write a TypeScript function",
          "Debug null pointer in UserService.ts:47",
          "Add JSDoc to exported functions in auth.ts",
          "Optimize query to use indexed columns"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Strong verbs establish clear, specific patterns. 'Write' implies code that's ready to use. 'Debug' narrows scope to a specific error and location. 'Add JSDoc to exported functions in auth.ts' is precise enough that the model knows exactly what to do. Weak verbs like 'improve' or 'fix' leave everything ambiguous. The model must guess what aspect you care about. In training data, 'improve performance' appears in contexts ranging from adding caches to rewriting algorithms. The model's completion is essentially random.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why do you think 'Debug null pointer in UserService.ts:47' produces better results than 'Fix the bug'? What's the model completing differently?",
        "context": "Production teams report that switching from weak to strong verbs reduces iteration cycles by 40-50%. It's a simple rule with measurable impact.",
        "transition": "Strong verbs are half the equation. The other half is constraints—defining the boundaries of what you want."
      }
    },
    {
      "type": "codeComparison",
      "title": "Constraints as Guardrails",
      "leftCode": {
        "label": "Unconstrained",
        "language": "text",
        "code": "Add authentication to the API"
      },
      "rightCode": {
        "label": "Constrained",
        "language": "text",
        "code": "Add JWT authentication to POST\n/api/users endpoint:\n- Verify token in Authorization\n  header\n- Reject requests with invalid\n  or expired tokens\n- Extract user ID from token\n- Pass user to request handler\n\nSupport tokens valid for 24 hours"
      },
      "speakerNotes": {
        "talkingPoints": "Unconstrained prompts leave questions: What authentication? JWT? OAuth? Session tokens? Which endpoints? When does the token expire? The model fills gaps with assumptions—often contradictory ones. The constrained version answers these questions explicitly. No guessing. No assumptions. The completion space shrinks dramatically, and the model produces consistent, predictable code.",
        "timing": "2-3 minutes",
        "discussion": "Ask: If you saw an unconstrained prompt, what different implementations might you generate? Why?",
        "context": "Constraints eliminate ambiguity. They're your primary control mechanism for directing the model. In production, clear constraints reduce code review cycles because the code matches your intent the first time.",
        "transition": "We've covered imperatives, verbs, and constraints. Now let's talk about personas—when they help, and when they waste tokens."
      }
    },
    {
      "type": "concept",
      "title": "Personas: Vocabulary, Not Knowledge",
      "content": [
        "Personas bias vocabulary distribution toward domain-specific terms",
        "Use when: domain terminology matters or consistency across tasks",
        "Skip when: the task is straightforward and context is clear",
        "Example: 'security engineer' retrieves threat modeling patterns, not new knowledge",
        "It's a semantic shortcut—not adding capability, just redirecting which training patterns activate"
      ],
      "speakerNotes": {
        "talkingPoints": "A persona like 'You are a security engineer' doesn't teach the model security—it changes which vocabulary gets predicted. 'Security engineer' biases toward terms like 'threat model,' 'attack surface,' 'least privilege' that appear in security-focused training data. These terms act as semantic anchors, retrieving different patterns than generic terms like 'check for issues.' The persona is efficient only when vocabulary matters. If you need security analysis, a persona saves you from listing every security term explicitly. If you're asking for a simple function signature, adding a persona wastes tokens.",
        "timing": "2-3 minutes",
        "discussion": "Ask: When would a security persona help? When would it waste tokens? What about a 'performance engineer' persona?",
        "context": "This vocabulary principle applies everywhere: searching code with ChunkHound, researching with ArguSeek, querying vector databases. 'Authentication middleware patterns' finds different code than 'login code.' Choose vocabulary strategically.",
        "transition": "Imperatives, constraints, and personas are about precision. Now let's look at control over execution: Chain-of-Thought."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought: Explicit Execution Path",
      "leftCode": {
        "label": "Without CoT",
        "language": "text",
        "code": "Review this code for security\nvulnerabilities and suggest fixes"
      },
      "rightCode": {
        "label": "With CoT",
        "language": "text",
        "code": "Review this code for security.\nFollow these steps:\n\n1. Identify authentication checks\n2. Verify authorization logic\n3. Check for injection vectors\n4. Review error handling\n5. Identify secrets in code\n6. Suggest fixes with priority\n\nStop after step 5 before\nsuggesting fixes"
      },
      "speakerNotes": {
        "talkingPoints": "Without CoT, the model rushes through a holistic analysis and might miss issues because it's optimizing for speed. With CoT, you dictate the execution path step-by-step. The model can't skip steps or take shortcuts. Each step must complete before the next begins. This is particularly powerful for QA workflows where you need methodical execution. You see exactly what happened at each step, making debugging straightforward. If the model misses an injection vector at step 3, you know where to focus.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why might step-by-step execution catch more issues? What happens if you let the model optimize for speed?",
        "context": "In production QA, CoT-guided analysis catches 15-25% more issues than unguided reviews. The time cost is minimal; the bug prevention is substantial. See Lesson 8 for production QA workflow examples.",
        "transition": "CoT is about execution control. Now let's talk about information structure—how formatting directs the model's attention."
      }
    },
    {
      "type": "code",
      "title": "Structure Directs Attention",
      "language": "markdown",
      "code": "# Implementation Task: API Rate Limiter\n\n## What to build\nCreate a rate limiter middleware\n\n## How to build it\nUse token bucket algorithm\n\n## How to test\nWrite tests for edge cases\n\n## What to avoid\nDon't use in-memory state",
      "caption": "Markdown structure makes requirements scannable and highlights distinct concerns",
      "speakerNotes": {
        "talkingPoints": "Structure organizes information and directs the model's attention. Markdown is highly information-dense: headings, lists, code blocks convey semantic meaning with minimal overhead. Compare a wall of paragraph text to the same information structured with headings. The structured version uses fewer tokens but communicates more clearly because the semantic relationships are explicit. The model's attention mechanism latches onto structure—headings become high-weight tokens, lists become clear sections to process independently.",
        "timing": "2 minutes",
        "discussion": "Ask: If you had to review a 500-word paragraph vs a structured document with the same content, which would you prefer? Why?",
        "context": "Well-structured prompts improve token efficiency and reduce iteration cycles. In production, this compounds: fewer iterations × faster individual completions = significant time savings.",
        "transition": "Now let's look at failure modes. Understanding what breaks helps you avoid it."
      }
    },
    {
      "type": "codeComparison",
      "title": "Negation: Affirmation Bias",
      "leftCode": {
        "label": "Risky: Negation",
        "language": "text",
        "code": "Do NOT store passwords\nin plain text. Do NOT use\nunsafe string operations.\nDo NOT hardcode secrets."
      },
      "rightCode": {
        "label": "Better: Negation + Opposite",
        "language": "text",
        "code": "Do NOT store passwords in\nplain text.\nInstead, always store\npasswords as bcrypt hashes\nwith 12+ salt rounds.\n\nDo NOT use unsafe string ops.\nInstead, use parameterized\nqueries."
      },
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with negation because attention mechanisms treat 'NOT' as just another token competing for weight. When 'NOT' receives low attention during processing, the model focuses on the concepts mentioned ('passwords,' 'plain text') rather than their negation. This 'affirmation bias' means the model might generate exactly what you told it to avoid. The fix: state negation explicitly, then immediately follow with the positive opposite. 'Do NOT store plain text' tells the model what's forbidden. 'Instead, always use bcrypt' tells it what to do. The 'instead' creates a semantic connection that's harder to miss.",
        "timing": "2-3 minutes",
        "discussion": "Ask: Why would a model focus on 'passwords' and 'plain text' but miss 'NOT'? Think about attention mechanisms.",
        "context": "This is why production code from generic prompts often contains the security issues you explicitly told it to avoid. The pattern to remember: negation + immediate positive alternative.",
        "transition": "One more failure mode: mathematical ability."
      }
    },
    {
      "type": "comparison",
      "title": "Math Limitations: Don't Ask, Command Code",
      "left": {
        "label": "Risky: Ask for Math",
        "content": [
          "Calculate optimal cache size for 1M users",
          "Model generates plausible-sounding numbers",
          "Numbers are often completely wrong",
          "No verification—hallucinations look confident"
        ]
      },
      "right": {
        "label": "Better: Write Code",
        "content": [
          "Write a function to calculate cache size",
          "Takes user count, hit ratio, memory limit as parameters",
          "Model generates code that computes values",
          "Code is deterministic and verifiable"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "LLMs are probabilistic text predictors, not calculators. When you ask for math, the model generates plausible-sounding numbers—often completely wrong. The confidence in the answer is uncorrelated with accuracy. Instead of asking for calculations, have the model write code. Code is deterministic. You can test it. You can verify the math. The model's job becomes writing correct logic, not hallucinating numbers. This applies to any scenario where precision matters: performance calculations, sizing decisions, cost estimates. Always have the model write code that does the math.",
        "timing": "2 minutes",
        "discussion": "Ask: If a model generated a number, how would you verify it's correct? What if it generated code instead?",
        "context": "Production teams that follow this pattern avoid common mistakes: cache sizes that are too small, database connection pools that are inadequate, rate limits that are too loose. The discipline of writing verifiable code prevents these errors.",
        "transition": "Let's wrap up with the key takeaways that unify everything we've covered."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Pattern completion is precise—specificity constrains completion space",
        "Use action verbs, constraints, and structure to direct the model",
        "Chain-of-Thought controls execution for complex, multi-step tasks",
        "Personas are vocabulary shortcuts; use them strategically",
        "Avoid negation alone; always pair with positive alternatives"
      ],
      "speakerNotes": {
        "talkingPoints": "Everything we've covered comes down to one principle: be intentional about the pattern you're drawing. Vague patterns produce vague results because the completion space is huge. Specific patterns produce consistent results because the model has fewer options. Action verbs, constraints, structure, CoT—they're all mechanisms for making your pattern more specific and directing the model's attention. This isn't about being nice to the AI or following polite conventions. It's about engineering the input to get predictable, production-ready output.",
        "timing": "3-4 minutes",
        "discussion": "Ask: What's one technique you'll use in your next prompt? Which challenge you most: specificity, structure, or avoiding negation?",
        "context": "Senior engineers who internalize these principles report 40-50% reduction in iteration cycles and measurably better code quality from AI assistants. The return on this skill is immediate and compounds over time.",
        "transition": "Next lesson: Grounding. We'll look at how to anchor prompts in specific context—your codebase, documentation, and domain knowledge."
      }
    }
  ]
}