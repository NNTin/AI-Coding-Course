{
  "metadata": {
    "title": "Lesson 4: Prompting 101",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "35-45 minutes",
    "learningObjectives": [
      "Master pattern completion mechanics",
      "Write specific, action-oriented prompts",
      "Control execution with structure",
      "Avoid predictable failure modes"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson reframes how to think about AI coding assistants. They're not conversational partners—they're pattern completion engines. Understanding this distinction changes everything about how you write prompts. You'll learn why the words you choose matter, how to structure information for maximum clarity, and how to avoid the pitfalls that catch most engineers.",
        "timing": "1 minute",
        "discussion": "Ask: How many of you write prompts like you're asking a colleague for help? That's the mental model we're going to overturn.",
        "context": "This is foundational. The rest of the course—grounding (lesson 5), planning, execution—all build on this understanding of pattern completion.",
        "transition": "Let's start with the fundamental insight: what's actually happening when you write a prompt?"
      }
    },
    {
      "type": "concept",
      "title": "Prompting as Pattern Completion",
      "content": [
        "You're drawing the beginning of a pattern",
        "Model predicts what naturally follows",
        "Not a question—it's sequence initialization",
        "Specificity constrains the completion space",
        "Precision engineering, not conversation"
      ],
      "speakerNotes": {
        "talkingPoints": "When you write a prompt, you're not asking a question. You're initializing a pattern that the model will complete based on statistical patterns from training data. If you write 'Write a TypeScript function that validates...', you're literally drawing the beginning of a code pattern. The model's job is to predict what comes next. The more specific your pattern start, the more constrained the completion space—which means fewer wrong answers.",
        "timing": "3-4 minutes",
        "discussion": "Think about how you'd complete this pattern yourself: 'Write a TypeScript function that validates email addresses per RFC 5322...' What would naturally follow? Code. Specific code handling edge cases. That's what the model does—predict the most likely continuation based on training patterns.",
        "context": "This is why conversational prompts fail. 'Could you help me write a function?' is the beginning of a conversation pattern, not a code pattern. The model completes conversation, not code.",
        "transition": "Now let's see how this works in practice. We'll look at imperative commands—the right way to start patterns."
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands:\nIneffective vs Effective",
      "leftCode": {
        "label": "Ineffective",
        "language": "text",
        "code": "Could you help me write a function to validate\nemail addresses? Thanks in advance!"
      },
      "rightCode": {
        "label": "Effective",
        "language": "text",
        "code": "Write a TypeScript function that validates\nemail addresses per RFC 5322.\nHandle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\n\nReturn { valid: boolean, reason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The ineffective prompt is conversational. It asks 'could you help me' and includes pleasantries—tokens that dilute your signal. The model completes this as conversation, possibly with explanatory text instead of code. The effective prompt starts with an imperative verb ('Write'), specifies the language, defines the validation standard, lists edge cases, and specifies the return type. Every token constrains the completion toward code.",
        "timing": "3-4 minutes",
        "discussion": "Walk through why each element matters: 'Write' (not 'make' or 'could you') establishes action, TypeScript specifies language, RFC 5322 specifies the standard, edge cases define the scope, return type specifies the interface. This is precision engineering.",
        "context": "In production, this matters. Vague prompts require iteration (5+ rounds). Specific prompts get it right in 1-2 rounds. Token cost, time cost, and error rate all improve dramatically.",
        "transition": "You might notice the effective prompt is quite detailed. Let's talk about why that specificity matters—and how the model actually uses it."
      }
    },
    {
      "type": "concept",
      "title": "Action Verbs Direct the Pattern",
      "content": [
        "Write (vs Make) → code pattern",
        "Debug X in File.ts:47 (vs Fix) → scope",
        "Add JSDoc to exported functions",
        "(vs Improve docs) → specificity",
        "Choose verbs that start the right pattern"
      ],
      "speakerNotes": {
        "talkingPoints": "Weak verbs like 'make' or 'fix' are vague about what pattern you want completed. Strong verbs start specific patterns. 'Write' signals code. 'Debug X in File.ts:47' signals a localized code examination. 'Add JSDoc to exported functions' signals documentation work. Notice how each strong verb starts a different pattern completion. The verb is your primary signal—choose it carefully.",
        "timing": "2-3 minutes",
        "discussion": "Give examples: 'Make a cache' vs 'Write an LRU cache with O(1) lookup and eviction'. 'Fix the performance bug' vs 'Optimize the N+1 query in ProductService.ts:124 to use a single indexed join'. The specificity compounds.",
        "context": "This applies everywhere you interact with AI: sub-agents (Task tool), semantic search (ChunkHound), research agents (ArguSeek). The verb and specificity control which patterns are retrieved.",
        "transition": "Now let's look at how to add structure to your prompts—making them even more constrained and effective."
      }
    },
    {
      "type": "concept",
      "title": "Personas: Vocabulary Control",
      "content": [
        "Personas bias vocabulary distribution",
        "Not adding knowledge—shifting retrieval",
        "Use when domain terminology matters",
        "Skip when straightforward tasks",
        "Vocabulary is your semantic control interface"
      ],
      "speakerNotes": {
        "talkingPoints": "A persona doesn't teach the model security concepts it doesn't know. Writing 'You are a security engineer' increases the probability of security-specific terms like 'threat model', 'attack surface', 'least privilege' appearing in the response. These terms act as semantic queries during attention, retrieving different training patterns than generic terms. The persona is a vocabulary shortcut—instead of listing every security consideration, you trigger the semantic cluster associated with 'security engineer'.",
        "timing": "3-4 minutes",
        "discussion": "Compare: 'Review this code' (generic advice) vs 'You are a security engineer. Review this code for vulnerabilities' (security-specific analysis). Same code, different retrieval patterns based on vocabulary. Ask: What vocabulary would you use if you were reviewing for performance vs security vs accessibility? That's what personas do.",
        "context": "This principle applies to all knowledge retrieval: ChunkHound semantic search ('authentication patterns' vs 'login code'), research queries ('rate limiting algorithms' vs 'slow down requests'), sub-agent instructions. Vocabulary choice controls what patterns are retrieved.",
        "transition": "Personas are about vocabulary. Now let's look at explicit structure—how to dictate the exact path the model should follow."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought:\nWithout vs With Steps",
      "leftCode": {
        "label": "Without CoT",
        "language": "text",
        "code": "Refactor this authentication middleware\nto use JWT tokens with role-based\naccess control"
      },
      "rightCode": {
        "label": "With CoT",
        "language": "text",
        "code": "Refactor authentication middleware step-by-step:\n\n1. Read existing middleware\n   (understand current flow)\n2. Design JWT structure\n   (include roles, expiry)\n3. Update token generation\n4. Add role validation checks\n5. Write unit tests for each role\n6. Verify backward compatibility\n\nStop after each step for review"
      },
      "speakerNotes": {
        "talkingPoints": "Without CoT, you're asking the model to figure out the entire refactoring strategy. It might skip steps or make assumptions. With CoT, you dictate exactly what happens in what order. The model executes each step sequentially, and you control the sequence. This is particularly powerful for QA workflows where you need methodical execution. CoT is essential for complex operations (5+ steps) where errors compound.",
        "timing": "3-4 minutes",
        "discussion": "Ask: When refactoring real code, what goes wrong with 'just do it' approaches? Missed edge cases, broken tests, backward compatibility. CoT forces the model to be methodical. You're not asking for reasoning—you're dictating the path.",
        "context": "In production QA workflows (Lesson 8: Tests as Guardrails), CoT is how you ensure accuracy. Without it, multi-step operations are unreliable. With it, you get consistent, debuggable execution.",
        "transition": "CoT gives you control through explicit steps. Structure gives you control through information density. Let's talk about how to structure information effectively."
      }
    },
    {
      "type": "concept",
      "title": "Structure Directs Attention",
      "content": [
        "Markdown/JSON/XML are information-dense",
        "Headings, lists, code blocks provide clarity",
        "Well-structured prompts get structured responses",
        "Format controls token efficiency",
        "Semantic structure reduces ambiguity"
      ],
      "speakerNotes": {
        "talkingPoints": "Different formats have different information density. Markdown is highly information-dense: headings signal hierarchy, lists organize options, code blocks delimit code. This matters for two reasons: token efficiency (more meaning per token) and grounding (clear semantic structure helps the model parse intent and respond with matching structure). A prompt with 'REQUIREMENTS:', 'IMPLEMENTATION NOTES:', 'EDGE CASES:' headings will yield organized responses with those sections.",
        "timing": "2-3 minutes",
        "discussion": "Show a prompt structured with clear sections vs unstructured prose. Same content, different effectiveness. The model responds to structure with structure. This is why the email validation prompt worked—it had clear sections (edge cases, return type).",
        "context": "This applies to all interaction: prompts to AI assistants, prompts to sub-agents, structured data for semantic search. Format matters. JSON schema in prompts yields valid JSON responses. Markdown headings yield organized markdown responses.",
        "transition": "We've covered the effective patterns. Now let's talk about the pitfalls—what to avoid when prompting."
      }
    },
    {
      "type": "codeComparison",
      "title": "Negation Pitfall:\nRisky vs Better",
      "leftCode": {
        "label": "Risky (Negation)",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nDo NOT log credentials.\nDo NOT hardcode secrets."
      },
      "rightCode": {
        "label": "Better (Positive Pattern)",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nInstead: Always hash passwords\nwith bcrypt (salt rounds: 10).\n\nDo NOT log credentials.\nInstead: Log hashed identifiers only.\n\nDo NOT hardcode secrets.\nInstead: Load from environment\nvariables via process.env"
      },
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with negation. Attention mechanisms treat 'NOT' as just another token. When 'NOT' receives low attention during processing, the model focuses on the concepts mentioned ('passwords', 'plain text') rather than their negation—affirmation bias. The model leans toward positive selection (what to include) rather than negative exclusion (what to avoid). The risky approach might generate plain text password storage. The better approach explicitly states the constraint AND provides the positive opposite pattern.",
        "timing": "3-4 minutes",
        "discussion": "This is a fundamental AI failure mode. The model's token generation mechanism is inherently positive. When you say 'don't do X', the model's attention has already activated the tokens for X during processing. You have to provide the positive pattern. Ask: What's the positive pattern that replaces bad behavior?",
        "context": "This is why security reviews of AI-generated code are critical. The model won't reliably avoid doing the wrong thing—you have to tell it the right thing explicitly. This connects to Lesson 5: Grounding (reviewing AI output) and Lesson 8: Tests as Guardrails (validating security properties).",
        "transition": "Negation is one failure mode. There's another critical one: math. Let's look at why LLMs can't be trusted with arithmetic."
      }
    },
    {
      "type": "codeComparison",
      "title": "Math Pitfall:\nText Prediction vs Code",
      "leftCode": {
        "label": "Don't: Ask for Math",
        "language": "text",
        "code": "Calculate optimal cache size for\n1M users, 64KB average object size,\n50% hit ratio target"
      },
      "rightCode": {
        "label": "Do: Ask for Code",
        "language": "text",
        "code": "Write JavaScript code that calculates\noptimal cache size:\n- Input: users (number),\n  objectSize (bytes),\n  hitRatio (0-1)\n- Output: cacheSize in GB\n- Include constants for\n  memory overhead (15%)"
      },
      "speakerNotes": {
        "talkingPoints": "LLMs are probabilistic text predictors, not calculators. They generate plausible-sounding numbers that are often completely wrong. If you ask 'what's 1M users × 64KB?', the model will produce a grammatically correct answer that may be arithmetically nonsense. Instead, ask the model to write code that does the math. The code runs deterministically and correctly. You're delegating computation (which LLMs fail at) to code execution (which is reliable).",
        "timing": "2-3 minutes",
        "discussion": "This is why agents are powerful: they can't do math, but they can write code that does math, then execute it. You're not asking the LLM to be a calculator. You're asking it to write code that a calculator can execute. Completely different reliability profile.",
        "context": "This is the core agent principle: LLMs are weak at deterministic tasks (math, logic, state machines) but excellent at probabilistic tasks (code generation, analysis, architectural decisions). Match the task to the capability.",
        "transition": "We've covered the mechanics of effective prompting: pattern completion, imperatives, structure, personas, CoT, and failure modes. Let's summarize the key principles."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Principles",
      "content": [
        "Prompting is pattern completion, not conversation",
        "Skip pleasantries—specificity is signal",
        "Structure directs attention and controls responses",
        "CoT dictates the execution path you need",
        "Avoid negation—state the positive pattern instead"
      ],
      "speakerNotes": {
        "talkingPoints": "These five principles cover the core of effective prompting. Pattern completion is the mental model shift. Specificity is how you constrain that completion. Structure is how you organize information at scale. CoT is how you control multi-step execution. Avoiding negation is how you prevent affirmation bias. Together, they form a complete mental model for precision engineering with AI.",
        "timing": "2-3 minutes",
        "discussion": "Ask each engineer to pick the one principle they violate most in their own prompts. Have them commit to improving one this week before they leave.",
        "context": "These principles carry forward: Lesson 5 (Grounding) teaches you how to verify the output is correct. Lesson 6 (Project Onboarding) applies these to large-scale context. Lesson 7 (Planning and Execution) uses these for agent planning. Lesson 8 (Tests as Guardrails) validates these patterns through testing.",
        "transition": "Before we finish, are there questions about any of these principles? These form the foundation for everything in the course."
      }
    },
    {
      "type": "concept",
      "title": "When to Use Each Technique",
      "content": [
        "Imperatives for straightforward coding tasks",
        "Personas when domain terminology matters",
        "CoT for multi-step operations (5+ steps)",
        "Structure for complex requirements",
        "Combine techniques for difficult problems"
      ],
      "speakerNotes": {
        "talkingPoints": "You don't use every technique on every prompt. Imperatives alone work for simple tasks. Add personas for security/performance/accessibility focus. Add CoT for methodical execution. Add structure for complex requirements. Master engineers mix and match based on task complexity. A simple bug fix might just need imperatives. A security audit needs personas. A multi-step refactoring needs CoT. A complex feature needs structure. Start minimal, add technique as needed.",
        "timing": "2-3 minutes",
        "discussion": "Walk through examples: simple task (just imperatives), security review (imperatives + persona), refactoring (imperatives + CoT + structure). What does your typical prompt look like? Where's the minimum effective dose?",
        "context": "This connects to minimalism—the core principle from CLAUDE.md. Less code is better code. Less prompting technique is better prompting. Don't add structure if it's not needed. Don't add personas if the task is straightforward.",
        "transition": "That brings us to the final piece: practice. Effective prompting is a skill you develop by doing, reviewing failures, and refining. The next lesson—Grounding—teaches you how to review and validate AI output."
      }
    }
  ]
}
