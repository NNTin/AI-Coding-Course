{
  "metadata": {
    "title": "Prompting 101: Pattern Completion, Not Conversation",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Understand pattern completion model fundamentally",
      "Write specific, action-oriented prompts effectively",
      "Use constraints to define completion space",
      "Avoid common failure modes defensively"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "Today we're reframing how you think about prompting. Most engineers treat AI assistants as conversational partners. That's a critical misconception. These are sophisticated pattern completion engines—statistical machines that predict what comes next based on training patterns. Understanding this distinction fundamentally changes your prompting strategy. We'll move from vague requests to precision engineering.",
        "timing": "1 minute",
        "discussion": "Ask: Has anyone ever been surprised by what an AI model generated from their prompt? Those surprises are usually pattern completion following statistical probability, not understanding your intent.",
        "context": "This lesson sits at the intersection of linguistics and probability. The model isn't 'thinking'—it's completing patterns based on token probability distributions learned from billions of examples.",
        "transition": "Let's start with the fundamental mental model: prompting as pattern drawing."
      }
    },
    {
      "type": "concept",
      "title": "Prompting as Pattern Completion",
      "content": [
        "Your prompt is the start of a sequence",
        "Model predicts what naturally follows",
        "More specific pattern start = narrower completion space",
        "Pleasantries dilute signal without clarity"
      ],
      "speakerNotes": {
        "talkingPoints": "Think of prompting like drawing the beginning of a pattern, then asking someone to complete it. When you write a clear pattern start, the model has constrained options for what comes next. Vague prompts (like 'Could you help me...?') create wide completion spaces—the model fills gaps with statistical assumptions. Clear prompts (like 'Write a TypeScript function that...') draw tighter patterns. The model's statistical machinery naturally completes similar patterns from training data. Also: skip 'please' and 'thank you.' They're conversational niceties that consume tokens without adding information. You're not being rude—you're optimizing signal density.",
        "timing": "3 minutes",
        "discussion": "Why does 'Could you help me write a function?' create a wider completion space than 'Write a TypeScript function that validates email addresses'? (Because the first leaves language, return type, and scope undefined.)",
        "context": "Attention mechanisms in transformers process all tokens, but 'please' and 'thank you' receive low attention weight when predicting next tokens. They add noise without semantic value.",
        "transition": "Now let's apply this to concrete examples. We'll look at how imperative commands draw sharper patterns."
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands: Ineffective vs Effective",
      "leftCode": {
        "label": "Ineffective",
        "language": "text",
        "code": "Could you help me write a function to validate\n  email addresses?\nThanks in advance!"
      },
      "rightCode": {
        "label": "Effective",
        "language": "text",
        "code": "Write a TypeScript function that\n  validates email addresses per RFC 5322.\nHandle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\n\nReturn { valid: boolean, reason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The ineffective prompt is conversational and vague. It doesn't specify language, validation standard, return type, or edge cases. The model must guess. The effective prompt draws a precise pattern: language (TypeScript), standard (RFC 5322), specific edge cases, and exact return type. Notice the difference in information density. The effective prompt is about 5x longer but constrains the completion space dramatically. The model knows exactly what pattern to complete.",
        "timing": "4 minutes",
        "discussion": "What ambiguities does the ineffective version leave? (Language, standard, edge cases, error handling, return type.) How does the effective version eliminate them? (Explicit about all of these.) Have you noticed this in practice—do you get better results with more specific prompts?",
        "context": "In production, the difference between these two approaches usually means the difference between 1-2 iterations and 5-6 iterations. Specificity is not pedantic—it's efficiency.",
        "transition": "Specificity is the foundation. Let's dig into what makes prompts specific: action verbs, constraints, and structure."
      }
    },
    {
      "type": "concept",
      "title": "Action Verbs Drive Pattern Specificity",
      "content": [
        "Write (not make) → establishes code pattern",
        "Debug X in File.ts:47 (not fix) → pinpoints scope",
        "Add JSDoc to exported functions (not improve docs) → defines scope",
        "Optimize query to use indexed columns (not improve performance) → concrete action"
      ],
      "speakerNotes": {
        "talkingPoints": "Weak verbs are vague. 'Fix' could mean anything. 'Make' could be code, a plan, or a decision. Strong verbs establish the exact pattern you want completed. 'Debug' tells the model to follow a debugging pattern. 'Write' tells the model to generate code. 'Add JSDoc' is more specific than 'improve docs.' When you read 'Debug the null pointer exception in UserService.ts:47,' the model's attention focuses on debugging patterns from its training data—specifically null pointer investigation and fix patterns. The file path grounds the context window. Compare that to 'Fix the bug'—the model must guess what bug, where, and how to fix it.",
        "timing": "2-3 minutes",
        "discussion": "Why does specificity compound effectiveness? (Because each constraint narrows the statistically plausible completions.) What happens if you use weak verbs? (The model fills gaps with assumptions, often wrong ones.)",
        "context": "This principle scales from single functions to refactoring entire services. The more specific your verb and scope, the better the result.",
        "transition": "Verbs establish intent. Now let's talk about constraints—explicit boundaries that prevent unwanted assumptions."
      }
    },
    {
      "type": "codeComparison",
      "title": "Constraints: Unconstrained vs Constrained",
      "leftCode": {
        "label": "Unconstrained",
        "language": "text",
        "code": "Add authentication to the API"
      },
      "rightCode": {
        "label": "Constrained",
        "language": "text",
        "code": "Add JWT authentication to the API:\n- Do NOT modify existing session middleware\n- Use jsonwebtoken library\n- Protect all /api/v1/* endpoints except /api/v1/auth/login\n- Token expiry: 24 hours\n- Store user ID and role in payload\n- Return 401 for missing/invalid tokens"
      },
      "speakerNotes": {
        "talkingPoints": "The unconstrained version leaves everything open. What auth mechanism? JWT? OAuth? Session tokens? Which endpoints? How do you integrate with existing systems? The constrained version defines the completion space precisely. It specifies: auth mechanism (JWT), library (jsonwebtoken), scope (all /api/v1/* except login), implementation details (24-hour expiry, specific payload), and error behavior. Without constraints, the model fills gaps with assumptions. With constraints, there's a single well-defined pattern to complete.",
        "timing": "3 minutes",
        "discussion": "In production, vague prompts often result in code that compiles but doesn't meet requirements. Then you iterate 5+ times. How many iterations would the constrained version take? Why? (Usually 1-2, because expectations are explicit.)",
        "context": "Constraints are particularly important when integrating with existing systems. Without explicit 'Do NOT modify existing middleware,' the model might refactor everything unnecessarily.",
        "transition": "Constraints prevent assumptions. But sometimes you need domain expertise. That's where personas come in—not to add knowledge, but to shift vocabulary."
      }
    },
    {
      "type": "codeComparison",
      "title": "Personas: Generic vs Security-Focused",
      "leftCode": {
        "label": "Generic",
        "language": "text",
        "code": "Review this authentication code for issues."
      },
      "rightCode": {
        "label": "Security-Focused",
        "language": "text",
        "code": "You are a security engineer conducting a code review.\nReview this authentication code. Flag vulnerabilities:\nSQL injection, XSS, auth bypasses, secrets in code.\nAssume adversarial input and untrusted networks."
      },
      "speakerNotes": {
        "talkingPoints": "Personas don't add knowledge—they shift vocabulary. Writing 'You are a security engineer' biases token probability toward security-specific terms like 'threat model,' 'attack surface,' 'least privilege,' and specific vulnerability names. These terms act as semantic queries during attention, retrieving different patterns than generic terms like 'check for issues.' The generic prompt might generate safe but obvious advice. The security-focused prompt retrieves specialized vulnerability patterns from training. This principle applies everywhere: vector database queries, codebase search tools, web research agents. Choose vocabulary that retrieves the patterns you need.",
        "timing": "3-4 minutes",
        "discussion": "Notice the persona didn't teach the model new security knowledge. It changed which knowledge gets retrieved. How would you apply this principle to other domains? (Replace 'security engineer' with 'performance engineer,' 'accessibility specialist,' etc.)",
        "context": "This is why search term choice matters so much in tools like ChunkHound. 'Authentication middleware patterns' retrieves different code than 'login code.' Vocabulary is the control interface for semantic retrieval.",
        "transition": "Vocabulary controls retrieval. But complex tasks need more than vocabulary—they need explicit step-by-step guidance. That's Chain-of-Thought."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought: Without vs With",
      "leftCode": {
        "label": "Without CoT",
        "language": "text",
        "code": "Debug the failing test in UserService.test.ts"
      },
      "rightCode": {
        "label": "With CoT",
        "language": "text",
        "code": "Debug the failing test in UserService.test.ts:\n\n1. Read the test file, identify which test is failing\n2. Analyze test assertion: expected vs actual values\n3. Trace code path through UserService to find the bug\n4. Explain root cause\n5. Provide a fix\n\nProvide conclusions with evidence."
      },
      "speakerNotes": {
        "talkingPoints": "Without CoT, the model makes its own decisions about execution order. It might skip steps or take shortcuts. With explicit Chain-of-Thought, you dictate the exact sequence. You're not asking for reasoning—you're defining the path the model must follow. Why does this matter? Because step-by-step execution surfaces errors early. If step 2 (analyzing assertions) reveals something unexpected, that informs steps 3-5. CoT is particularly powerful for QA workflows where you need methodical execution—see Lesson 8 for production examples. Simple tasks (under 5 steps) don't need CoT. But multi-step operations require explicit guidance for accuracy.",
        "timing": "3-4 minutes",
        "discussion": "When have you used multi-step prompts? Did you notice errors surface at each step, making them easier to debug? How does CoT improve QA workflows? (Tests become explicit verification at each step.)",
        "context": "Token efficiency note: CoT uses more tokens upfront but saves tokens overall by reducing iteration cycles from 5+ rounds to 1-2.",
        "transition": "Explicit steps ensure accurate execution. But all this information needs organization. Structure is how you direct the model's attention."
      }
    },
    {
      "type": "code",
      "title": "Structure Directs Attention: Markdown Example",
      "language": "markdown",
      "code": "# Task: Implement OAuth 2.0 Client Credentials Flow\n\n## Requirements\n\n- Support multiple authorization servers (configurable)\n- Cache tokens until expiry (Redis)\n- Auto-retry on 401 with token refresh\n- Expose as Express middleware\n\n## Implementation Steps\n\n1. Create OAuthClient class with getToken() method\n2. Implement token caching with TTL\n3. Add retry logic with exponential backoff\n4. Write middleware injecting token into req.context\n\n## Testing\n\n- Unit tests for OAuthClient\n- Integration tests against mock OAuth server\n- Error cases: network failure, invalid credentials,\n  expired tokens\n\n## Constraints\n\n- Use axios for HTTP requests\n- Use ioredis for caching\n- No global state—client must be instantiated",
      "caption": "Markdown is information-dense and directs model attention",
      "speakerNotes": {
        "talkingPoints": "Structure organizes information and directs the model's attention. Markdown is particularly effective—headings, lists, and code blocks are information-dense formats well-represented in training data. Notice how the structure separates concerns: what to build (requirements), how to build it (steps), how to test it (testing), and what to avoid (constraints). When the model parses this structure, it allocates attention proportionally. Headings receive high attention weight. Lists receive high attention weight. This structure makes requirements scannable and helps the model match response structure to prompt structure. Compare this to a paragraph-based prompt—the model has to extract meaning from continuous text, which typically results in less structured responses.",
        "timing": "2-3 minutes",
        "discussion": "How does markdown structure affect token efficiency? (Headings compress information semantically—the model understands 'Requirements' more efficiently than 'The things you need to do are...'.) What other formats are information-dense? (JSON for structured data, XML for hierarchical information.)",
        "context": "Information density matters for both token efficiency and response quality. You're not being pedantic—you're optimizing the model's parsing and generation.",
        "transition": "Structure helps. But there are predictable failure modes to avoid. Let's talk about what breaks prompts and how to defend against it."
      }
    },
    {
      "type": "concept",
      "title": "Failure Mode: Negation",
      "content": [
        "LLMs struggle with NOT due to attention bias",
        "Negation receives low attention weight during processing",
        "Model focuses on mentioned concepts, not their negation",
        "Solution: Pair negation with positive opposite immediately after"
      ],
      "speakerNotes": {
        "talkingPoints": "This is a predictable failure mode. When you write 'Do NOT store passwords in plain text,' the attention mechanism often misses the NOT. Why? Because attention treats NOT as just another token competing for weight. When processing, the model's focus lands on 'passwords' and 'plain text' (the concepts mentioned) rather than their negation. The phenomenon is called affirmation bias—models naturally lean toward positive selection (what to include) rather than negative exclusion (what to avoid). The fix: Always pair negation with the positive opposite. Write: 'Do NOT store passwords in plain text. Instead, always hash passwords with bcrypt using 10 salt rounds.' This pattern works because you've stated both the constraint and its logical replacement. The model can't miss the positive opposite because it comes immediately after the negation.",
        "timing": "3 minutes",
        "discussion": "Why is this a fundamental limitation? (Because token prediction is fundamentally about selecting what comes next, not excluding what shouldn't.) Have you experienced this—where the model did exactly what you said NOT to do? (Common in refactoring prompts: 'Don't use global state.' Model often does anyway.)",
        "context": "This isn't a flaw in the model—it's a direct consequence of how language models work as token predictors. Understanding this changes how you write defensive prompts.",
        "transition": "Another failure mode: mathematical reasoning. LLMs are terrible at arithmetic."
      }
    },
    {
      "type": "codeComparison",
      "title": "Failure Mode: Math Limitations",
      "leftCode": {
        "label": "Don't Rely on LLMs",
        "language": "text",
        "code": "Calculate the optimal cache size for 1M users with\n  2KB average data per user,\nassuming 80% hit rate and 4GB available memory."
      },
      "rightCode": {
        "label": "Have Them Write Code",
        "language": "text",
        "code": "Write a Python function that calculates optimal cache size.\n\nInputs:\n- user_count: number of users\n- avg_data_per_user_kb: average data size in KB\n- hit_rate: cache hit rate (0.0 to 1.0)\n- available_memory_gb: available memory in GB\n\nReturn optimal cache size in MB with reasoning.\nInclude unit tests validating the calculation."
      },
      "speakerNotes": {
        "talkingPoints": "This is straightforward: LLMs are probabilistic text predictors, not calculators. They're terrible at arithmetic. When you ask for a mathematical calculation, the model generates plausible-sounding numbers that may be completely wrong. The numbers might be statistically likely (they 'look reasonable') but they're not computed. The solution: Have the model write code that does the math. Code doesn't lie—it either computes correctly or it fails. You can test it. This applies to any situation where correctness is deterministic: unit conversion, financial calculations, performance projections. If accuracy matters and can be verified, push the work to code.",
        "timing": "2-3 minutes",
        "discussion": "Why does code solve this? (Because execution is deterministic—wrong code fails tests.) Have you seen plausible-looking but wrong numbers from prompts? (They're everywhere, which is why financial teams never trust LLM calculations directly.)",
        "context": "This is a general pattern: when determinism matters, push to code. Don't ask LLMs for reasoning about probabilities, financial projections, or calculations.",
        "transition": "Now we've covered the core principles: specificity, structure, personas, CoT, and failure modes. Let's synthesize: what's the one mindset shift that changes everything?"
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Prompting is pattern completion, not conversation—draw the beginning of the pattern you want completed",
        "Specificity compounds: constraints, action verbs, and structure narrow completion space exponentially",
        "Personas shift vocabulary, not capability—use them to bias toward domain-specific patterns you need",
        "Defend against predictable failures: negation bias and math limitations are fundamental, not mistakes",
        "Precision engineering beats natural conversation—be specific, be structured, be explicit"
      ],
      "speakerNotes": {
        "talkingPoints": "The fundamental shift: stop thinking of AI assistants as conversational partners who need politeness and hints. Think of them as pattern completion engines that need precision and structure. Everything flows from this. Specificity compounds because each constraint eliminates statistically plausible alternatives. Personas don't teach—they retrieve. Defensive prompting acknowledges what LLMs fundamentally are: statistical machines with predictable limits. You don't fight these limits—you work within them. That's precision engineering.",
        "timing": "2-3 minutes",
        "discussion": "Which principle do you think will have the biggest impact on your prompting? Specificity? Structure? Understanding failure modes? Why?",
        "context": "This lesson lays the foundation for every subsequent lesson: grounding (Lesson 5), agent design (Lesson 6), parallelization (Lesson 7), and testing (Lesson 8). These principles apply at every scale.",
        "transition": "Next lesson: Lesson 5 focuses on grounding—how to anchor pattern completion to external information so it stops hallucinating."
      }
    }
  ]
}