{
  "metadata": {
    "title": "Prompting 101: Pattern Completion, Not Conversation",
    "lessonId": "lesson-4-prompting-101",
    "estimatedDuration": "45-60 minutes",
    "learningObjectives": [
      "Master pattern completion mindset",
      "Structure precise, actionable prompts",
      "Use constraints and personas effectively",
      "Avoid predictable LLM failure modes"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "Prompting 101",
      "subtitle": "Pattern Completion, Not Conversation",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson fundamentally reframes how engineers should think about prompting. AI models aren't conversational partners—they're pattern completion engines. This distinction changes everything about how you structure your prompts. We'll move from 'asking nicely' to 'precision engineering'.",
        "timing": "1 minute",
        "discussion": "Ask students: 'How many of you treat AI like a conversation partner? Who's used \"please\" and \"thank you\" in prompts?'",
        "context": "Many senior engineers come from traditional API design backgrounds. Prompting requires a fundamentally different mental model.",
        "transition": "Let's start with the core insight: understanding what prompting actually is."
      }
    },
    {
      "type": "concept",
      "title": "Prompting as Pattern Completion",
      "content": [
        "Models predict next tokens based on training data patterns",
        "Your prompt is the pattern START, not a question",
        "Specificity constrains the completion space",
        "Pleasantries dilute signal—they're wasted tokens"
      ],
      "speakerNotes": {
        "talkingPoints": "When you write a prompt, you're not making a request—you're initializing a sequence the model will complete. The model is fundamentally a statistical pattern predictor. If you write 'Write a TypeScript function that validates...', you're not asking a question. You're starting a code block pattern. The model's job is to predict what naturally follows based on patterns it learned during training.",
        "timing": "3-4 minutes",
        "discussion": "Show concrete example: 'If you type \"function validate(\" in VS Code, autocomplete predicts what comes next. LLMs do the same thing, but with larger context windows. How does that change how you'd prompt?'",
        "context": "This insight is crucial for senior engineers who might come from service design backgrounds. LLMs aren't 'understanding' your request in the way a human would. They're doing statistical inference on token sequences.",
        "transition": "Now that we understand pattern completion, let's apply this to actual prompts. How do we draw the beginning of the pattern we want?"
      }
    },
    {
      "type": "codeComparison",
      "title": "Imperative Commands: Ineffective vs Effective",
      "leftCode": {
        "label": "Ineffective",
        "language": "text",
        "code": "Could you help me write a function\nto validate email addresses?\nThanks in advance!"
      },
      "rightCode": {
        "label": "Effective",
        "language": "text",
        "code": "Write a TypeScript function that validates\nemail addresses per RFC 5322.\nHandle edge cases:\n- Multiple @ symbols (invalid)\n- Missing domain (invalid)\n- Plus addressing (valid)\n\nReturn { valid: boolean, reason?: string }"
      },
      "speakerNotes": {
        "talkingPoints": "The ineffective version wastes tokens on politeness and provides no pattern constraints. 'Could you help me' doesn't establish the pattern start. The effective version draws a complete pattern: language (TypeScript), requirement (RFC 5322), edge cases, return type. The model completes this with code that matches the established pattern.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'What pattern does the ineffective version start? What pattern does the effective one start? How would completions differ?' Have students identify the specific elements: language, standard, constraints, return type.",
        "context": "In production, this difference means 1-2 iterations for the effective prompt vs 5+ for the ineffective one. Every token matters because you're being charged for them.",
        "transition": "The action verbs themselves matter. Let's look at how verb choice constrains patterns."
      }
    },
    {
      "type": "comparison",
      "title": "Action Verbs: Weak vs Strong",
      "left": {
        "label": "Weak",
        "content": [
          "\"Make a function\" → too vague",
          "\"Fix the bug\" → which bug?",
          "\"Update the docs\" → unclear scope",
          "\"Improve performance\" → which metric?"
        ]
      },
      "right": {
        "label": "Strong",
        "content": [
          "\"Write a function\" → establishes code pattern",
          "\"Debug null pointer in UserService.ts:47\" → pinpoints scope",
          "\"Add JSDoc to exported functions in auth.ts\" → defines scope clearly",
          "\"Optimize query to use indexed columns\" → specifies target"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "Strong verbs establish specific patterns. 'Write' signals code completion. 'Debug' with a file path constrains the search space. When you say 'Fix the bug' with no context, the model guesses. When you say 'Debug the null pointer exception in UserService.ts:47', you've drawn the exact pattern boundary. Specificity compounds—each additional constraint dramatically improves the completion.",
        "timing": "2-3 minutes",
        "discussion": "Have students rate their own prompts. Share one from their work (anonymously). How could it be more specific? What's missing?",
        "context": "This is where senior engineers' debugging skills apply directly. You debug by narrowing scope. Prompting works the same way—narrow the pattern space with specificity.",
        "transition": "Specificity is pattern refinement. But we also need constraints to define boundaries. Let's see how to use constraints as guardrails."
      }
    },
    {
      "type": "codeComparison",
      "title": "Constraints: Unconstrained vs Defined",
      "leftCode": {
        "label": "Unconstrained",
        "language": "text",
        "code": "Add authentication to the API"
      },
      "rightCode": {
        "label": "Defined",
        "language": "text",
        "code": "Add JWT-based authentication to the API.\nRequirements:\n- Sign tokens with HS256 algorithm\n- 1-hour expiration\n- Refresh token endpoint at /auth/refresh\n- Protect all /api/* routes\n- Return 401 for invalid tokens"
      },
      "speakerNotes": {
        "talkingPoints": "Without constraints, the model fills gaps with assumptions. 'Add authentication' could mean JWT, OAuth, sessions, API keys, or something else. Each assumption is a branch in the completion space. The constrained version closes every branch except one. This is precision engineering—you're defining the completion space exactly.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'In the unconstrained version, what are the possible completions? How many iterations would you need to get JWT with 1-hour expiration?' Walk through how constraints eliminate ambiguity.",
        "context": "This mirrors API contract definition. You don't just say 'build a REST endpoint'—you define the schema, status codes, constraints. Prompting works the same way.",
        "transition": "Constraints define the what. Now let's talk about adding domain context through personas."
      }
    },
    {
      "type": "concept",
      "title": "Personas: Vocabulary as Control Interface",
      "content": [
        "Personas bias token distribution toward domain-specific terms",
        "\"Security engineer\" retrieves threat models, attack surfaces",
        "Shorthand for listing security concerns explicitly",
        "Use when domain vocabulary matters; skip for straightforward tasks"
      ],
      "speakerNotes": {
        "talkingPoints": "A persona like 'You are a security engineer' doesn't add knowledge—it changes which patterns get retrieved. During attention, the persona shifts probability toward security terms like 'threat model', 'attack surface', 'least privilege'. These terms act as semantic queries during inference, retrieving different training patterns than generic terms like 'check for issues'. Think of a persona as a vocabulary shortcut. Instead of listing every security concern explicitly, you trigger the cluster associated with that domain.",
        "timing": "3 minutes",
        "discussion": "Comparison exercise: 'Analyze this code for issues' vs 'You are a security engineer. Review this code.' Same code, different persona, vastly different output vocabulary. Why? How is that useful?",
        "context": "This principle applies everywhere: ChunkHound queries, ArguSeek research, vector databases. 'Authentication middleware patterns' retrieves different results than 'login code'. Vocabulary is the control interface for semantic retrieval.",
        "transition": "Personas add vocabulary precision. But when tasks get complex, you need more control. That's where Chain-of-Thought comes in."
      }
    },
    {
      "type": "codeComparison",
      "title": "Chain-of-Thought: Without vs With CoT",
      "leftCode": {
        "label": "Without CoT",
        "language": "text",
        "code": "Write a function that validates user input\nand returns an error message if invalid"
      },
      "rightCode": {
        "label": "With CoT",
        "language": "text",
        "code": "Write a function that validates user input.\nFirst, check if input is a string.\nSecond, verify length is between 3-50 chars.\nThird, ensure no special characters except\nhyphens and underscores.\nFourth, return { valid: true } or\n{ valid: false, reason: '...' }."
      },
      "speakerNotes": {
        "talkingPoints": "CoT (Chain-of-Thought) gives you explicit control over execution sequence. Without CoT, the model chooses the path. With CoT, you dictate each step. For simple tasks (1-3 steps), models handle it fine without explicit guidance. For complex operations (5+ steps), CoT is essential. You're not asking for reasoning—you're defining the path the model must follow.",
        "timing": "3-4 minutes",
        "discussion": "What happens if you run the 'without CoT' prompt? Discuss potential completions. Then ask: 'How would CoT make this deterministic? Why does that matter in production?'",
        "context": "In QA workflows and complex operations, CoT is mandatory. See Lesson 8: Tests as Guardrails for production examples of using tests with agent workflows. CoT helps surface errors at each stage rather than compounding through multiple steps.",
        "transition": "Step-by-step instructions guide execution. But how we structure that guidance matters. Let's talk about information density."
      }
    },
    {
      "type": "concept",
      "title": "Structure Directs Attention",
      "content": [
        "Markdown, JSON, XML are information-dense formats",
        "Clear structure helps models parse intent",
        "Headings and lists guide semantic attention",
        "Well-structured prompts get matching structure in responses"
      ],
      "speakerNotes": {
        "talkingPoints": "Information density describes how much meaning is conveyed per token. Markdown is highly information-dense because headings, lists, and code blocks provide clear semantic structure with minimal overhead. A prompt that says 'Build a class with constructor and validation' is less dense than one using markdown with clear sections. Structure guides the model's attention during token generation—the model learns that sections marked with '##' are important patterns to follow.",
        "timing": "2-3 minutes",
        "discussion": "Compare two versions of the same requirement: one as prose paragraph, one as structured markdown. Which gets better completion? Why?",
        "context": "This is why engineering specs use formal structure. Prose requirements breed interpretation variance. Structured requirements create shared understanding.",
        "transition": "Now we know how to write good prompts. Let's cover what breaks them: predictable failure modes."
      }
    },
    {
      "type": "codeComparison",
      "title": "Negation Issues: Risky vs Better",
      "leftCode": {
        "label": "Risky",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nDo NOT use deprecated authentication.\nDo NOT hardcode secrets."
      },
      "rightCode": {
        "label": "Better",
        "language": "text",
        "code": "Do NOT store passwords in plain text.\nInstead, always hash passwords using bcrypt\nwith salt rounds = 12.\n\nDo NOT use deprecated authentication.\nInstead, implement JWT with HS256.\n\nDo NOT hardcode secrets.\nInstead, load from environment variables\nor secure vault."
      },
      "speakerNotes": {
        "talkingPoints": "LLMs struggle with negation because attention mechanisms treat 'NOT' as just another token. When 'NOT' gets low attention during processing, the model focuses on the concepts mentioned ('passwords', 'plain text') rather than their negation. This is 'affirmation bias'—the model leans toward positive selection (what to include) rather than negative exclusion (what to avoid). The better pattern fixes this: explicit negation followed immediately by the positive opposite. This gives the model both the constraint AND the correct pattern to follow.",
        "timing": "3-4 minutes",
        "discussion": "Show the risky version to students. What might the model generate despite the 'DO NOT'? Then show how the better version provides an alternative pattern. Why does 'instead' make it work?",
        "context": "This is a known limitation of transformer attention. It's not that LLMs are stupid—it's that negation is hard for probabilistic models. Understanding this lets you work around it.",
        "transition": "We've covered what makes prompts work. There's one more pitfall: math."
      }
    },
    {
      "type": "concept",
      "title": "LLM Math Limitations",
      "content": [
        "LLMs are text predictors, not calculators",
        "Don't ask them to do arithmetic—they'll guess",
        "Plausible-sounding numbers are often completely wrong",
        "Instead, have them write code that computes results"
      ],
      "speakerNotes": {
        "talkingPoints": "LLMs are fundamentally terrible at arithmetic. They predict plausible tokens, and plausible numbers that are mathematically wrong are still plausible to predict. If you ask 'What is 2,547 × 3,819?', the model will generate a confident-sounding answer that may be completely wrong. It's not being stupid—it's doing what it's designed to do: predict likely next tokens. For calculations, have the model write code instead. Code is deterministic; text prediction isn't.",
        "timing": "2 minutes",
        "discussion": "Try it: ask the model to calculate something complex. Compare to asking it to write JavaScript that calculates it. What's the difference in accuracy?",
        "context": "This is why financial institutions don't use LLMs for calculation-heavy tasks. This is why you pair LLMs with code execution.",
        "transition": "Let's synthesize everything into key takeaways for production use."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Prompting is pattern completion—draw the beginning of the pattern you want",
        "Be specific: action verbs, file paths, constraints, return types matter",
        "Structures like markdown guide attention and improve responses",
        "Avoid negation alone; follow with positive opposite (\"instead\")",
        "CoT for complex tasks; simple tasks usually work without it",
        "Personas shift vocabulary; use them for domain-specific grounding",
        "Never ask LLMs to do math—have them write code instead"
      ],
      "speakerNotes": {
        "talkingPoints": "These takeaways represent the core principles of effective prompting. Each one directly applies to production work. The biggest shift is moving from 'polite requests' to 'precision engineering'. You're not having a conversation—you're initializing a pattern completion engine. The more specific, structured, and explicit you are, the better the completion.",
        "timing": "3-4 minutes",
        "discussion": "Open floor: 'Which of these principles surprised you? Which one changes how you'll prompt in your next project?' Have students commit to trying one or two in their work.",
        "context": "Senior engineers who master these principles see immediate improvement in AI-assisted productivity. Vague prompts become specific. Iterative refinement becomes first-pass accuracy.",
        "transition": "Next lesson (Lesson 5) covers grounding—how to connect these prompting skills to your specific codebase. You'll learn how vocabulary precision helps when searching for code patterns and context."
      }
    }
  ]
}
