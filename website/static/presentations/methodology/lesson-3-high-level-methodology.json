{
  "metadata": {
    "title": "Lesson 3: High-Level Methodology",
    "lessonId": "lesson-3-high-level-methodology",
    "estimatedDuration": "40-50 minutes",
    "learningObjectives": [
      "Shift from craftsman to operator",
      "Master four-phase workflow",
      "Apply exploration vs exact planning",
      "Validate code against mental model"
    ]
  },
  "slides": [
    {
      "type": "title",
      "title": "High-Level Methodology",
      "subtitle": "From Craftsman to Operator",
      "content": [],
      "speakerNotes": {
        "talkingPoints": "This lesson is about the psychological and methodological shift required to work effectively with AI agents at scale. You're moving from writing every line of code yourself to directing systems that generate thousands of lines. The core challenge isn't technical—it's learning to delegate implementation while maintaining architectural control. We'll walk through a proven four-phase workflow that makes this shift practical.",
        "timing": "1-2 minutes",
        "discussion": "Ask students: 'How would your job change if you could delegate all implementation details but still owned the results?'",
        "context": "Many senior engineers struggle with this because their entire career has been built on being the person who understands every line of code. This lesson reframes that value at a higher architectural level.",
        "transition": "Let's start by understanding what changes when you work with agents at scale."
      }
    },
    {
      "type": "comparison",
      "title": "Traditional Developer vs Operator",
      "left": {
        "label": "Traditional Developer",
        "content": [
          "Write implementation code",
          "Read and verify every line",
          "Debug syntax and logic errors",
          "Refactor for consistency",
          "Personal code ownership"
        ]
      },
      "right": {
        "label": "Operator Mindset",
        "content": [
          "Understand system architecture",
          "Research patterns and context",
          "Plan architectural decisions",
          "Direct agent with precise context",
          "Validate against requirements"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This comparison shows the fundamental shift. You're not doing the same job faster—you're doing a different job. In traditional development, your value is in the implementation details. In the operator model, your value is in architectural thinking, pattern recognition, and decision-making. The agent handles the implementation; you handle the direction.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'Which responsibilities feel comfortable to delegate? Which feel risky?' Discuss that this is a normal feeling and how we address it through validation.",
        "context": "This is the biggest psychological barrier. Engineers who've built their identity on code craftsmanship often feel like they're losing their edge. In reality, they're gaining leverage.",
        "transition": "This shift isn't abandoning quality—it's ensuring quality differently. Let's look at the four phases that make it work."
      }
    },
    {
      "type": "visual",
      "title": "The Four-Phase Workflow",
      "component": "WorkflowCircle",
      "caption": "Systematic framework for agent-directed development",
      "speakerNotes": {
        "talkingPoints": "Every significant agent interaction follows this cycle: Research to ground the agent in your codebase and domain, Plan to decide your approach, Execute to have the agent do the work, and Validate to check if the results match your requirements. This isn't linear—you iterate when needed. The power is having a framework that catches issues before they become expensive mistakes.",
        "timing": "2 minutes",
        "discussion": "Which phase do you think most engineers skip or underestimate? (Answer: Research and Planning—people want to jump to execution.)",
        "context": "This workflow applies to any agent interaction, from small bug fixes to architectural refactoring. The phases scale.",
        "transition": "Let's walk through each phase and understand what happens in each one."
      }
    },
    {
      "type": "concept",
      "title": "Phase 1: Research (Grounding)",
      "content": [
        "Semantic code search (ChunkHound)",
        "Architectural patterns and conventions",
        "Domain knowledge retrieval (ArguSeek)",
        "Build context before agent executes",
        "Prevents hallucination and inconsistency"
      ],
      "speakerNotes": {
        "talkingPoints": "You wouldn't start coding in a new codebase without understanding the patterns. Your agent needs the same. Grounding bridges the gap between the model's general knowledge and your specific system. ChunkHound does code-level research—finding how authentication actually works in your system. ArguSeek pulls in external knowledge—API docs, best practices, algorithms. Without this phase, agents invent APIs, miss existing implementations, and hallucinate patterns.",
        "timing": "3-4 minutes",
        "discussion": "Share an example: 'What happens if an agent doesn't know your authentication pattern? It invents one. Now you have two authentication systems in your codebase.'",
        "context": "This phase is why production-grade AI development is different from playing with ChatGPT. You're customizing the agent's knowledge to your specific system.",
        "transition": "Once you have research, you plan. But planning isn't one-size-fits-all—it depends on what you know."
      }
    },
    {
      "type": "visual",
      "title": "Planning Strategies: Exploration vs Exact",
      "component": "PlanningStrategyComparison",
      "caption": "Choose strategy based on solution clarity",
      "speakerNotes": {
        "talkingPoints": "Planning isn't a single approach—it's a strategic choice. Exploration planning asks the agent to research your system, consider alternatives, and iterate with you to discover the best approach. This takes longer but catches architectural issues early and builds your mental model. Exact planning gives the agent a precise specification: here's the task, here's how it integrates, here are the constraints. This is faster but requires upfront clarity. Use exploration when you're uncertain; use exact when you've already done the architectural thinking.",
        "timing": "4-5 minutes",
        "discussion": "Ask: 'When would you use exploration planning? When would you lock in with exact planning?' Discuss the tradeoff: speed vs discovery.",
        "context": "Most engineers underestimate planning because they jump to implementation. But 5 minutes of planning saves 30 minutes of iteration. This is the leverage point.",
        "transition": "With your plan in place, you execute. But execution has two modes that dramatically change your productivity."
      }
    },
    {
      "type": "concept",
      "title": "Phase 2: Plan - Building Mental Model",
      "content": [
        "Refine your understanding of system relationships",
        "How authentication flows through stack",
        "Where validation and business logic separate",
        "Error propagation and boundaries",
        "Performance and security implications"
      ],
      "speakerNotes": {
        "talkingPoints": "As you plan, you're building your mental model of the system. This isn't about memorizing code—it's about understanding relationships. How does data flow? Where are the integration points? What are the failure modes? This mental model is what allows you to validate generated code quickly. You're not reading every line; you're checking: 'Does this match my mental model of how this system works?' If yes, it's probably correct. If no, investigate.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'How do you currently validate someone else's code?' Compare that to validating against a mental model.",
        "context": "This is the secret to shipping thousands of lines of generated code without reading every character. Your mental model is the filter.",
        "transition": "Now let's look at execution modes—where the real productivity transformation happens."
      }
    },
    {
      "type": "comparison",
      "title": "Execution Modes: Supervised vs Autonomous",
      "left": {
        "label": "Supervised (Babysitting)",
        "content": [
          "Watch each agent action",
          "Review intermediate outputs",
          "Steer in real time",
          "Maximum control and precision",
          "High cognitive load"
        ]
      },
      "right": {
        "label": "Autonomous (Autopilot)",
        "content": [
          "Define task from plan, let it run",
          "Check results when done",
          "Work on other projects in parallel",
          "Genuine multitasking in development",
          "Requires excellent grounding"
        ]
      },
      "speakerNotes": {
        "talkingPoints": "This is where the real productivity transformation happens. Supervised mode gives you maximum control but blocks you—you can't context-switch. Autonomous mode is where you become genuinely more productive. You give the agent a well-defined task, then do something else. You can have three agents running simultaneously on different projects. Even if the agent takes slightly longer per task, you're shipping more total code because you're not blocked. The trick: autonomous mode only works if you did research and planning correctly.",
        "timing": "4-5 minutes",
        "discussion": "Ask: 'Which mode sounds more productive?' Then follow up: 'Why doesn't everyone use autonomous mode?' (Answer: Because it requires discipline in research and planning.)",
        "context": "This is counterintuitive. Most engineers think the 10x productivity comes from speed per task. It actually comes from parallel work and elimination of blocking. You can literally ship code while cooking dinner.",
        "transition": "But autonomous only works if you did the upfront work. What happens after execution, and how do you know if it worked?"
      }
    },
    {
      "type": "concept",
      "title": "Phase 3: Execute - Two Paths",
      "content": [
        "Supervised: Full control, low parallelization",
        "Autonomous: High parallelization, trust-dependent",
        "Start supervised to build trust",
        "Shift to autonomous as grounding improves",
        "Real 10x gain: multitasking, not speed"
      ],
      "speakerNotes": {
        "talkingPoints": "Most engineers start in supervised mode—watching the agent work, steering it, building confidence. This is your training ground. As your research and planning get stronger, you graduate to autonomous mode. The real productivity isn't that one task finishes faster. It's that you can have three tasks running simultaneously while you attend meetings or cook dinner. You're genuinely multitasking for the first time in your career. That's the game-changer.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'What's the difference between finishing one task in 20 minutes vs finishing three tasks in 45 minutes while you're not at your keyboard?'",
        "context": "The productivity transformation is about time leverage, not speed optimization.",
        "transition": "But the agent isn't always perfect. Let's talk about validation—how you know what to do with the output."
      }
    },
    {
      "type": "concept",
      "title": "Phase 4: Validate - Three Checks",
      "content": [
        "Actually run your code—be the user",
        "Use agent to review its own work",
        "Automated checks: build, tests, linters",
        "Manual validation: behavior vs requirements",
        "Decide: iterate or regenerate?"
      ],
      "speakerNotes": {
        "talkingPoints": "LLMs almost never produce 100% perfect output on first pass. That's expected. Your validation goal isn't perfection—it's accurately identifying what's wrong and deciding: iterate with fixes or regenerate from scratch? Iterate when the foundation is right but has gaps. Regenerate when something fundamental is wrong. Here's the key: it's usually easier to fix your input (the prompt, context, examples) than to fix the output. Think of yourself as debugging your directions, not the code.",
        "timing": "3-4 minutes",
        "discussion": "Ask: 'How do you currently decide if code is good enough to ship?' Compare traditional code review to this agent-directed approach.",
        "context": "This mindset shift is critical. Don't get attached to generated code. It's a draft. Your input determines the quality.",
        "transition": "Let's put this all together and see how these phases close the loop."
      }
    },
    {
      "type": "codeExecution",
      "title": "Complete Four-Phase Cycle",
      "steps": [
        {
          "line": "RESEARCH: Query ChunkHound: 'How is authentication handled in this codebase?'",
          "highlightType": "human",
          "annotation": "You frame the research question"
        },
        {
          "line": "ChunkHound returns patterns and examples from codebase",
          "highlightType": "feedback",
          "annotation": "Context grounding - codebase knowledge"
        },
        {
          "line": "PLAN: 'Add JWT token refresh. Follow existing pattern in auth.ts'",
          "highlightType": "human",
          "annotation": "You specify exact requirements"
        },
        {
          "line": "Agent predicts architectural approach and integration points",
          "highlightType": "prediction",
          "annotation": "Agent understands constraints"
        },
        {
          "line": "EXECUTE: Agent reads auth patterns, writes implementation",
          "highlightType": "execution",
          "annotation": "Autonomous mode - you do something else"
        },
        {
          "line": "Implementation returned: 50 files generated, tests pass",
          "highlightType": "feedback",
          "annotation": "Output ready for validation"
        },
        {
          "line": "VALIDATE: Run code, test refresh flow, check against mental model",
          "highlightType": "human",
          "annotation": "You spot-check behavior"
        },
        {
          "line": "Mental model match? Yes → ship. No → regenerate with adjusted context.",
          "highlightType": "summary",
          "annotation": "Decision point closes the loop"
        }
      ],
      "speakerNotes": {
        "talkingPoints": "This shows the complete cycle from grounding through shipping. Notice where you're active (research questions, planning, validation) versus where the agent is active (understanding context, predicting approach, executing). This is the operator mindset: you direct, the agent executes. The cycle only closes when the output matches your mental model.",
        "timing": "3-4 minutes",
        "discussion": "Walk through each phase. Ask: 'What happens if we skip research? What happens if we skip planning?'",
        "context": "This is how you maintain quality and architectural control while scaling your output.",
        "transition": "Let's close with the key principle that ties everything together."
      }
    },
    {
      "type": "takeaway",
      "title": "Key Takeaways",
      "content": [
        "Operator mindset: direct systems, not write code",
        "Four phases: Research, Plan, Execute, Validate",
        "Mental model validates output, not line-by-line reading",
        "Autonomous mode scales through parallelization, not speed",
        "Quality comes from context and planning, not code review"
      ],
      "speakerNotes": {
        "talkingPoints": "This lesson reframes your role. You're not doing the same job faster—you're doing a different job. Your value moves from implementation details to architectural decisions. The four-phase workflow is your process for maintaining quality at scale. Your mental model is your validation filter. Autonomous mode unlocks true multitasking. And quality comes from excellent context and planning upfront, not from reading every line after.",
        "timing": "2-3 minutes",
        "discussion": "Ask: 'Which part of this workflow will be hardest to adopt in your work?' Discuss real obstacles: context switching, building trust, accepting probabilistic output.",
        "context": "This is foundational. Everything in subsequent lessons builds on this framework.",
        "transition": "In Lesson 4, we'll zoom into the specifics of how to prompt effectively within this framework. But the workflow is constant—research, plan, execute, validate."
      }
    }
  ]
}
