---
title: AI-Assisted PR Review
sidebar_position: 2
---

import AIAssistedReview from '@site/shared-prompts/\_ai-assisted-review.mdx';

<AIAssistedReview />

### Overview

**Why GitHub CLI integration prevents hallucination:** [Persona directive](/docs/methodology/lesson-4-prompting-101#assigning-personas) ("$PROJECT_NAME's maintainer") biases toward critical analysis and architectural awareness instead of generic approval patterns—reviewers defend system integrity, not PR authors. GitHub CLI integration (`gh pr view --comments`) loads PR metadata, discussion threads, and linked issues directly into context, providing [multi-source grounding](/docs/methodology/lesson-5-grounding#grounding-anchoring-agents-in-reality) beyond just code diff—identifies author intent through comments, catches breaking changes from issue discussions, surfaces previously raised concerns. Pasting AI-optimized description from [Dual-Optimized PR](/prompts/pull-requests/dual-optimized-pr) provides technical grounding (file paths, module responsibilities, breaking changes, implementation rationale) that contextualizes changes within project architecture. Evidence requirement ("never speculate...investigate files") implements explicit [grounding directive](/docs/methodology/lesson-5-grounding#grounding-anchoring-agents-in-reality)—forces code research tool usage before making claims, preventing hallucinated issues based on training patterns ("probably violates SOLID" → must cite specific violations with file:line evidence). [Chain of Draft (CoD)](/docs/practical-techniques/lesson-9-reviewing-code#mechanisms-at-work) maintains [structured reasoning](/docs/methodology/lesson-4-prompting-101#chain-of-thought-paving-a-clear-path) like Chain-of-Thought but with concise intermediate steps (5 words max per draft), reducing token consumption 60-80% while preserving reasoning quality—agent still thinks through architecture/quality/maintainability/reusability sequentially, just doesn't generate verbose explanations until final assessment after `####` separator. Critical Checks framework provides concrete [constraints](/docs/methodology/lesson-4-prompting-101#constraints-as-guardrails) for architectural validation: "Can existing code be extended?" forces reusability analysis, "Search the codebase for similar patterns" requires semantic search instead of assumptions, "Is this introducing duplication?" prevents incremental technical debt. Structured output format ([constrained format](/docs/methodology/lesson-4-prompting-101#constraints-as-guardrails)) with severity classification (Critical/Major/Minor) and file:line references ensures [evidence requirements](/docs/practical-techniques/lesson-7-planning-execution#require-evidence-to-force-grounding)—can't list issues without citing specific locations and code.

**When to use—primary use cases:** Systematic PR review for architectural changes touching core modules, introducing new patterns, or significant refactoring where architectural consistency matters more than surface-level correctness. Most effective when PR author provides AI-optimized description (explicit file paths, module boundaries, breaking changes), though you can compensate with additional code research time if human-only description exists. Best used pre-merge as final validation in [Validate phase](/docs/methodology/lesson-3-high-level-methodology#the-four-phase-workflow), not during active development (use [Comprehensive Review](/prompts/code-review/comprehensive-review) for worktree changes). Critical: verify every issue the agent raises—LLMs can be confident and wrong, especially about architectural violations. Cross-check cited file:line references actually contain the code claimed. Challenge vague criticisms ("violates separation of concerns") by demanding specific evidence: which modules are coupled, what responsibilities are mixed, show exact lines. Use this pattern when human reviewers need LLM assistance for deep codebase searches (finding duplication, pattern conformance, similar implementations elsewhere)—GitHub CLI + code research combination excels at multi-file analysis humans find tedious. Less effective for trivial PRs (documentation-only, dependency updates, simple bug fixes) where review overhead exceeds value.

**Prerequisites:** [GitHub CLI](https://cli.github.com/) (`gh`) installed, authenticated, and configured for target repository (run `gh auth status` to verify), [code research capabilities](https://chunkhound.github.io/) (semantic search across codebase for finding patterns, duplication, architectural violations), repository access with read permissions for all relevant files. Requires PR link (URL or number like `#123`), project name for persona context (`$PROJECT_NAME`), ideally AI-optimized description from [Dual-Optimized PR](/prompts/pull-requests/dual-optimized-pr) workflow (if unavailable, agent compensates with longer code research phase). Agent produces structured feedback with severity-classified issues, file:line citations, specific refactoring opportunities, and APPROVE/REQUEST CHANGES/REJECT decision. [Adapt Critical Checks for specialized review focus](/docs/practical-techniques/lesson-9-reviewing-code#mechanisms-at-work): security reviews (input validation boundaries—are user inputs sanitized? authentication/authorization checks—who can access this endpoint? credential handling—are secrets exposed? injection attack vectors—can SQL/XSS occur? OWASP Top 10 coverage), performance reviews (algorithmic complexity—is this O(n²) unavoidable? database query efficiency—N+1 queries or missing indexes? memory allocation patterns—unnecessary copies or leaks? I/O operations—blocking calls in hot paths? caching strategy—are expensive operations memoized?), accessibility reviews (semantic HTML structure—meaningful tags or div soup? keyboard navigation—can users tab through? ARIA labels—screen reader compatible? focus management—obvious visual indicators? color contrast ratios—WCAG AA/AAA compliance?).

### Related Lessons

- **[Lesson 3: High-Level Methodology](/docs/methodology/lesson-3-high-level-methodology#the-four-phase-workflow)** - Four-phase workflow (Research > Plan > Execute > Validate)—PR review is Validate phase
- **[Lesson 4: Prompting 101](/docs/methodology/lesson-4-prompting-101#assigning-personas)** - [Persona directives](/docs/methodology/lesson-4-prompting-101#assigning-personas), [Chain-of-Thought](/docs/methodology/lesson-4-prompting-101#chain-of-thought-paving-a-clear-path), [constraints](/docs/methodology/lesson-4-prompting-101#constraints-as-guardrails), structured output
- **[Lesson 5: Grounding](/docs/methodology/lesson-5-grounding#grounding-anchoring-agents-in-reality)** - Multi-source grounding (GitHub CLI + code research + AI-optimized descriptions), explicit grounding directives
- **[Lesson 7: Planning & Execution](/docs/practical-techniques/lesson-7-planning-execution#require-evidence-to-force-grounding)** - Evidence requirements force file:line citations, challenging vague claims
- **[Lesson 9: Reviewing Code](/docs/practical-techniques/lesson-9-reviewing-code#mechanisms-at-work)** - Chain of Draft (CoD) efficiency, dual-optimized PR workflow, specialized review adaptations
